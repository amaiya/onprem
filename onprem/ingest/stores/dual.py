"""Dual vector store implementation for ingesting documents into both sparse and dense stores"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/01_ingest.stores.dual.ipynb.

# %% auto 0
__all__ = ['DualStore', 'ElasticsearchDualStore']

# %% ../../../nbs/01_ingest.stores.dual.ipynb 3
import os
from typing import List, Optional, Callable, Dict, Sequence, Union
from tqdm import tqdm
from langchain_core.documents import Document

from ..base import VectorStore
from .dense import DenseStore
from .sparse import SparseStore, ElasticsearchStore, ELASTICSEARCH_INSTALLED
from ..helpers import doc_from_dict

class DualStore(VectorStore):
    def __init__(
        self,
        dense_kind:str='chroma',
        dense_persist_directory: Optional[str] = None,
        sparse_kind:str='whoosh',
        sparse_persist_directory: Optional[str] = None,
        **kwargs
    ):
        """
        Initialize a dual vector store that manages both dense and sparse stores.
        
        **Args**:
        
          - *dense_persist_directory*: Path to dense vector database (created if it doesn't exist).
          - *sparse_persist_directory*: Path to sparse vector database (created if it doesn't exist).
          - *embedding_model_name*: name of sentence-transformers model
          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.
          - *embedding_encode_kwargs*: arguments to encode method of embedding model (e.g., `{'normalize_embeddings': False}`).
        """
        self.init_embedding_model(**kwargs)  # stored in self.embeddings
        
        # Initialize both stores
        self.dense_store = DenseStore.create(
            kind=dense_kind,
            persist_directory=dense_persist_directory,
            embedding_model_name=kwargs.get('embedding_model_name'),
            embedding_model_kwargs=kwargs.get('embedding_model_kwargs'),
            embedding_encode_kwargs=kwargs.get('embedding_encode_kwargs')
        )
        self.sparse_store = SparseStore.create(
            kind=sparse_kind,
            persist_directory=sparse_persist_directory,
            embedding_model_name=kwargs.get('embedding_model_name'),
            embedding_model_kwargs=kwargs.get('embedding_model_kwargs'),
            embedding_encode_kwargs=kwargs.get('embedding_encode_kwargs')
        )
        
        # For compatibility with the VectorStore interface
        self.persist_directory = dense_persist_directory

    @classmethod
    def create(cls, dense_kind='chroma', sparse_kind='whoosh', persist_directory=None, **kwargs):
        """
        Factory method to construct a DualStore instance.
        
        Args:
            dense_kind: type of dense store ('chroma', 'elasticsearch')
            sparse_kind: type of sparse store ('whoosh', 'elasticsearch')
            persist_directory: base directory for stores or Elasticsearch URL
            **kwargs: additional arguments passed to store initialization
            
        For traditional dual stores (different dense_kind and sparse_kind):
            dense_persist_directory: directory for dense store  
            sparse_persist_directory: directory for sparse store
            
        For unified Elasticsearch dual store (both kinds are 'elasticsearch'):
            index_name: name of Elasticsearch index
            dense_vector_field: field name for dense vectors
            vector_dims: dimension of dense vectors
            basic_auth: authentication credentials
            verify_certs: SSL verification
            ca_certs: CA certificate path
            timeout: connection timeout
            
        Returns:
            DualStore instance
        """
        # If both dense and sparse are elasticsearch, use unified ElasticsearchDualStore
        if dense_kind == 'elasticsearch' and sparse_kind == 'elasticsearch':
            if not ELASTICSEARCH_INSTALLED:
                raise ImportError('Please install elasticsearch packages: pip install onprem[elasticsearch]')
            return ElasticsearchDualStore(persist_directory=persist_directory, **kwargs)
        
        # Otherwise, use traditional dual store approach
        return cls(
            dense_kind=dense_kind,
            sparse_kind=sparse_kind,
            dense_persist_directory=persist_directory,
            sparse_persist_directory=persist_directory,
            **kwargs
        )


    def get_dense_db(self):
        """
        Returns the dense store's database instance.
        """
        return self.dense_store.get_db()
    
    def get_sparse_db(self):
        """
        Returns the sparse store's database instance.
        """
        return self.sparse_store.get_db()


    #------------------------------
    # overrides of abstract methods
    # -----------------------------
  
    
    def exists(self):
        """
        Returns True if either store exists.
        """
        return self.dense_store.exists() or self.sparse_store.exists()
    
    def add_documents(self, documents: Sequence[Document], batch_size: int = 1000, **kwargs):
        """
        Add documents to both dense and sparse stores.
        """
        if not documents:
            return
        # Add to dense store
        self.dense_store.add_documents(documents, batch_size=batch_size, **kwargs)
        
        # Add to sparse store
        self.sparse_store.add_documents(documents, **kwargs)

   
    def remove_document(self, id_to_delete):
        """
        Remove a document from both stores.
        """
        self.dense_store.remove_document(id_to_delete)
        self.sparse_store.remove_document(id_to_delete)

    def remove_source(self, source:str):
        """
        Remove a document by source from both stores.

        The `source` can either be the full path to a document
        or a parent folder.  Returns the number of records deleted.
        """
        num_deleted_1 = self.dense_store.remove_source(source)
        num_deleted_2 = self.sparse_store.remove_source(source)
        return num_deleted_1
    

    def update_documents(self, doc_dicts: dict, **kwargs):
        """
        Update documents in both stores.
        """
        self.dense_store.update_documents(doc_dicts, **kwargs)
        self.sparse_store.update_documents(doc_dicts, **kwargs)
    
    def get_all_docs(self):
        """
        Get all documents from the dense store.
        For simplicity, we only return documents from one store since they should be the same.
        """
        return self.dense_store.get_all_docs()
    
    def get_doc(self, id):
        """
        Get a document by ID from the dense store.
        """
        return self.dense_store.get_doc(id)
    
    def get_size(self):
        """
        Get the size of the dense store.
        """
        return self.dense_store.get_size()
    
    def erase(self, confirm=True):
        """
        Erase both stores.
        """
        dense_erased = self.dense_store.erase(confirm=confirm)
        sparse_erased = self.sparse_store.erase(confirm=False)  # Second confirmation not needed
        return dense_erased and sparse_erased
    
    def query(self, q: str, **kwargs):
        """
        Query using the sparse store.
        """
        return self.sparse_store.query(q, **kwargs)
    
    def semantic_search(self, query: str, **kwargs):
        """
        Perform semantic search using the dense store.
        """
        return self.dense_store.semantic_search(query, **kwargs)
    
    def hybrid_search(self, query: str, limit: int = 10, weights: Union[List[float], float] = 0.5, **kwargs):
        """
        Perform hybrid search combining dense and sparse results.
        
        **Args**:
        - *query*: Search query string
        - *limit*: Maximum number of results to return
        - *weights*: Weights for combining dense and sparse scores. 
                    If float, represents dense weight (sparse = 1 - dense).
                    If list, [dense_weight, sparse_weight]
        - *kwargs*: Additional arguments passed to individual search methods
        
        **Returns**:
        Dictionary with 'hits' and 'total_hits' keys
        """
        # Create weights array if single number passed
        if isinstance(weights, (int, float)):
            weights = [weights, 1 - weights]
        
        # Get expanded results from both stores
        search_limit = limit * 10  # Get more candidates for better fusion
        
        # Get dense results
        dense_results = self.dense_store.query(query, limit=search_limit, return_dict=True, **kwargs)
        dense_hits = dense_results.get('hits', [])
        
        # Get sparse results  
        sparse_results = self.sparse_store.query(query, limit=search_limit, return_dict=True, **kwargs)
        sparse_hits = sparse_results.get('hits', [])
        
        # Combine scores using hybrid approach
        uids = {}
        
        # Process dense results (similarity-based scores)
        if weights[0] > 0:
            for dense_doc in dense_hits:
                uid = dense_doc.get('id')
                if uid:
                    score = dense_doc.get('score', 0.0)
                    uids[uid] = score * weights[0]
        
        # Process sparse results (use RRF since sparse doesn't have normalized scores)
        if weights[1] > 0:
            for rank, sparse_doc in enumerate(sparse_hits):
                uid = sparse_doc.get('id')
                if uid:
                    # Use Reciprocal Rank Fusion (RRF) for sparse results
                    rrf_score = 1.0 / (rank + 1)
                    if uid in uids:
                        uids[uid] += rrf_score * weights[1]
                    else:
                        uids[uid] = rrf_score * weights[1]
        
        # Sort by combined score and limit results
        sorted_results = sorted(uids.items(), key=lambda x: x[1], reverse=True)[:limit]
        
        # Convert back to result dictionaries
        results = []
        # Create a lookup dict for efficient document retrieval
        doc_lookup = {}
        for doc in dense_hits + sparse_hits:
            doc_lookup[doc.get('id')] = doc
        
        for uid, combined_score in sorted_results:
            if uid in doc_lookup:
                doc_dict = doc_lookup[uid].copy()
                doc_dict['score'] = combined_score  # Update with combined score
                results.append(doc_dict)
        
        return {'hits': results, 'total_hits': len(results)}


class ElasticsearchDualStore(ElasticsearchStore):
    """
    A unified Elasticsearch-based dual store that supports both dense vector searches
    and sparse text searches in a single index. Extends ElasticsearchStore with dense vector capabilities.
    """
    
    def __init__(self,
                dense_vector_field: str = 'dense_vector',
                vector_dims: int = 384,  # Default for sentence-transformers models
                **kwargs):
        """
        Initializes unified Elasticsearch dual store with both dense and sparse capabilities.
        
        **Args:**
        - *dense_vector_field*: field name for dense vectors (default: 'dense_vector')
        - *vector_dims*: dimension of dense vectors (default: 384)
        - All other args are passed to ElasticsearchStore (persist_directory, index_name, basic_auth, etc.)
        """
        self.dense_vector_field = dense_vector_field
        self.vector_dims = vector_dims
        
        # Initialize parent ElasticsearchStore
        super().__init__(**kwargs)

    def _create_index(self):
        """Create Elasticsearch index with both text and vector mappings"""
        # Get the standard mapping from parent class
        mapping = {
            "mappings": {
                "properties": {
                    # Essential fields for core functionality
                    "page_content": {"type": "text", "analyzer": "standard"},
                    "id": {"type": "keyword"},
                    "source": {"type": "keyword"},
                    "source_search": {"type": "text", "analyzer": "standard"},
                    
                    # Dense vector field for semantic search
                    self.dense_vector_field: {
                        "type": "dense_vector",
                        "dims": self.vector_dims,
                        "index": True,
                        "similarity": "cosine"
                    }
                }
            }
        }
        
        self.es.indices.create(index=self.index_name, body=mapping)

    def doc2dict(self, doc: Document, include_vector: bool = True):
        """Convert LangChain Document to expected format with optional vector embedding"""
        # Get the standard dict from parent class
        d = super().doc2dict(doc)
        
        # Add dense vector embedding if requested
        if include_vector and hasattr(self, 'embeddings'):
            try:
                # Generate embedding for the document text
                embedding = self.embeddings.embed_documents([doc.page_content])[0]
                d[self.dense_vector_field] = embedding
            except Exception as e:
                # If embedding fails, continue without it
                pass
                
        return d


    def semantic_search(self,
                       query: str,
                       limit: int = 4,
                       filters: Optional[Dict[str, str]] = None,
                       return_dict: bool = True,
                       **kwargs):
        """Perform semantic search using dense vectors (equivalent to ChromaStore.semantic_search)"""
        if not hasattr(self, 'embeddings'):
            raise ValueError("Embeddings not initialized. Cannot perform semantic search.")
        
        # Generate query embedding
        query_embedding = self.embeddings.embed_query(query)
        
        # Build Elasticsearch KNN query
        knn_query = {
            "knn": {
                "field": self.dense_vector_field,
                "query_vector": query_embedding,
                "k": limit,
                "num_candidates": limit * 10  # More candidates for better results
            }
        }
        
        # Add filters if provided
        if filters:
            filter_clauses = []
            for k, v in filters.items():
                filter_clauses.append({"term": {k: v}})
            knn_query["knn"]["filter"] = filter_clauses
        
        # Execute search
        response = self.es.search(index=self.index_name, body=knn_query, size=limit)
        
        # Process results
        hits = []
        for hit in response['hits']['hits']:
            doc_dict = hit['_source'].copy()
            # Convert Elasticsearch score to similarity score (higher is better)
            doc_dict['score'] = hit['_score']
            hits.append(doc_dict)
        
        total_hits = response['hits']['total']['value']
        
        if return_dict:
            return {'hits': hits, 'total_hits': total_hits}
        else:
            return [doc_from_dict(hit) for hit in hits]


    def hybrid_search(self,
                     query: str,
                     limit: int = 10,
                     weights: Union[List[float], float] = 0.5,
                     filters: Optional[Dict[str, str]] = None,
                     **kwargs):
        """Perform hybrid search combining dense vector and sparse text search using Reciprocal Rank Fusion (RRF)"""
        # Create weights array if single number passed
        if isinstance(weights, (int, float)):
            weights = [weights, 1 - weights]
        
        # Get expanded results from both search types
        search_limit = limit * 10  # Get more candidates for better fusion
        
        # Get dense vector results
        dense_results = []
        if weights[0] > 0:
            try:
                dense_results = self.semantic_search(
                    query, 
                    limit=search_limit, 
                    filters=filters, 
                    return_dict=True
                )['hits']
            except Exception:
                # If dense search fails, continue with sparse only
                pass
        
        # Get sparse text results  
        sparse_results = []
        if weights[1] > 0:
            try:
                sparse_results = self.query(
                    query, 
                    limit=search_limit, 
                    filters=filters, 
                    return_dict=True
                )['hits']
            except Exception:
                # If sparse search fails, continue with dense only
                pass
        
        # Combine scores using Reciprocal Rank Fusion (RRF)
        doc_scores = {}
        
        # Process dense results (use actual similarity scores)
        if weights[0] > 0:
            for rank, doc in enumerate(dense_results):
                doc_id = doc.get('id')
                if doc_id:
                    # Use RRF: 1 / (k + rank) where k=60 is standard
                    rrf_score = 1.0 / (60 + rank + 1)
                    doc_scores[doc_id] = {
                        'doc': doc,
                        'score': rrf_score * weights[0]
                    }
        
        # Process sparse results (use RRF for ranking)
        if weights[1] > 0:
            for rank, doc in enumerate(sparse_results):
                doc_id = doc.get('id')
                if doc_id:
                    # Use RRF: 1 / (k + rank) where k=60 is standard
                    rrf_score = 1.0 / (60 + rank + 1)
                    if doc_id in doc_scores:
                        # Combine scores if document appears in both results
                        doc_scores[doc_id]['score'] += rrf_score * weights[1]
                    else:
                        doc_scores[doc_id] = {
                            'doc': doc,
                            'score': rrf_score * weights[1]
                        }
        
        # Sort by combined score and limit results
        sorted_results = sorted(
            doc_scores.items(), 
            key=lambda x: x[1]['score'], 
            reverse=True
        )[:limit]
        
        # Convert back to result format
        hits = []
        for doc_id, result in sorted_results:
            doc_dict = result['doc'].copy()
            doc_dict['score'] = result['score']  # Update with combined RRF score
            hits.append(doc_dict)
        
        return {'hits': hits, 'total_hits': len(hits)}

    def get_dense_db(self):
        """Returns the Elasticsearch client for dense operations"""
        return self.es
    
    def get_sparse_db(self):
        """Returns the Elasticsearch client for sparse operations"""
        return self.es
