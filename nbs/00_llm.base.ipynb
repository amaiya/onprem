{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm\n",
    "\n",
    "> Core functionality for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp llm.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from onprem.utils import get_datadir, get_models_dir, download, format_string, DEFAULT_DB\n",
    "from onprem.llm import helpers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain_litellm import ChatLiteLLM\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Callable, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "\n",
    "MIN_MODEL_SIZE = 250000000\n",
    "OLLAMA_URL = 'http://localhost:11434/v1'\n",
    "MISTRAL_MODEL_URL = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "MISTRAL_MODEL_ID = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "MISTRAL_PROMPT_TEMPLATE = \"[INST] {prompt} [/INST]\"\n",
    "ZEPHYR_MODEL_URL = \"https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "ZEPHYR_MODEL_ID = \"TheBloke/zephyr-7B-beta-AWQ\"\n",
    "ZEPHYR_PROMPT_TEMPLATE = \"<|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\"\n",
    "LLAMA_MODEL_URL = \"https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n",
    "LLAMA_MODEL_ID = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\"\n",
    "LLAMA_PROMPT_TEMPLATE = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a super-intelligent helpful assistant that executes instructions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "MODEL_URL_DICT = {'mistral' : MISTRAL_MODEL_URL, 'zephyr':ZEPHYR_MODEL_URL, 'llama' : LLAMA_MODEL_URL}\n",
    "URL2NAME = dict([(v,k) for k,v in MODEL_URL_DICT.items()])\n",
    "MODEL_ID_DICT = {'mistral': MISTRAL_MODEL_ID, 'zephyr' : ZEPHYR_MODEL_ID, 'llama': LLAMA_MODEL_ID}\n",
    "LLAMA_CPP = 'llama.cpp'\n",
    "TRANSFORMERS = 'transformers'\n",
    "ENGINE_DICT = {LLAMA_CPP: MODEL_URL_DICT, TRANSFORMERS : MODEL_ID_DICT}\n",
    "PROMPT_DICT = {'mistral': MISTRAL_PROMPT_TEMPLATE, 'zephyr' : ZEPHYR_PROMPT_TEMPLATE, 'llama' : LLAMA_PROMPT_TEMPLATE} \n",
    "DEFAULT_MODEL = 'zephyr'\n",
    "DEFAULT_ENGINE = LLAMA_CPP\n",
    "DEFAULT_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DEFAULT_QA_PROMPT = \"\"\"\"Use the following pieces of context delimited by three backticks to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "```{context}```\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url:Optional[str] = None,\n",
    "        model_id:Optional[str] = None,\n",
    "        default_model:str  = DEFAULT_MODEL,\n",
    "        default_engine:str = DEFAULT_ENGINE,\n",
    "        n_gpu_layers: Optional[int] = None,\n",
    "        prompt_template: Optional[str] = None,\n",
    "        model_download_path: Optional[str] = None,\n",
    "        vectordb_path: Optional[str] = None,\n",
    "        store_type:str='dense',\n",
    "        max_tokens: int = 512,\n",
    "        n_ctx: int = 3900,\n",
    "        n_batch: int = 1024,\n",
    "        stop:list=[],\n",
    "        mute_stream: bool = False,\n",
    "        callbacks=[],\n",
    "        embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        embedding_model_kwargs: Optional[dict] = None,\n",
    "        embedding_encode_kwargs: dict = {\"normalize_embeddings\": False},\n",
    "        rag_num_source_docs: int = 4,\n",
    "        rag_score_threshold: float = 0.0,\n",
    "        check_model_download:bool=True,\n",
    "        confirm: bool = True,\n",
    "        verbose: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LLM Constructor.  Extra `kwargs` (e.g., temperature) are fed directly\n",
    "        to `langchain.llms.LlamaCpp`  (if `model_url` is supplied) or\n",
    "        `transformers.pipeline` (if `model_id` is supplied).\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *model_url*: URL to `.GGUF` model (or the filename if already been downloaded to `model_download_path`).\n",
    "                       To use an OpenAI-compatible REST API (e.g., vLLM, OpenLLM, Ollama), supply the URL (e.g., `http://localhost:8080/v1`).\n",
    "                       To use a cloud-based OpenAI model, replace URL with: `openai://<name_of_model>` (e.g., `openai://gpt-3.5-turbo`).\n",
    "                       To use Azure OpenAI, replace URL with: with: `azure://<deployment_name>`.\n",
    "                       If None, use the model indicated by `default_model`.\n",
    "        - *model_id*: Name of or path to Hugging Face model (e.g., in SafeTensor format). Hugging Face Transformers is used for LLM generation instead of **llama-cpp-python**. Mutually-exclusive with `model_url` and `default_model`. The `n_gpu_layers` and `model_download_path` parameters are ignored if `model_id` is supplied.\n",
    "        - *default_model*: One of {'mistral', 'zephyr', 'llama'}, where mistral is Mistral-Instruct-7B-v0.2, zephyr is Zephyr-7B-beta, and llama is Llama-3.1-8B.\n",
    "        - *default_engine*: The engine used to run the `default_model`. One of {'llama.cpp', 'transformers'}.\n",
    "        - *n_gpu_layers*: Number of layers to be loaded into gpu memory. Default is `None`.\n",
    "        - *prompt_template*: Optional prompt template (must have a variable named \"prompt\"). Prompt templates are not typically needed when using the `model_id` parameter, as transformers sets it automatically.\n",
    "        - *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
    "        - *vectordb_path*: Path to vector database (created if it doesn't exist).\n",
    "                           Default is `onprem_data/vectordb` in user's home directory.\n",
    "        - *store_type*: One of `dense` for the default dense vector database (i.e., chroma) or\n",
    "                        `sparse` for the sparse vector store (i.e., a keyword search engine).  \n",
    "                        (Documents stored in sparse vector databases are converted to dense vectors at inference time \n",
    "                        when used with `LLM.ask`.)\n",
    "        - *max_tokens*: The maximum number of tokens to generate.\n",
    "        - *n_ctx*: Token context window. Only used for llama-cpp backend.\n",
    "                   For Ollama backend, explicitly supply `num_ctx` instead which is passed to LiteLLM.\n",
    "                   Hugging Face Transformers backend (i.e., when using the model_id parameter)\n",
    "                   sets context window automatically.\n",
    "        - *n_batch*: Number of tokens to process in parallel.\n",
    "        - *stop*: a list of strings to stop generation when encountered (applied to all calls to `LLM.prompt`)\n",
    "        - *mute_stream*: Mute ChatGPT-like token stream output during generation\n",
    "        - *callbacks*: Callbacks to supply model\n",
    "        - *embedding_model_name*: name of sentence-transformers model. Used for `LLM.ingest` and `LLM.ask`.\n",
    "        - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, uses GPU if available.\n",
    "        - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                     embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        - *rag_num_source_docs*: The maximum number of documents retrieved and fed to `LLM.ask` and `LLM.chat` to generate answers.\n",
    "        - *rag_score_threshold*: Minimum similarity score for source to be considered by `LLM.ask` and `LLM.chat`.\n",
    "        - *confirm*: whether or not to confirm with user before downloading a model\n",
    "        - *verbose*: Verbosity\n",
    "        \"\"\"\n",
    "        self.model_id = None\n",
    "        self.model_url = None\n",
    "\n",
    "        if model_url in URL2NAME:\n",
    "            default_model = URL2NAME[model_url]\n",
    "            model_url = None\n",
    "\n",
    "        if model_url and model_id:\n",
    "            raise ValueError('The parameters model_url and model_id are mutually-exclusive.')\n",
    "        elif model_id:\n",
    "            self.model_id = model_id\n",
    "            self.model_name = os.path.basename(model_id)\n",
    "        elif model_url:\n",
    "            self.model_url = model_url.split(\"?\")[0]\n",
    "            self.model_name = os.path.basename(model_url)\n",
    "\n",
    "            # handle Ollama-style URL\n",
    "            provider_model = self.process_service(self.model_url)\n",
    "            if provider_model and provider_model.split(\"/\")[0] == 'ollama':\n",
    "                # point ollama to ollama_chat as convenience\n",
    "                self.model_url = self.model_url.replace('ollama', 'ollama_chat')\n",
    "\n",
    "            # override model name if explicitly provided\n",
    "            # for local APIs, this replaces \"v1\" with actual model name\n",
    "            self.model_name = kwargs.get('model', self.model_name)\n",
    "        else: # neither supplied so use defaults\n",
    "            url_or_id = ENGINE_DICT[default_engine][default_model]\n",
    "            self.model_name = os.path.basename(url_or_id)\n",
    "            if default_engine == LLAMA_CPP:\n",
    "                self.model_url = url_or_id\n",
    "                self.model_id = None\n",
    "                prompt_template = PROMPT_DICT[default_model] if not prompt_template else prompt_template\n",
    "            else:\n",
    "                self.model_url = None\n",
    "                self.model_id = url_or_id\n",
    "        self.model_download_path = model_download_path or get_models_dir()\n",
    "\n",
    "        if self.is_llamacpp():\n",
    "            try:\n",
    "                from llama_cpp import Llama\n",
    "            except ImportError:\n",
    "                raise ValueError('To run local LLMs, the llama-cpp-python package is required. ' +\\\n",
    "                                 'You can visit https://python.langchain.com/docs/integrations/llms/llamacpp ' +\\\n",
    "                                 'and follow the instructions for your operating system.')\n",
    "        if self.is_llamacpp() and not os.path.isfile(os.path.join(self.model_download_path, self.model_name)):\n",
    "            self.download_model(\n",
    "                self.model_url,\n",
    "                model_download_path=self.model_download_path,\n",
    "                confirm=confirm,\n",
    "            )\n",
    "        self.prompt_template = prompt_template\n",
    "        self.vectordb_path = vectordb_path or os.path.join(get_datadir(), DEFAULT_DB)\n",
    "        self.store_type = store_type  # one of 'dense', 'sparse', or 'dual'\n",
    "        self.llm = None\n",
    "        self.vectorstore = None\n",
    "        self.qa = None\n",
    "        self.chatbot = None\n",
    "        self.n_gpu_layers = n_gpu_layers\n",
    "        self.max_tokens = max_tokens\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_batch = n_batch\n",
    "        self.stop = stop\n",
    "        self.mute_stream = mute_stream\n",
    "        self.callbacks = [] if mute_stream else [StreamingStdOutCallbackHandler()]\n",
    "        if callbacks:\n",
    "            self.callbacks.extend(callbacks)\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_kwargs = embedding_model_kwargs\n",
    "        self.embedding_encode_kwargs = embedding_encode_kwargs\n",
    "        self.rag_num_source_docs = rag_num_source_docs\n",
    "        self.rag_score_threshold = rag_score_threshold\n",
    "        self.check_model_download = check_model_download\n",
    "        self.verbose = verbose\n",
    "        self.extra_kwargs = kwargs\n",
    "\n",
    "\n",
    "        # explicitly set offload_kqv\n",
    "        # reference: https://github.com/abetlen/llama-cpp-python/issues/999#issuecomment-1858041458\n",
    "        # commented out now: no longer needed as of llama-cpp-python==0.2.75\n",
    "        #self.offload_kqv = True if n_gpu_layers is not None and n_gpu_layers > 0 else False\n",
    "\n",
    "        # load LLM\n",
    "        self.load_llm()\n",
    "\n",
    "    def set_store_type(self, store_type:str):\n",
    "        \"\"\"\n",
    "        Change store type. Not that this will reload the default vectorstore on selected store type.\n",
    "        \"\"\"\n",
    "        if store_type not in ['dense', 'sparse', 'dual']:\n",
    "            raise ValueError('store_type must be one of {\"dense\", \"sparse\", \"dual\"}')\n",
    "        self.store_type = store_type\n",
    "        self.vectorstore = None\n",
    "        self.load_vectorstore()\n",
    "        return\n",
    "\n",
    "    def get_store_type(self):\n",
    "        return self.store_type\n",
    "\n",
    "    def is_sparse_store(self):\n",
    "        return self.store_type == 'sparse'\n",
    "\n",
    "    def is_dense_store(self):\n",
    "        return self.store_type == 'dense'\n",
    "\n",
    "    def is_dual_store(self):\n",
    "        return self.store_type == 'dual'\n",
    "        \n",
    "    def is_local_api(self):\n",
    "        basename = os.path.basename(self.model_url) if self.model_url else None\n",
    "        return self.model_url and self.model_url.lower().startswith('http') and not basename.lower().endswith('.gguf') and not basename.lower().endswith('.bin')\n",
    "\n",
    "\n",
    "    def process_service(self, model_url):\n",
    "        \"\"\"\n",
    "        If model_url represents an LLM service (e.g, openai), return it\n",
    "        in LiteLLM-style syntax.\n",
    "        \"\"\"\n",
    "        if model_url:\n",
    "            if (\":\" not in model_url or model_url.startswith('ollama')) and model_url.count(\"/\") == 1:\n",
    "                # conclude that this a standard LiteLLM path (e.g., openai/gpt-4o)\n",
    "                # or Ollama model which contain \":\"\n",
    "                return model_url\n",
    "            provider = model_url.split(\":\")[0]\n",
    "            if provider.startswith('http'):\n",
    "                return None\n",
    "            model = os.path.basename(self.model_url)\n",
    "            return model if provider == model else f'{provider}/{model}'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def is_local(self):\n",
    "        return (self.model_url and self.model_url.lower().endswith('.gguf')) or self.model_id\n",
    "\n",
    "\n",
    "    def is_llamacpp(self):\n",
    "        return self.is_local() and not self.is_hf()\n",
    "\n",
    "    def is_hf(self):\n",
    "        return self.model_id is not None\n",
    "\n",
    "    def is_openai_model(self):\n",
    "        return self.model_url and self.model_url.lower().startswith('openai')\n",
    "\n",
    "    def is_azure(self):\n",
    "        return self.model_url and self.model_url.lower().startswith('azure')\n",
    "\n",
    "    def update_max_tokens(self, value:int=512):\n",
    "        \"\"\"\n",
    "        Update `max_tokens` (maximum length of generation).\n",
    "        \"\"\"\n",
    "        llm = self.load_llm()\n",
    "        llm.max_tokens = value\n",
    "\n",
    "\n",
    "    def update_stop(self, value:list=[]):\n",
    "        \"\"\"\n",
    "        Update `max_tokens` (maximum length of generation).\n",
    "        \"\"\"\n",
    "        llm = self.load_llm()\n",
    "        llm.stop = value\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def download_model(\n",
    "        cls,\n",
    "        model_url:Optional[str] = None,\n",
    "        default_model:str=DEFAULT_MODEL,\n",
    "        model_download_path: Optional[str] = None,\n",
    "        confirm: bool = True,\n",
    "        ssl_verify: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *model_url*: URL of model. If None, then use default_model.\n",
    "        - *default_model*: One of {'mistral', 'zephyr', 'llama'}, where mistral is Mistral-Instruct-7B-v0.2, zephyr is Zephyr-7B-beta, and llama is Llama-3.1-8B.\n",
    "        - *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
    "        - *confirm*: whether or not to confirm with user before downloading\n",
    "        - *ssl_verify*: If True, SSL certificates are verified.\n",
    "                        You can set to False if corporate firewall gives you problems.\n",
    "        \"\"\"\n",
    "        model_url = MODEL_URL_DICT[default_model] if not model_url else model_url\n",
    "        if 'https://huggingface.co' in model_url and 'resolve' not in model_url:\n",
    "            warnings.warn('\\n\\nThe supplied URL may not be pointing to the actual GGUF model file.  Please check it.\\n\\n')\n",
    "        datadir = model_download_path or get_models_dir()\n",
    "        model_name = os.path.basename(model_url)\n",
    "        filename = os.path.join(datadir, model_name)\n",
    "        confirm_msg = f\"\\nYou are about to download the LLM {model_name} to the {datadir} folder. Are you sure?\"\n",
    "        if os.path.isfile(filename):\n",
    "            confirm_msg = f\"There is already a file {model_name} in {datadir}.\\n Do you want to still download it?\"\n",
    "\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            shall = input(\"%s (y/N) \" % confirm_msg).lower() == \"y\"\n",
    "        if shall:\n",
    "            download(model_url, filename, verify=ssl_verify)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f'{model_name} was not downloaded because \"Y\" was not selected.'\n",
    "            )\n",
    "        return\n",
    "\n",
    "\n",
    "    def load_vectorstore(self, custom_vectorstore=None, reset=False):\n",
    "        \"\"\"\n",
    "        Get `VectorStore` instance.\n",
    "        You can access the `langchain_chroma.Chroma` instance with `load_vectorstore().get_db()`.\n",
    "        Supply `custom_vectorstore` to use your own `VectorStore` instance\n",
    "        (i.e., subclass `DenseStore` or `SparseStore`).\n",
    "        Supply `reset=True` to reload the default vectorstore.\n",
    "        \"\"\"\n",
    "        from onprem.ingest.stores import DenseStore, SparseStore, DualStore\n",
    "\n",
    "        if reset:\n",
    "            self.vectorstore = None\n",
    "        if custom_vectorstore:\n",
    "            self.vectorstore = custom_vectorstore\n",
    "\n",
    "        if not self.vectorstore:\n",
    "\n",
    "            # store vector stores within subfolders under `vectordb_path`\n",
    "            if self.is_sparse_store():\n",
    "                store_path = os.path.join(self.vectordb_path, 'sparse')\n",
    "                # create vector store\n",
    "                self.vectorstore = SparseStore.create(\n",
    "                    embedding_model_name=self.embedding_model_name,\n",
    "                    embedding_model_kwargs=self.embedding_model_kwargs,\n",
    "                    embedding_encode_kwargs=self.embedding_encode_kwargs,\n",
    "                    persist_directory=store_path,\n",
    "                )\n",
    "            elif self.is_dense_store():\n",
    "                store_path = os.path.join(self.vectordb_path, 'dense')\n",
    "\n",
    "                # create vector store\n",
    "                self.vectorstore = DenseStore.create(\n",
    "                    embedding_model_name=self.embedding_model_name,\n",
    "                    embedding_model_kwargs=self.embedding_model_kwargs,\n",
    "                    embedding_encode_kwargs=self.embedding_encode_kwargs,\n",
    "                    persist_directory=store_path,\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                # Create paths for both stores\n",
    "                dense_path = os.path.join(self.vectordb_path, 'dense')\n",
    "                sparse_path = os.path.join(self.vectordb_path, 'sparse')\n",
    "\n",
    "                # create vector store\n",
    "                self.vectorstore = DualStore(\n",
    "                    embedding_model_name=self.embedding_model_name,\n",
    "                    embedding_model_kwargs=self.embedding_model_kwargs,\n",
    "                    embedding_encode_kwargs=self.embedding_encode_kwargs,\n",
    "                    sparse_persist_directory=sparse_path,\n",
    "                    dense_persist_directory=dense_path,\n",
    "                )\n",
    "        return self.vectorstore\n",
    "\n",
    "    def ingest(\n",
    "        self,\n",
    "        source_directory: str, # path to folder containing documents\n",
    "        chunk_size: int = 500, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        chunk_overlap: int = 50, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        ignore_fn:Optional[Callable] = None, # callable that accepts the file path and returns True for ignored files\n",
    "        batch_size:int=1000, # batch size used when processing documents(e.g, creating embeddings).\n",
    "        **kwargs, # Extra kwargs fed to downstream functions, `load_single_document` and/or `load_documents`\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` into vector database.\n",
    "        Previously-ingested documents are ignored.\n",
    "        Extra kwargs fed to `load_single_document`, load_documents`, and/or `chunk_documents`.\n",
    "        \"\"\"\n",
    "        vectorstore = self.load_vectorstore()\n",
    "        return vectorstore.ingest(\n",
    "            source_directory,\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, ignore_fn=ignore_fn,\n",
    "            llm=kwargs['llm'] if 'llm' in kwargs else self,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def check_model(self, silent:bool=False):\n",
    "        \"\"\"\n",
    "        Returns the path to the model\n",
    "        \"\"\"\n",
    "        if not self.is_llamacpp():\n",
    "            return None\n",
    "        datadir = self.model_download_path\n",
    "        model_path = os.path.join(datadir, self.model_name)\n",
    "        if not os.path.isfile(model_path):\n",
    "            if silent: return None\n",
    "            raise ValueError(\n",
    "                f\"The LLM model {self.model_name} does not appear to have been downloaded. \"\n",
    "                + \"Execute the download_model() method to download it.\"\n",
    "            )\n",
    "        return model_path\n",
    "\n",
    "    def load_llm(self):\n",
    "        \"\"\"\n",
    "        Loads the LLM from the model path.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if not self.llm and self.is_openai_model():\n",
    "            self.llm = ChatOpenAI(model_name=self.model_name, \n",
    "                                  callbacks=self.callbacks, \n",
    "                                  streaming=not self.mute_stream,\n",
    "                                  max_tokens=self.max_tokens,\n",
    "                                  model_kwargs=self.extra_kwargs)\n",
    "        elif not self.llm and\\\n",
    "                self.process_service(self.model_url) and\\\n",
    "                not self.check_model(silent=True):\n",
    "            self.llm = ChatLiteLLM(model=self.process_service(self.model_url),\n",
    "                                  callbacks=self.callbacks,\n",
    "                                  streaming=not self.mute_stream,\n",
    "                                  max_tokens=self.max_tokens,\n",
    "                                  model_kwargs=self.extra_kwargs)\n",
    "        elif not self.llm and self.is_azure():\n",
    "            self.llm = AzureChatOpenAI(azure_deployment=self.model_name, \n",
    "                                  callbacks=self.callbacks, \n",
    "                                  streaming=not self.mute_stream,\n",
    "                                  max_tokens=self.max_tokens,\n",
    "                                  model_kwargs=self.extra_kwargs)\n",
    "\n",
    "        elif not self.llm and self.is_local_api():\n",
    "            self.llm = ChatOpenAI(base_url=self.model_url,\n",
    "                                  #model_name=self.model_name, \n",
    "                                  callbacks=self.callbacks, \n",
    "                                  streaming=not self.mute_stream,\n",
    "                                  max_tokens=self.max_tokens,\n",
    "                                  model_kwargs=self.extra_kwargs)\n",
    "        elif not self.llm and self.is_hf():\n",
    "            from transformers import pipeline, TextStreamer, AutoTokenizer\n",
    "            from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "            tokenizer = self.extra_kwargs['tokenizer'] if 'tokenizer' in self.extra_kwargs\\\n",
    "                        else AutoTokenizer.from_pretrained(self.model_id)\n",
    "\n",
    "            if 'tokenizer' in self.extra_kwargs:\n",
    "                del self.extra_kwargs['tokenizer']\n",
    "\n",
    "\n",
    "            streamer = TextStreamer(tokenizer)\n",
    "\n",
    "\n",
    "            pipe = pipeline('text-generation',\n",
    "                              self.model_id,\n",
    "                              tokenizer=tokenizer,\n",
    "                              streamer=streamer if not self.mute_stream else None,\n",
    "                              max_new_tokens = self.max_tokens,\n",
    "                              return_full_text=False,\n",
    "                              do_sample=True if\\\n",
    "                                     self.extra_kwargs.get('temperature', 0.8)>0.0 else False ,\n",
    "                              **self.extra_kwargs)\n",
    "\n",
    "            model = pipe.model\n",
    "            if not model.generation_config.pad_token_id:\n",
    "                tokenid = model.generation_config.eos_token_id\n",
    "                model.generation_config.pad_token_id = tokenid[0] if isinstance(tokenid, list) else tokenid\n",
    "\n",
    "            hfpipe = HuggingFacePipeline(pipeline=pipe)\n",
    "            self.llm = ChatHuggingFace(llm=hfpipe, model_id=self.model_id, tokenizer=tokenizer)\n",
    "\n",
    "            # Set generation_config.pad_token_id\n",
    "            model = self.llm.llm.pipeline.model\n",
    "            if not model.generation_config.pad_token_id:\n",
    "                tokenid = model.generation_config.eos_token_id\n",
    "                model.generation_config.pad_token_id = tokenid[0] if isinstance(tokenid, list) else tokenid\n",
    "\n",
    "        elif not self.llm:\n",
    "            model_path = self.check_model()\n",
    "            if self.check_model_download and os.path.getsize(model_path) < MIN_MODEL_SIZE:\n",
    "                raise ValueError(f'The model file ({model_path} is less than {MIN_MODEL_SIZE} bytes. ' +\\\n",
    "                                 'It may not have been fully downloaded. Please delete the file and start again. ')\n",
    "            self.llm = LlamaCpp(\n",
    "                model_path=model_path,\n",
    "                max_tokens=self.max_tokens,\n",
    "                n_batch=self.n_batch,\n",
    "                callbacks=self.callbacks,\n",
    "                verbose=self.verbose,\n",
    "                n_gpu_layers=self.n_gpu_layers,\n",
    "                n_ctx=self.n_ctx,\n",
    "                model_kwargs=self.extra_kwargs,\n",
    "            )\n",
    "\n",
    "        return self.llm\n",
    "\n",
    "\n",
    "    def _format_image_prompt(self, prompt:str, image_path_or_url:str):\n",
    "        \"\"\"\n",
    "        Correctly format image prompt\n",
    "        \"\"\"\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        import base64\n",
    "        if not image_path_or_url.startswith('http'):\n",
    "            with open(image_path_or_url, \"rb\") as f:\n",
    "                image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "            image_path_or_url = f\"data:image/jpeg;base64,{image_data}\"\n",
    "\n",
    "        message = HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": image_path_or_url},\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        prompt = [message]\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def _format_pydantic_prompt(self, prompt, pydantic_model):\n",
    "        \"\"\"\n",
    "        Correctly format prompt for Pydantic model\n",
    "        \"\"\"\n",
    "        parser = PydanticOutputParser(pydantic_object=pydantic_model)\n",
    "        prompt_obj = PromptTemplate(\n",
    "                     template=\"Answer the user query.\\n{format_instructions}\\n{prompt}\\n\",\n",
    "                     input_variables=[\"prompt\"],\n",
    "                     partial_variables={\"format_instructions\": parser.get_format_instructions()},)\n",
    "        return (prompt_obj.invoke({'prompt': prompt}).text, parser)\n",
    "\n",
    "\n",
    "    def pydantic_prompt(self, \n",
    "                        prompt:str,\n",
    "                        pydantic_model=None,\n",
    "                        attempt_fix:bool= False,\n",
    "                        fix_llm=None,\n",
    "                        stop:list=[],\n",
    "                        **kwargs):\n",
    "        \"\"\"\n",
    "        Accept a prompt as string and Pydantic model describing the desired output.\n",
    "        Output will be a Pydantic object in the requested format.\n",
    "\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *prompt*: The prompt to supply to the model.\n",
    "                    Either a string or OpenAI-style list of dictionaries\n",
    "                    representing messages (e.g., \"human\", \"system\").\n",
    "        - *pydantic_model*: A Pydanatic model (sublass of `pydantic.BaseModel` that describes the desired output format.\n",
    "                           Output will be a desired Pydantic object.\n",
    "                           If `put_format=None`, then output is a string.\n",
    "        - *attempt_fix*: Use an LLM call in attempt to correct malformed or incomplete outputs\n",
    "        - *fix_llm*:  LLM to use for fixing (e.g., `langchain_openai.ChatOpenAI()`). If `None`, then existing `LLM.llm` used.\n",
    "        - *stop*: a list of strings to stop generation when encountered. \n",
    "                  This value will override the `stop` parameter supplied to `LLM` constructor.\n",
    "        \"\"\"\n",
    "        # setup up prompt for output parsing\n",
    "        prompt, output_parser = self._format_pydantic_prompt(prompt, pydantic_model)\n",
    "\n",
    "        # generate output\n",
    "        output = self.prompt(prompt, stop=stop, **kwargs)\n",
    "\n",
    "        # set parser\n",
    "        fix_llm = fix_llm if fix_llm else self.llm\n",
    "        parser = OutputFixingParser.from_llm(parser=output_parser, llm=fix_llm)\\\n",
    "                if attempt_fix else output_parser\n",
    "\n",
    "        # parse output into Pydantic class\n",
    "        try:\n",
    "            return parser.parse(output)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                from onprem.llm.helpers import extract_json\n",
    "                json_string = extract_json(output)\n",
    "                if json_string:\n",
    "                    return parser.parse(json_string)\n",
    "                raise Exception(str(e))\n",
    "            except Exception as e:\n",
    "                print()\n",
    "                print()\n",
    "                warnings.warn(f'LLM output was malformed or incomplete, so returning raw string output: {str(e)}')\n",
    "                print()\n",
    "                return output\n",
    "\n",
    "\n",
    "    def prompt(self,\n",
    "               prompt:Union[str, List[Dict]],\n",
    "               output_parser:Optional[Any]=None,\n",
    "               image_path_or_url:Optional[str] = None,\n",
    "               prompt_template: Optional[str] = None, stop:list=[],\n",
    "               truncate_prompt:bool=False, truncate_strategy:str='start',\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Send prompt to LLM to generate a response.\n",
    "        Extra keyword arguments are sent directly to the model invocation.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *prompt*: The prompt to supply to the model.\n",
    "                    Either a string or OpenAI-style list of dictionaries\n",
    "                    representing messages (e.g., \"human\", \"system\").\n",
    "        - *image_path_or_url*: Path or URL to an image file\n",
    "        - *prompt_template*: Optional prompt template (must have a variable named \"prompt\").\n",
    "                             This value will override any `prompt_template` value supplied \n",
    "                             to `LLM` constructor.\n",
    "        - *stop*: a list of strings to stop generation when encountered. \n",
    "                  This value will override the `stop` parameter supplied to `LLM` constructor.\n",
    "        - *truncate_prompt*: Truncate long string prompts. Only applies to `llama-cpp-python` and `transformers` LLMs.\n",
    "        - *truncate_strategy*: Either 'first' (keep latest) or 'last` (keep earliest). Ignored if `truncate_prompt=False`.\n",
    "\n",
    "        \"\"\"\n",
    "        # load llm\n",
    "        llm = self.load_llm()\n",
    "\n",
    "        # prompt is a list of dictionaries reprsenting messages\n",
    "        if isinstance(prompt, list):\n",
    "            if self.is_llamacpp():\n",
    "                # LangChain's LlamaCpp does not provide access to create_chat_completion,\n",
    "                # so access it directly (with streaming disabled)\n",
    "                res = self.llm.client.create_chat_completion(prompt)\n",
    "                return res['choices'][0]['message']['content']\n",
    "            else:\n",
    "                try:\n",
    "                    res = llm.invoke(prompt, stop=stop, **kwargs)\n",
    "                except Exception as e: # stop param fails with GPT-4o vision prompts\n",
    "                    res = llm.invoke(prompt, **kwargs)\n",
    "        # prompt is string\n",
    "        else:\n",
    "            if image_path_or_url:\n",
    "                prompt = self._format_image_prompt(prompt, image_path_or_url)\n",
    "                res = llm.invoke(prompt, **kwargs) # including stop causes errors in gpt-4o\n",
    "            else:\n",
    "                # set prompt template\n",
    "                prompt_template = self.prompt_template if prompt_template is None else prompt_template\n",
    "\n",
    "                # truncate prompt\n",
    "                if truncate_prompt and self.is_hf():\n",
    "                    prompt = helpers.truncate_prompt(llm.llm.pipeline,\n",
    "                                                     prompt,\n",
    "                                                     max_gen_tokens=self.max_tokens,\n",
    "                                                     truncate_from=truncate_strategy,\n",
    "                                                     prompt_template=prompt_template)\n",
    "                elif truncate_prompt and self.is_llamacpp():\n",
    "                    prompt = helpers.truncate_prompt(self.llm.client,\n",
    "                                                     prompt,\n",
    "                                                     max_gen_tokens=self.max_tokens,\n",
    "                                                     truncate_from=truncate_strategy,\n",
    "                                                     prompt_template=prompt_template)\n",
    "\n",
    "                # apply prompt template\n",
    "                if prompt_template:\n",
    "                    prompt = format_string(prompt_template, prompt=prompt)\n",
    "\n",
    "                # set stop characters\n",
    "                stop = stop if stop else self.stop\n",
    "\n",
    "                # handle hf models\n",
    "                if self.is_hf():\n",
    "                    tokenizer = llm.llm.pipeline.tokenizer\n",
    "                    # FIX for #113/#114\n",
    "                    prompt = [{'role':'user', 'content':prompt}] if tokenizer.chat_template else prompt\n",
    "                    # Call HF pipeline directly instead of `invoke`\n",
    "                    # since LangChain is not passing along stop_strings\n",
    "                    # parameter to pipeline\n",
    "                    if 'max_tokens' in kwargs:\n",
    "                        kwargs['max_new_tokens'] = kwargs['max_tokens']\n",
    "                        del kwargs['max_tokens']\n",
    "                    res = llm.llm.pipeline(prompt,\n",
    "                                           stop_strings=stop if stop else None,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           **kwargs)[0]['generated_text']\n",
    "\n",
    "                # handle other models (e.g., llama_cpp, LLMs served through APIs)\n",
    "                else:\n",
    "                    res = llm.invoke(prompt, stop=stop, **kwargs)\n",
    "        return res.content if isinstance(res, AIMessage) else res\n",
    "\n",
    "\n",
    "\n",
    "    def load_chatbot(self):\n",
    "        \"\"\"\n",
    "        Prepares and loads a `langchain.chains.ConversationChain` instance\n",
    "        \"\"\"\n",
    "        if self.chatbot is None:\n",
    "            from langchain.chains import ConversationChain\n",
    "            from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "            llm = self.load_llm()\n",
    "\n",
    "            memory = ConversationBufferMemory(\n",
    "                memory_key=\"history\", return_messages=True\n",
    "            )\n",
    "            self.chatbot = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "        return self.chatbot\n",
    "\n",
    "\n",
    "    def query(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform a semantic search of vectorstore.\n",
    "        \"\"\"\n",
    "        return self.semantic_search(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def semantic_search(self,\n",
    "              query:str, # query string\n",
    "              limit:int = 4, # max number of results to return\n",
    "              score_threshold:float=0.0, # minimum score for document to be considered as answer source\n",
    "              filters:Optional[Dict[str, str]] = None, # filter sources by metadata values (e.g., {'table':True})\n",
    "              where_document:Optional[Any] = None, # If `store_type` is `dense, filter sources by document content \n",
    "              folders:Optional[list]=None, # folders to search (needed because LangChain does not forward \"where\" parameter)\n",
    "              **kwargs):\n",
    "        \"\"\"\n",
    "        Perform a semantic search of the vector DB. Alias for `LLM.query`.\n",
    "\n",
    "        The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
    "        If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
    "        to filter results.\n",
    "        If `LLM.store_stype` is 'sparse', then `where_document` should be a a boolean search string  to filter query in Lucne syntax.\n",
    "        \"\"\"\n",
    "        store = self.load_vectorstore()\n",
    "        if folders:\n",
    "            folders = [folders] if isinstance(folders, str) else folders\n",
    "            # This is needed because only the where argument supports the $like operator\n",
    "            # and Langchain does not properly forward the where parameter to Chroma\n",
    "            n_candidates = store.get_size() if store.get_size() < 10000 else 10000\n",
    "            results = store.semantic_search(query, \n",
    "                                            filters=filters,\n",
    "                                            where_document=where_document,\n",
    "                                            limit = n_candidates, **kwargs)\n",
    "            # Handle path separator differences between Windows and Unix\n",
    "            if os.name == 'nt':  # Windows\n",
    "                # Normalize paths for case-insensitive comparison on Windows\n",
    "                normalized_folders = [os.path.normpath(f).lower().replace('\\\\', '/') for f in folders]\n",
    "                results = [d for d in results if any(os.path.normpath(d.metadata['source']).lower().replace('\\\\', '/').startswith(nf) for nf in normalized_folders)]\n",
    "            else:\n",
    "                # On Unix systems, use direct path comparison\n",
    "                results = [d for d in results if any(d.metadata['source'].startswith(f) for f in folders)]\n",
    "            results = results[:limit]\n",
    "            \n",
    "        else:\n",
    "            results = store.semantic_search(query, \n",
    "                                            filters=filters,\n",
    "                                            where_document=where_document,\n",
    "                                            limit = limit, **kwargs)\n",
    "\n",
    "        return [d for d in results if d.metadata['score'] >= score_threshold]\n",
    "\n",
    "\n",
    "    def _ask(self,\n",
    "            question: str, # question as sting\n",
    "            contexts:Optional[list]=None, # optional lists of contexts to answer question. If None, retrieve from vectordb.\n",
    "            qa_template=DEFAULT_QA_PROMPT, # question-answering prompt template to tuse\n",
    "            filters:Optional[Dict[str, str]] = None, # filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True})\n",
    "            where_document:Optional[Dict[str, str]] = None, # filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
    "            folders:Optional[list]=None, # folders to search (needed because LangChain does not forward \"where\" parameter)\n",
    "            limit:Optional[int]=None, # Number of sources to consider.  If None, use `LLM.rag_num_source_docs`.\n",
    "            score_threshold:Optional[float]=None, # minimum similarity score of source. If None, use `LLM.rag_score_threshold`.\n",
    "            table_k:int=1, # maximum number of tables to consider when generating answer\n",
    "            table_score_threshold:float=0.35, # minimum similarity score for table to be considered in answer\n",
    "             **kwargs):\n",
    "        \"\"\"\n",
    "        Answer a question based on source documents fed to the `LLM.ingest` method.\n",
    "        Extra keyword arguments are sent directly to `LLM.prompt`.\n",
    "        Returns a dictionary with keys: `answer`, `source_documents`, `question`\n",
    "        \"\"\"\n",
    "\n",
    "        if not contexts:\n",
    "            # query the vector db\n",
    "            docs = self.semantic_search(question, filters=filters, where_document=where_document, folders=folders,\n",
    "                              limit=limit if limit else self.rag_num_source_docs,\n",
    "                              score_threshold=score_threshold if score_threshold else self.rag_score_threshold)\n",
    "            if table_k>0:\n",
    "                table_filters = filters.copy() if filters else {}\n",
    "                table_filters = dict(table_filters, table=True)\n",
    "                table_docs = self.semantic_search(f'{question} (table)', \n",
    "                                        filters=table_filters, \n",
    "                                        where_document=where_document,\n",
    "                                        folders=folders,\n",
    "                                        limit=table_k,\n",
    "                                        score_threshold=table_score_threshold)\n",
    "                if table_docs:\n",
    "                    docs.extend(table_docs[:limit])\n",
    "            context = '\\n\\n'.join([d.page_content for d in docs])\n",
    "        else:\n",
    "            docs = [Document(page_content=c, metadata={'source':'<SUBANSWER>'}) for c in contexts]\n",
    "            context = \"\\n\\n\".join(contexts)\n",
    "\n",
    "        # setup prompt\n",
    "        prompt = format_string(qa_template,\n",
    "                                 question=question,\n",
    "                                 context = context)\n",
    "\n",
    "        # prompt LLM\n",
    "        answer = self.prompt(prompt,**kwargs)\n",
    "\n",
    "        # return answer\n",
    "        res = {}\n",
    "        res['question'] = question\n",
    "        res['answer'] = answer\n",
    "        res['source_documents'] = docs\n",
    "        return res\n",
    "\n",
    "\n",
    "    def ask(self,\n",
    "            question: str, # question as sting\n",
    "            selfask:bool=False, # If True, use an agentic Self-Ask prompting strategy.\n",
    "            qa_template=DEFAULT_QA_PROMPT, # question-answering prompt template to tuse\n",
    "            filters:Optional[Dict[str, str]] = None, # filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True})\n",
    "            where_document:Optional[Dict[str, str]] = None, # filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
    "            folders:Optional[list]=None, # folders to search (needed because LangChain does not forward \"where\" parameter)\n",
    "            limit:Optional[int]=None, # Number of sources to consider.  If None, use `LLM.rag_num_source_docs`.\n",
    "            score_threshold:Optional[float]=None, # minimum similarity score of source. If None, use `LLM.rag_score_threshold`.\n",
    "            table_k:int=1, # maximum number of tables to consider when generating answer\n",
    "            table_score_threshold:float=0.35, # minimum similarity score for table to be considered in answer\n",
    "             **kwargs):\n",
    "        \"\"\"\n",
    "        Answer a question based on source documents fed to the `LLM.ingest` method.\n",
    "        Extra keyword arguments are sent directly to `LLM.prompt`.\n",
    "        Returns a dictionary with keys: `answer`, `source_documents`, `question`\n",
    "        \"\"\"\n",
    "\n",
    "        if selfask and helpers.needs_followup(question, self):\n",
    "            subquestions = helpers.decompose_question(question, self)\n",
    "            subanswers = []\n",
    "            sources = []\n",
    "            for q in subquestions:\n",
    "                res = self._ask(q, \n",
    "                                qa_template=qa_template, \n",
    "                                filters=filters,\n",
    "                                where_document=where_document,\n",
    "                                folders=folders,\n",
    "                                limit=limit, score_threshold=score_threshold,\n",
    "                                table_k=table_k, table_score_threshold=table_score_threshold,\n",
    "                                **kwargs) \n",
    "                subanswers.append(res['answer'])\n",
    "                for doc in res['source_documents']:\n",
    "                    doc.metadata = dict(doc.metadata, subquestion=q)\n",
    "                sources.extend(res['source_documents'])\n",
    "            res = self._ask(question=question,\n",
    "                            contexts=subanswers,\n",
    "                            qa_template=qa_template, \n",
    "                            filters = filters,\n",
    "                            where_document=where_document,\n",
    "                            folders=folders, **kwargs) \n",
    "            res['source_documents'] = sources\n",
    "            return res\n",
    "        else:       \n",
    "            res = self._ask(question=question,\n",
    "                            qa_template=qa_template, \n",
    "                            filters = filters,\n",
    "                            where_document=where_document,\n",
    "                            folders=folders,\n",
    "                            limit=limit, score_threshold=score_threshold,\n",
    "                            table_k=table_k, table_score_threshold=table_score_threshold,\n",
    "                            **kwargs)\n",
    "            return res\n",
    "\n",
    "\n",
    "    def chat(self, prompt: str, prompt_template=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Chat with LLM.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *question*: a question you want to ask\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = self.prompt_template if prompt_template is None else prompt_template\n",
    "        if prompt_template:\n",
    "            prompt = format_string(prompt_template, prompt=prompt)\n",
    "        chatbot = self.load_chatbot()\n",
    "        res = chatbot.invoke(prompt, **kwargs)\n",
    "        return res.get('response', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L328){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.download_model\n",
       "\n",
       ">      LLM.download_model (model_url:Optional[str]=None,\n",
       ">                          default_model:str='zephyr',\n",
       ">                          model_download_path:Optional[str]=None,\n",
       ">                          confirm:bool=True, ssl_verify:bool=True)\n",
       "\n",
       "*Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *model_url*: URL of model. If None, then use default_model.\n",
       "- *default_model*: One of {'mistral', 'zephyr', 'llama'}, where mistral is Mistral-Instruct-7B-v0.2, zephyr is Zephyr-7B-beta, and llama is Llama-3.1-8B.\n",
       "- *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
       "- *confirm*: whether or not to confirm with user before downloading\n",
       "- *ssl_verify*: If True, SSL certificates are verified.\n",
       "                You can set to False if corporate firewall gives you problems.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L328){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.download_model\n",
       "\n",
       ">      LLM.download_model (model_url:Optional[str]=None,\n",
       ">                          default_model:str='zephyr',\n",
       ">                          model_download_path:Optional[str]=None,\n",
       ">                          confirm:bool=True, ssl_verify:bool=True)\n",
       "\n",
       "*Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *model_url*: URL of model. If None, then use default_model.\n",
       "- *default_model*: One of {'mistral', 'zephyr', 'llama'}, where mistral is Mistral-Instruct-7B-v0.2, zephyr is Zephyr-7B-beta, and llama is Llama-3.1-8B.\n",
       "- *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
       "- *confirm*: whether or not to confirm with user before downloading\n",
       "- *ssl_verify*: If True, SSL certificates are verified.\n",
       "                You can set to False if corporate firewall gives you problems.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.download_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L359){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_llm\n",
       "\n",
       ">      LLM.load_llm ()\n",
       "\n",
       "*Loads the LLM from the model path.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L359){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_llm\n",
       "\n",
       ">      LLM.load_llm ()\n",
       "\n",
       "*Loads the LLM from the model path.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L284){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_vectorstore\n",
       "\n",
       ">      LLM.load_vectorstore ()\n",
       "\n",
       "*Get `VectorStore` instance.\n",
       "You can access the `langchain_chroma.Chroma` instance with `load_vectorstore().get_db()`.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L284){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_vectorstore\n",
       "\n",
       ">      LLM.load_vectorstore ()\n",
       "\n",
       "*Get `VectorStore` instance.\n",
       "You can access the `langchain_chroma.Chroma` instance with `load_vectorstore().get_db()`.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L574){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_chatbot\n",
       "\n",
       ">      LLM.load_chatbot ()\n",
       "\n",
       "*Prepares and loads a `langchain.chains.ConversationChain` instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L574){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_chatbot\n",
       "\n",
       ">      LLM.load_chatbot ()\n",
       "\n",
       "*Prepares and loads a `langchain.chains.ConversationChain` instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L593){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.query\n",
       "\n",
       ">      LLM.query (query:str, k:int=4, score_threshold:float=0.0,\n",
       ">                 filters:Optional[Dict[str,str]]=None,\n",
       ">                 where_document:Optional[Any]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB.\n",
       "\n",
       "The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
       "If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
       "to filter results.\n",
       "If `LLM.store_stype` is 'sparse', then `where_document` should be a a boolean search string  to filter query in Lucne syntax.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | query string |\n",
       "| k | int | 4 | max number of results to return |\n",
       "| score_threshold | float | 0.0 | minimum score for document to be considered as answer source |\n",
       "| filters | Optional | None | filter sources by metadata values (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | If `store_type` is `dense, filter sources by document content |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L593){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.query\n",
       "\n",
       ">      LLM.query (query:str, k:int=4, score_threshold:float=0.0,\n",
       ">                 filters:Optional[Dict[str,str]]=None,\n",
       ">                 where_document:Optional[Any]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB.\n",
       "\n",
       "The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
       "If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
       "to filter results.\n",
       "If `LLM.store_stype` is 'sparse', then `where_document` should be a a boolean search string  to filter query in Lucne syntax.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | query string |\n",
       "| k | int | 4 | max number of results to return |\n",
       "| score_threshold | float | 0.0 | minimum score for document to be considered as answer source |\n",
       "| filters | Optional | None | filter sources by metadata values (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | If `store_type` is `dense, filter sources by document content |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L516){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.prompt\n",
       "\n",
       ">      LLM.prompt (prompt:Union[str,List[Dict]],\n",
       ">                  output_parser:Optional[Any]=None,\n",
       ">                  image_path_or_url:Optional[str]=None,\n",
       ">                  prompt_template:Optional[str]=None, stop:list=[], **kwargs)\n",
       "\n",
       "*Send prompt to LLM to generate a response.\n",
       "Extra keyword arguments are sent directly to the model invocation.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *prompt*: The prompt to supply to the model.\n",
       "            Either a string or OpenAI-style list of dictionaries\n",
       "            representing messages (e.g., \"human\", \"system\").\n",
       "- *image_path_or_url*: Path or URL to an image file\n",
       "- *prompt_template*: Optional prompt template (must have a variable named \"prompt\").\n",
       "                     This value will override any `prompt_template` value supplied \n",
       "                     to `LLM` constructor.\n",
       "- *stop*: a list of strings to stop generation when encountered. \n",
       "          This value will override the `stop` parameter supplied to `LLM` constructor.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L516){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.prompt\n",
       "\n",
       ">      LLM.prompt (prompt:Union[str,List[Dict]],\n",
       ">                  output_parser:Optional[Any]=None,\n",
       ">                  image_path_or_url:Optional[str]=None,\n",
       ">                  prompt_template:Optional[str]=None, stop:list=[], **kwargs)\n",
       "\n",
       "*Send prompt to LLM to generate a response.\n",
       "Extra keyword arguments are sent directly to the model invocation.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *prompt*: The prompt to supply to the model.\n",
       "            Either a string or OpenAI-style list of dictionaries\n",
       "            representing messages (e.g., \"human\", \"system\").\n",
       "- *image_path_or_url*: Path or URL to an image file\n",
       "- *prompt_template*: Optional prompt template (must have a variable named \"prompt\").\n",
       "                     This value will override any `prompt_template` value supplied \n",
       "                     to `LLM` constructor.\n",
       "- *stop*: a list of strings to stop generation when encountered. \n",
       "          This value will override the `stop` parameter supplied to `LLM` constructor.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L578){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.pydantic_prompt\n",
       "\n",
       ">      LLM.pydantic_prompt (prompt:str, pydantic_model=None,\n",
       ">                           attempt_fix:bool=False, fix_llm=None, stop:list=[],\n",
       ">                           **kwargs)\n",
       "\n",
       "*Accept a prompt as string and Pydantic model describing the desired output.\n",
       "Output will be a Pydantic object in the requested format.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *prompt*: The prompt to supply to the model.\n",
       "            Either a string or OpenAI-style list of dictionaries\n",
       "            representing messages (e.g., \"human\", \"system\").\n",
       "- *pydantic_model*: A Pydanatic model (sublass of `pydantic.BaseModel` that describes the desired output format.\n",
       "                   Output will be a desired Pydantic object.\n",
       "                   If `put_format=None`, then output is a string.\n",
       "- *attempt_fix*: Use an LLM call in attempt to correct malformed or incomplete outputs\n",
       "- *fix_llm*:  LLM to use for fixing (e.g., `langchain_openai.ChatOpenAI()`). If `None`, then existing `LLM.llm` used.\n",
       "- *stop*: a list of strings to stop generation when encountered. \n",
       "          This value will override the `stop` parameter supplied to `LLM` constructor.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L578){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.pydantic_prompt\n",
       "\n",
       ">      LLM.pydantic_prompt (prompt:str, pydantic_model=None,\n",
       ">                           attempt_fix:bool=False, fix_llm=None, stop:list=[],\n",
       ">                           **kwargs)\n",
       "\n",
       "*Accept a prompt as string and Pydantic model describing the desired output.\n",
       "Output will be a Pydantic object in the requested format.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *prompt*: The prompt to supply to the model.\n",
       "            Either a string or OpenAI-style list of dictionaries\n",
       "            representing messages (e.g., \"human\", \"system\").\n",
       "- *pydantic_model*: A Pydanatic model (sublass of `pydantic.BaseModel` that describes the desired output format.\n",
       "                   Output will be a desired Pydantic object.\n",
       "                   If `put_format=None`, then output is a string.\n",
       "- *attempt_fix*: Use an LLM call in attempt to correct malformed or incomplete outputs\n",
       "- *fix_llm*:  LLM to use for fixing (e.g., `langchain_openai.ChatOpenAI()`). If `None`, then existing `LLM.llm` used.\n",
       "- *stop*: a list of strings to stop generation when encountered. \n",
       "          This value will override the `stop` parameter supplied to `LLM` constructor.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.pydantic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L321){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ingest\n",
       "\n",
       ">      LLM.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                  chunk_overlap:int=50, ignore_fn:Optional[Callable]=None,\n",
       ">                  batch_size:int=1000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_folder` into vector database.\n",
       "Previously-ingested documents are ignored.\n",
       "Extra kwargs fed to `load_single_document` and/or `load_docments`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing documents |\n",
       "| chunk_size | int | 500 | text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | callable that accepts the file path and returns True for ignored files |\n",
       "| batch_size | int | 1000 | batch size used when processing documents(e.g, creating embeddings). |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L321){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ingest\n",
       "\n",
       ">      LLM.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                  chunk_overlap:int=50, ignore_fn:Optional[Callable]=None,\n",
       ">                  batch_size:int=1000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_folder` into vector database.\n",
       "Previously-ingested documents are ignored.\n",
       "Extra kwargs fed to `load_single_document` and/or `load_docments`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing documents |\n",
       "| chunk_size | int | 500 | text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | callable that accepts the file path and returns True for ignored files |\n",
       "| batch_size | int | 1000 | batch size used when processing documents(e.g, creating embeddings). |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L670){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ask\n",
       "\n",
       ">      LLM.ask (question:str, selfask:bool=False, qa_template='\"Use the\n",
       ">               following pieces of context delimited by three backticks to\n",
       ">               answer the question at the end. If you don\\'t know the answer,\n",
       ">               just say that you don\\'t know, don\\'t try to make up an\n",
       ">               answer.\\n\\n```{context}```\\n\\nQuestion: {question}\\nHelpful\n",
       ">               Answer:', filters:Optional[Dict[str,str]]=None,\n",
       ">               where_document:Optional[Dict[str,str]]=None,\n",
       ">               k:Optional[int]=None, score_threshold:Optional[float]=None,\n",
       ">               table_k:int=1, table_score_threshold:float=0.35, **kwargs)\n",
       "\n",
       "*Answer a question based on source documents fed to the `LLM.ingest` method.\n",
       "Extra keyword arguments are sent directly to `LLM.prompt`.\n",
       "Returns a dictionary with keys: `answer`, `source_documents`, `question`*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| question | str |  | question as sting |\n",
       "| selfask | bool | False | If True, use an agentic Self-Ask prompting strategy. |\n",
       "| qa_template | str | \"Use the following pieces of context delimited by three backticks to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.<br><br>```{context}```<br><br>Question: {question}<br>Helpful Answer: | question-answering prompt template to tuse |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}) |\n",
       "| k | Optional | None | Number of sources to consider.  If None, use `LLM.rag_num_source_docs`. |\n",
       "| score_threshold | Optional | None | minimum similarity score of source. If None, use `LLM.rag_score_threshold`. |\n",
       "| table_k | int | 1 | maximum number of tables to consider when generating answer |\n",
       "| table_score_threshold | float | 0.35 | minimum similarity score for table to be considered in answer |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L670){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ask\n",
       "\n",
       ">      LLM.ask (question:str, selfask:bool=False, qa_template='\"Use the\n",
       ">               following pieces of context delimited by three backticks to\n",
       ">               answer the question at the end. If you don\\'t know the answer,\n",
       ">               just say that you don\\'t know, don\\'t try to make up an\n",
       ">               answer.\\n\\n```{context}```\\n\\nQuestion: {question}\\nHelpful\n",
       ">               Answer:', filters:Optional[Dict[str,str]]=None,\n",
       ">               where_document:Optional[Dict[str,str]]=None,\n",
       ">               k:Optional[int]=None, score_threshold:Optional[float]=None,\n",
       ">               table_k:int=1, table_score_threshold:float=0.35, **kwargs)\n",
       "\n",
       "*Answer a question based on source documents fed to the `LLM.ingest` method.\n",
       "Extra keyword arguments are sent directly to `LLM.prompt`.\n",
       "Returns a dictionary with keys: `answer`, `source_documents`, `question`*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| question | str |  | question as sting |\n",
       "| selfask | bool | False | If True, use an agentic Self-Ask prompting strategy. |\n",
       "| qa_template | str | \"Use the following pieces of context delimited by three backticks to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.<br><br>```{context}```<br><br>Question: {question}<br>Helpful Answer: | question-answering prompt template to tuse |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}) |\n",
       "| k | Optional | None | Number of sources to consider.  If None, use `LLM.rag_num_source_docs`. |\n",
       "| score_threshold | Optional | None | minimum similarity score of source. If None, use `LLM.rag_score_threshold`. |\n",
       "| table_k | int | 1 | maximum number of tables to consider when generating answer |\n",
       "| table_score_threshold | float | 0.35 | minimum similarity score for table to be considered in answer |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L718){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.chat\n",
       "\n",
       ">      LLM.chat (prompt:str, prompt_template=None, **kwargs)\n",
       "\n",
       "*Chat with LLM.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *question*: a question you want to ask*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L718){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.chat\n",
       "\n",
       ">      LLM.chat (prompt:str, prompt_template=None, **kwargs)\n",
       "\n",
       "*Chat with LLM.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *question*: a question you want to ask*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "We'll use a small 3B-parameter model here for testing purposes. The vector database is stored under `~/onprem_data` by default. In this example, we will store the vector store in temporary folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "vectordb_path = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (3904) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "url = 'https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf'\n",
    "llm = LLM(model_url=url,\n",
    "          prompt_template = \"<|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\", verbose=False, confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "assert os.path.isfile(\n",
    "    os.path.join(U.get_datadir(), os.path.basename(url))\n",
    "), \"missing model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"List three cute names for a cat.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Luna - this name means \"moon\" in Latin and is perfect for a cat with soft, moon-like fur or bright green eyes that seem to glow like the full moon.\n",
      "\n",
      "2. Willow - named after the delicate branches of a willow tree, this name would suit a sweet, gentle kitty who loves to snuggle and purr contentedly in your lap.\n",
      "\n",
      "3. Marshmallow - if you have a fluffy cat with a round tummy and a plump body, why not call her Marshmallow? This adorable name is sure to melt your heart as soon as you see her cute little face."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to existing vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from ./sample_data/1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 new documents from ./sample_data/1/\n",
      "Split into 41 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "llm.ingest(\"./tests/sample_data/ktrain_paper/\", chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ktrain is a Python library for machine learning that aims to provide a simple and unified interface for easily executing the three main steps of the machine learning process - preparing data, training models, and evaluating results - regardless of the type of data being used (such as text, images, or graphs). It is designed to help beginners and domain experts with limited programming or data science experience to build sophisticated machine learning models with minimal coding, while also serving as a useful toolbox for more experienced users. Ktrain follows a standard template for supervised learning tasks and supports custom models and data formats. It is licensed under the Apache license and can be found on GitHub at https://github.com/amaiya/ktrain. The text material mentions that ktrain was inspired by other low-code (and no-code) open-source ML libraries such as fastai and ludwig, and aims to further democratize machine learning by making it more accessible to a wider range of users.\n",
      "\n",
      "References:\n",
      "\n",
      "\n",
      "\n",
      "1.> /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\n",
      "transferred to, and executed on new data in a production environment.\n",
      "ktrain is a Python library for machine learning with the goal of presenting a simple,\n",
      "uniﬁed interface to easily perform the above steps regardless of the type of data (e.g., text\n",
      "vs. images vs. graphs). Moreover, each of the three steps above can be accomplished in\n",
      "©2022 Arun S. Maiya.\n",
      "License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are\n",
      "\n",
      "2.> /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\n",
      "custom models and data formats, as well. Inspired by other low-code (and no-code) open-\n",
      "source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig (Molino et al.,\n",
      "2019), ktrain is intended to help further democratize machine learning by enabling begin-\n",
      "ners and domain experts with minimal programming or data science experience to build\n",
      "sophisticated machine learning models with minimal coding. It is also a useful toolbox for\n",
      "\n",
      "3.> /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\n",
      "ktrain.Learner instance, which is an abstraction to facilitate training.\n",
      "1. https://www.fast.ai/2018/07/16/auto-ml2/\n",
      "2\n",
      "\n",
      "4.> /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\n",
      "Apache license, and available on GitHub at: https://github.com/amaiya/ktrain.\n",
      "2. Building Models\n",
      "Supervised learning tasks in ktrain follow a standard, easy-to-use template.\n",
      "STEP 1: Load and Preprocess Data. This step involves loading data from diﬀerent\n",
      "sources and preprocessing it in a way that is expected by the model. In the case of text,\n",
      "this may involve language-speciﬁc preprocessing (e.g., tokenization). In the case of images,\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "question = \"\"\"What is ktrain?\"\"\"\n",
    "result = llm.ask(question)\n",
    "print(\"\\n\\nReferences:\\n\\n\")\n",
    "for i, document in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\n{i+1}.> \" + document.metadata[\"source\"] + \":\")\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
