{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction\n",
    "\n",
    "> Pipline for information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union, Callable\n",
    "import pandas as pd\n",
    "from onprem.utils import segment\n",
    "\n",
    "from onprem.ingest import load_single_document\n",
    "\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        prompt_template: Optional[str] = None,              \n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `Extractor` applies a given prompt to each sentence or paragraph in a document and returns the results.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *llm*: An `onprem.LLM` object\n",
    "        - *prompt_template*: A model specific prompt_template with a single placeholder named \"{prompt}\".\n",
    "                             All prompts (e.g., Map-Reduce prompts) are wrapped within this prompt.\n",
    "                             If supplied, overrides the `prompt_template` supplied to the `LLM` constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template if prompt_template is not None else llm.prompt_template\n",
    "\n",
    "\n",
    "\n",
    "    def apply(self,\n",
    "              ex_prompt_template:str, \n",
    "              fpath: Optional[str] = None,\n",
    "              content: Optional[str] = None,\n",
    "              unit:str='paragraph',\n",
    "              filter_fn: Optional[Callable] = None,\n",
    "              pdf_pages:List[int]=[],\n",
    "              maxchars = 2048,\n",
    "              stop:list=[]\n",
    "             ):\n",
    "        \"\"\"\n",
    "        Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
    "        Results are stored in a `pandas.Dataframe`.\n",
    "\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *ex_prompt_template*: A prompt to apply to each `unit` in document. Should have a single variable, `{text}`.\n",
    "                               Example: `\"Extract universities from the following text delimited by ###:\\n\\n###{text}###\"`\n",
    "        - *fpath*: A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`.\n",
    "        - *content*: Text content of a document of interest.  Mutually-exclusive with `fpath`.\n",
    "        - *unit*: One of {'sentence', 'paragraph'}. \n",
    "        - *filter_fn*: A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it.\n",
    "                       If `filter_fn` returns False, the text is ignored and excluded from results.\n",
    "        - *pdf_pages*: If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages`.\n",
    "                       Page numbers start with 1, not 0 (e.g., `pdf_pages=[1,2,3]` for first three pages).\n",
    "                       If list is empty, prompt is applied to every page.\n",
    "        - *maxchars*: units (i.e., paragraphs or sentences) larger than `maxhcars` split.\n",
    "        - *stop*: list of characters to trigger the LLM to stop generating.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        - pd.Dataframe: a Dataframe with results\n",
    "        \"\"\"\n",
    "        if not(bool(fpath) != bool(content)):\n",
    "            raise ValueError('Either fpath argument or content argument must be supplied but not both.')\n",
    "            \n",
    "        # setup extraction prompt\n",
    "        extraction_prompt = ex_prompt_template if self.prompt_template is None else self.prompt_template.format(**{'prompt': ex_prompt_template})   \n",
    "\n",
    "        # extract text\n",
    "        if not content:\n",
    "            if not os.path.isfile(fpath):\n",
    "                raise ValueError(f'{fpath} is not a file')\n",
    "            docs = load_single_document(fpath)\n",
    "            ext = \".\" + fpath.rsplit(\".\", 1)[-1].lower()\n",
    "            if ext == '.pdf' and pdf_pages:\n",
    "                docs = [doc for i,doc in enumerate(docs) if i+1 in pdf_pages]\n",
    "            content = '\\n\\n'.join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # segment\n",
    "        chunks = segment(content)\n",
    "        extractions = []\n",
    "        texts = []\n",
    "        for chunk in chunks:\n",
    "            if filter_fn and not filter_fn(chunk): continue\n",
    "            prompt = extraction_prompt.format(text=chunk)\n",
    "            extractions.append(self.llm.prompt(prompt, stop=stop))\n",
    "            texts.append(chunk)\n",
    "        df = pd.DataFrame({'Extractions':extractions, 'Texts':texts})\n",
    "        return df\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/extractor.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Extractor.apply\n",
       "\n",
       ">      Extractor.apply (ex_prompt_template:str, fpath:Optional[str]=None,\n",
       ">                       content:Optional[str]=None, unit:str='paragraph',\n",
       ">                       filter_fn:Optional[Callable]=None,\n",
       ">                       pdf_pages:List[int]=[], maxchars=2048, stop:list=[])\n",
       "\n",
       "*Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
       "Results are stored in a `pandas.Dataframe`.\n",
       "\n",
       "        **Args:**\n",
       "\n",
       "        - *ex_prompt_template*: A prompt to apply to each `unit` in document. Should have a single variable, `{text}`.\n",
       "                               Example: `\"Extract universities from the following text delimited by ###:\n",
       "\n",
       "###{text}###\"`\n",
       "        - *fpath*: A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`.\n",
       "        - *content*: Text content of a document of interest.  Mutually-exclusive with `fpath`.\n",
       "        - *unit*: One of {'sentence', 'paragraph'}. \n",
       "        - *filter_fn*: A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it.\n",
       "                       If `filter_fn` returns False, the text is ignored and excluded from results.\n",
       "        - *pdf_pages*: If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages`.\n",
       "                       Page numbers start with 1, not 0 (e.g., `pdf_pages=[1,2,3]` for first three pages).\n",
       "                       If list is empty, prompt is applied to every page.\n",
       "        - *maxchars*: units (i.e., paragraphs or sentences) larger than `maxhcars` split.\n",
       "        - *stop*: list of characters to trigger the LLM to stop generating.\n",
       "\n",
       "        **Returns:**\n",
       "\n",
       "        - pd.Dataframe: a Dataframe with results*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/extractor.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Extractor.apply\n",
       "\n",
       ">      Extractor.apply (ex_prompt_template:str, fpath:Optional[str]=None,\n",
       ">                       content:Optional[str]=None, unit:str='paragraph',\n",
       ">                       filter_fn:Optional[Callable]=None,\n",
       ">                       pdf_pages:List[int]=[], maxchars=2048, stop:list=[])\n",
       "\n",
       "*Apply the prompt to each `unit` (where a \"unit\" is either a paragraph or sentence) optionally filtered by `filter_fn`.\n",
       "Results are stored in a `pandas.Dataframe`.\n",
       "\n",
       "        **Args:**\n",
       "\n",
       "        - *ex_prompt_template*: A prompt to apply to each `unit` in document. Should have a single variable, `{text}`.\n",
       "                               Example: `\"Extract universities from the following text delimited by ###:\n",
       "\n",
       "###{text}###\"`\n",
       "        - *fpath*: A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`.\n",
       "        - *content*: Text content of a document of interest.  Mutually-exclusive with `fpath`.\n",
       "        - *unit*: One of {'sentence', 'paragraph'}. \n",
       "        - *filter_fn*: A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it.\n",
       "                       If `filter_fn` returns False, the text is ignored and excluded from results.\n",
       "        - *pdf_pages*: If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages`.\n",
       "                       Page numbers start with 1, not 0 (e.g., `pdf_pages=[1,2,3]` for first three pages).\n",
       "                       If list is empty, prompt is applied to every page.\n",
       "        - *maxchars*: units (i.e., paragraphs or sentences) larger than `maxhcars` split.\n",
       "        - *stop*: list of characters to trigger the LLM to stop generating.\n",
       "\n",
       "        **Returns:**\n",
       "\n",
       "        - pd.Dataframe: a Dataframe with results*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Extractor.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "from onprem.pipelines import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaiya/mambaforge/envs/llm/lib/python3.9/site-packages/langchain_core/language_models/llms.py:239: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt_template = \"<s>[INST] {prompt} [/INST]\" # prompt template for Mistral\n",
    "llm = LLM(model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf', \n",
    "          n_gpu_layers=33,  # change based on your system\n",
    "          verbose=False, mute_stream=True, \n",
    "          prompt_template=prompt_template)\n",
    "extractor = Extractor(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Extract citations from the following sentences. Return #NA# if there are no citations in the text. Here are some examples:\n",
    "\n",
    "[SENTENCE]:pretrained BERT text classifier (Devlin et al., 2018), models for sequence tagging (Lample et al., 2016)\n",
    "[CITATIONS]:(Devlin et al., 2018), (Lample et al., 2016)\n",
    "[SENTENCE]:Machine learning (ML) is a powerful tool.\n",
    "[CITATIONS]:#NA#\n",
    "[SENTENCE]:Following inspiration from a blog post by Rachel Thomas of fast.ai (Howard and Gugger, 2020), we refer to this as Augmented Machine Learning or AugML\n",
    "[CITATIONS]:(Howard and Gugger, 2020)\n",
    "[SENTENCE]:{text}\n",
    "[CITATIONS]:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "content = \"\"\"\n",
    "For instance, the fit_onecycle method employs a 1cycle policy (Smith, 2018). \n",
    "\"\"\"\n",
    "df = extractor.apply(prompt, content=content, stop=['\\n'])\n",
    "assert df['Extractions'][0].strip().startswith('(Smith, 2018)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "content =\"\"\"In the case of text, this may involve language-specific preprocessing (e.g., tokenization).\"\"\"\n",
    "df = extractor.apply(prompt, content=content, stop=['\\n'])\n",
    "assert df['Extractions'][0].strip().startswith('#NA#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
