{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3eb2e57",
   "metadata": {},
   "source": [
    "# ingest.stores.dense\n",
    "\n",
    "> Data store for dense vector represenations of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1595cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from onprem.ingest.stores.base import VectorStore\n",
    "\n",
    "\n",
    "class DenseStore(VectorStore):\n",
    "    \"\"\"\n",
    "    A factory for built-in DenseStore instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        if type(self) is DenseStore:\n",
    "            raise TypeError(\"Use the DenseStore.create() method instead of instantiating DenseStore directly.\")\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, persist_location=None, kind=None, **kwargs) -> 'DenseStore':\n",
    "        \"\"\"\n",
    "        Factory method to construct a `DenseStore` instance. \n",
    "        \n",
    "        Extra kwargs passed to object instantiation.\n",
    "        \n",
    "        Args:\n",
    "            persist_location: where the vector database is stored\n",
    "            kind: one of {chroma, elasticsearch}\n",
    "\n",
    "        Returns:\n",
    "            DenseStore instance\n",
    "        \"\"\"\n",
    "        \n",
    "        kind = 'chroma' if not kind else kind\n",
    "        \n",
    "        if kind == 'chroma':\n",
    "            return ChromaStore(persist_location=persist_location, **kwargs)\n",
    "        elif kind == 'elasticsearch':\n",
    "            return ElasticsearchDenseStore(persist_location=persist_location, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown DenseStore type: {kind}. Supported types: 'chroma', 'elasticsearch'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44b4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.create\n",
       "\n",
       ">      DenseStore.create (persist_location=None, kind=None, **kwargs)\n",
       "\n",
       "*Factory method to construct a `DenseStore` instance. \n",
       "\n",
       "Extra kwargs passed to object instantiation.\n",
       "\n",
       "Args:\n",
       "    persist_location: where the vector database is stored\n",
       "    kind: one of {chroma}\n",
       "\n",
       "Returns:\n",
       "    DenseStore instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseStore.create\n",
       "\n",
       ">      DenseStore.create (persist_location=None, kind=None, **kwargs)\n",
       "\n",
       "*Factory method to construct a `DenseStore` instance. \n",
       "\n",
       "Extra kwargs passed to object instantiation.\n",
       "\n",
       "Args:\n",
       "    persist_location: where the vector database is stored\n",
       "    kind: one of {chroma}\n",
       "\n",
       "Returns:\n",
       "    DenseStore instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseStore.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53671de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from typing import List, Optional, Callable, Dict, Sequence\n",
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from onprem.ingest.helpers import doc_from_dict, dict_from_doc\n",
    "from onprem.utils import get_datadir, DEFAULT_DB\n",
    "from onprem.ingest.base import batchify_chunks, process_folder, does_vectorstore_exist\n",
    "from onprem.ingest.base import CHROMA_MAX\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    CHROMA_INSTALLED = True\n",
    "except ImportError:\n",
    "    CHROMA_INSTALLED = False\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "COLLECTION_NAME = \"onprem_chroma\"\n",
    "\n",
    "\n",
    "class ChromaStore(DenseStore):\n",
    "    \"\"\"\n",
    "    A dense vector store based on Chroma.   \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        persist_location: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
    "\n",
    "        **Args**:\n",
    "\n",
    "          - *persist_location*: Path to vector database (created if it doesn't exist).\n",
    "                                 Default is `onprem_data/vectordb` in user's home directory.\n",
    "          - *embedding_model*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                       embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "\n",
    "\n",
    "        **Returns**: `None`\n",
    "        \"\"\"\n",
    "        if not CHROMA_INSTALLED:\n",
    "            raise ImportError('Please install chroma packages: pip install onprem[chroma]')\n",
    "\n",
    "        from langchain_chroma import Chroma\n",
    "        import chromadb\n",
    "        from chromadb.config import Settings\n",
    "\n",
    "        self.persist_location = persist_location or os.path.join(\n",
    "            get_datadir(), DEFAULT_DB\n",
    "        )\n",
    "        self.init_embedding_model(**kwargs) # stored in self.embeddings\n",
    "\n",
    "        self.chroma_settings = Settings(\n",
    "            persist_directory=self.persist_location, anonymized_telemetry=False\n",
    "        )\n",
    "        self.chroma_client = chromadb.PersistentClient(\n",
    "            settings=self.chroma_settings, path=self.persist_location\n",
    "        )\n",
    "        return\n",
    "\n",
    "\n",
    "    def _convert_to_dict(self, raw_results):\n",
    "        \"\"\"\n",
    "        Convert raw results to dictionary\n",
    "        \"\"\"\n",
    "        ids = raw_results['ids']\n",
    "        texts = raw_results['documents']\n",
    "        metadatas = raw_results['metadatas']\n",
    "        results = []\n",
    "        for i, m in enumerate(metadatas):\n",
    "            m['page_content'] = texts[i]\n",
    "            m['id'] = ids[i]\n",
    "            results.append(m)\n",
    "        return results\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_chroma.Chroma` instance\n",
    "        \"\"\"\n",
    "        # Create ChromaDB settings\n",
    "        db = Chroma(\n",
    "            persist_directory=self.persist_location,\n",
    "            embedding_function=self.embeddings,\n",
    "            client_settings=self.chroma_settings,\n",
    "            client=self.chroma_client,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            collection_name=COLLECTION_NAME,\n",
    "        )\n",
    "        return db if does_vectorstore_exist(db) else None\n",
    "    \n",
    "    # get_db() method removed - use store methods instead\n",
    "    \n",
    "    #------------------------------\n",
    "    # overrides of abstract methods\n",
    "    # -----------------------------\n",
    "    \n",
    "\n",
    "    def exists(self):\n",
    "        return self.get_db() is not None\n",
    "\n",
    "\n",
    "    def add_documents(self, documents, batch_size:int=CHROMA_MAX):\n",
    "        \"\"\"\n",
    "        Stores instances of `langchain_core.documents.base.Document` in vectordb\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents, batch_size=batch_size)\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                db.add_documents(lst)\n",
    "        else:\n",
    "            chunk_batches, total_chunks = batchify_chunks(documents, batch_size)\n",
    "            print(\"Creating embeddings. May take some minutes...\")\n",
    "            db = None\n",
    "\n",
    "            for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                if not db:\n",
    "                    db = Chroma.from_documents(\n",
    "                        lst,\n",
    "                        self.embeddings,\n",
    "                        persist_directory=self.persist_location,\n",
    "                        client_settings=self.chroma_settings,\n",
    "                        client=self.chroma_client,\n",
    "                        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                        collection_name=COLLECTION_NAME,\n",
    "                    )\n",
    "                else:\n",
    "                    db.add_documents(lst)\n",
    "        return\n",
    "\n",
    "\n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a single document with ID, `id_to_delete`.\n",
    "        \"\"\"\n",
    "        if not self.exists(): return\n",
    "        id_to_delete = [id_to_delete] if not isinstance(id_to_delete, list) else id_to_delete\n",
    "        self.get_db().delete(ids=id_to_delete)\n",
    "        return\n",
    "\n",
    "    def remove_source(self, source:str):\n",
    "        \"\"\"\n",
    "        Deletes all documents in a Chroma collection whose `source` metadata field starts with the given prefix.\n",
    "        The `source` argument can either be a full path to a document or a prefix (e.g., parent folder).\n",
    "\n",
    "        **Args:**\n",
    "        - *source*: The source value or prefix\n",
    "\n",
    "        **Returns:**\n",
    "        - The number of documents deleted\n",
    "        \"\"\"\n",
    "        db = self.get_db()\n",
    "\n",
    "        # Only request metadata; ids are returned automatically\n",
    "        results = db.get(include=[\"metadatas\"])\n",
    "\n",
    "        to_delete = []\n",
    "        for doc_id, metadata in zip(results[\"ids\"], results[\"metadatas\"]):\n",
    "            if metadata and \"source\" in metadata:\n",
    "                if metadata[\"source\"].startswith(source):\n",
    "                    to_delete.append(doc_id)\n",
    "\n",
    "        if to_delete:\n",
    "            db.delete(ids=to_delete)\n",
    "            return len(to_delete)\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def update_documents(self,\n",
    "                         doc_dicts:dict, # dictionary with keys 'page_content', 'source', 'id', etc.\n",
    "                         **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Update a set of documents (doc in index with same ID will be over-written)\n",
    "        \"\"\"\n",
    "        self.check()\n",
    "        db = self.get_db()\n",
    "        docs = [doc_from_dict(d) for d in doc_dicts]\n",
    "        ids = [d['id'] for d in doc_dicts]\n",
    "        return db.update_documents(ids, docs)\n",
    "\n",
    "\n",
    "   \n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns all docs\n",
    "        \"\"\"\n",
    "        if not self.exists(): return []\n",
    "\n",
    "        raw_results =  self.get_db().get()\n",
    "        return self._convert_to_dict(raw_results)\n",
    "\n",
    "\n",
    "    def get_doc(self, id):\n",
    "        \"\"\"\n",
    "        Retrieve a record by ID\n",
    "        \"\"\"\n",
    "        if not self.exists(): return None\n",
    "        raw_results = self.get_db().get(ids=[id])\n",
    "        return self._convert_to_dict(raw_results)[0] if len(raw_results['ids']) > 0 else None\n",
    "\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get total number of records\n",
    "        \"\"\"\n",
    "        if not self.exists(): return 0\n",
    "        return len(self.get_db().get()['documents'])\n",
    "\n",
    "    \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Resets collection and removes and stored documents\n",
    "        \"\"\"\n",
    "        if not self.exists(): return True\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            msg = (\n",
    "                f\"You are about to remove all documents from the vector database.\"\n",
    "                + f\"(Original documents on file system will remain.) Are you sure?\"\n",
    "            )\n",
    "            shall = input(\"%s (Y/n) \" % msg) == \"Y\"\n",
    "        if shall:\n",
    "            self.get_db().reset_collection()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def search(self,\n",
    "              query:str, # query string\n",
    "              limit:int = 4, # max number of results to return\n",
    "              filters:Optional[Dict[str, str]] = None, # filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True})\n",
    "              where_document:Optional[Dict[str, str]] = None, # filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}),\n",
    "              return_dict:bool=True, # If True, return results as dictionaries. Otherwise, return LangChain Document objects.\n",
    "              **kwargs):\n",
    "        \"\"\"\n",
    "        Perform a semantic search of the vector DB. Returns results as dictionary by default.\n",
    "        \"\"\"\n",
    "        if not self.exists(): return []\n",
    "        db = self.get_db()\n",
    "        \n",
    "        # Convert standard dict filters to ChromaDB $and format when multiple keys present\n",
    "        chroma_filters = filters\n",
    "        if filters and len(filters) > 1:\n",
    "            chroma_filters = {\n",
    "                \"$and\": [{k: v} for k, v in filters.items()]\n",
    "            }\n",
    "        \n",
    "        results = db.similarity_search_with_score(query, \n",
    "                                                  filter=chroma_filters,\n",
    "                                                  where_document=where_document,\n",
    "                                                  k = limit, **kwargs)\n",
    "        if not results: return []\n",
    "        docs, scores = zip(*results)\n",
    "        for doc, score in zip(docs, scores):\n",
    "            simscore = 1 - score\n",
    "            doc.metadata[\"score\"] = 1-score\n",
    "        return {'hits' : [dict_from_doc(d) for d in docs], 'total_hits': len(docs)} if return_dict else docs      \n",
    "\n",
    "\n",
    "    def semantic_search(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform a semantic search of the vector DB. Returns results as LangChain Document objects.\n",
    "        \"\"\"\n",
    "        return self.search(*args, return_dict=False, **kwargs)\n",
    "    \n",
    "\n",
    "class ElasticsearchDenseStore(DenseStore):\n",
    "    \"\"\"\n",
    "    Elasticsearch store with dense vector search capabilities.\n",
    "    Extends DenseStore to provide Elasticsearch-based dense vector storage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                dense_vector_field: str = 'dense_vector',\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize Elasticsearch dense store.\n",
    "        \n",
    "        **Args:**\n",
    "        - *dense_vector_field*: field name for dense vectors (default: 'dense_vector')\n",
    "        - All other args are passed to ElasticsearchStore for the underlying implementation\n",
    "        \"\"\"\n",
    "        from .sparse import ElasticsearchSparseStore\n",
    "        \n",
    "        self.dense_vector_field = dense_vector_field\n",
    "        \n",
    "        # Create underlying ElasticsearchSparseStore instance\n",
    "        self.es_store = ElasticsearchSparseStore(**kwargs)\n",
    "        \n",
    "        # Set persist_location to match the underlying store\n",
    "        self.persist_location = self.es_store.persist_location\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.init_embedding_model(**kwargs)\n",
    "        \n",
    "        # Set up index with vector mappings\n",
    "        self._setup_vector_index()\n",
    "    \n",
    "    def _setup_vector_index(self):\n",
    "        \"\"\"Ensure the index has vector field mappings\"\"\"\n",
    "        # Check if index exists\n",
    "        if not self.es_store.es.indices.exists(index=self.es_store.index_name):\n",
    "            # Create index with vector mappings using custom field names from es_store\n",
    "            properties = {\n",
    "                # Essential fields for core functionality using custom field names and analyzer\n",
    "                self.es_store.content_field: {\"type\": \"text\", \"analyzer\": self.es_store.content_analyzer},\n",
    "                self.es_store.id_field: {\"type\": \"keyword\"},\n",
    "                \n",
    "                # Dense vector field for semantic search\n",
    "                self.dense_vector_field: {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"index\": True,\n",
    "                    \"similarity\": \"cosine\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add source field only if specified\n",
    "            if self.es_store.source_field:\n",
    "                properties[self.es_store.source_field] = {\"type\": \"keyword\"}\n",
    "                properties[f\"{self.es_store.source_field}_search\"] = {\"type\": \"text\", \"analyzer\": self.es_store.content_analyzer}\n",
    "            \n",
    "            mapping = {\n",
    "                \"mappings\": {\n",
    "                    \"properties\": properties\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            self.es_store.es.indices.create(index=self.es_store.index_name, body=mapping)\n",
    "        else:\n",
    "            # Index exists, update mapping to include vector field if needed\n",
    "            try:\n",
    "                mapping = {\n",
    "                    \"properties\": {\n",
    "                        self.dense_vector_field: {\n",
    "                            \"type\": \"dense_vector\",\n",
    "                            \"index\": True,\n",
    "                            \"similarity\": \"cosine\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                self.es_store.es.indices.put_mapping(index=self.es_store.index_name, body=mapping)\n",
    "            except Exception:\n",
    "                # Mapping update failed, but that's okay if field already exists\n",
    "                pass\n",
    "    \n",
    "    def doc2dict(self, doc: Document, include_vector: bool = True):\n",
    "        \"\"\"Convert LangChain Document to expected format with vector embedding\"\"\"\n",
    "        # Get the standard dict from ElasticsearchSparseStore\n",
    "        d = self.es_store.doc2dict(doc)\n",
    "        \n",
    "        # Add dense vector embedding if requested\n",
    "        if include_vector and hasattr(self, 'embeddings'):\n",
    "            try:\n",
    "                # Generate embedding for the document text\n",
    "                embedding = self.embeddings.embed_documents([doc.page_content])[0]\n",
    "                d[self.dense_vector_field] = embedding\n",
    "            except Exception as e:\n",
    "                # If embedding fails, continue without it\n",
    "                pass\n",
    "                \n",
    "        return d\n",
    "    \n",
    "    # Override abstract methods from DenseStore\n",
    "    def exists(self):\n",
    "        \"\"\"Returns True if the Elasticsearch index exists and has documents\"\"\"\n",
    "        return self.es_store.exists()\n",
    "    \n",
    "    def add_documents(self, documents, batch_size: int = 1000, **kwargs):\n",
    "        \"\"\"Add documents with vector embeddings to Elasticsearch\"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        \n",
    "        # Convert documents to dict format with vectors\n",
    "        actions = []\n",
    "        for doc in documents:\n",
    "            d = self.doc2dict(doc, include_vector=True)\n",
    "            action = {\n",
    "                \"_index\": self.es_store.index_name,\n",
    "                \"_id\": d[self.es_store.id_field],\n",
    "                \"_source\": d\n",
    "            }\n",
    "            actions.append(action)\n",
    "        \n",
    "        # Bulk index documents\n",
    "        if actions:\n",
    "            from elasticsearch.helpers import bulk\n",
    "            bulk(self.es_store.es, actions)\n",
    "            \n",
    "            # Force refresh to make documents immediately searchable\n",
    "            self.es_store.es.indices.refresh(index=self.es_store.index_name)\n",
    "    \n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"Remove a document by ID\"\"\"\n",
    "        return self.es_store.remove_document(id_to_delete)\n",
    "    \n",
    "    def remove_source(self, source: str):\n",
    "        \"\"\"Remove documents by source\"\"\"\n",
    "        return self.es_store.remove_source(source)\n",
    "    \n",
    "    def update_documents(self, doc_dicts: dict, **kwargs):\n",
    "        \"\"\"Update documents\"\"\"\n",
    "        return self.es_store.update_documents(doc_dicts, **kwargs)\n",
    "    \n",
    "    def get_all_docs(self):\n",
    "        \"\"\"Get all documents\"\"\"\n",
    "        return self.es_store.get_all_docs()\n",
    "    \n",
    "    def get_doc(self, id):\n",
    "        \"\"\"Get document by ID\"\"\"\n",
    "        return self.es_store.get_doc(id)\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"Get total number of documents\"\"\"\n",
    "        return self.es_store.get_size()\n",
    "    \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"Erase all documents\"\"\"\n",
    "        return self.es_store.erase(confirm=confirm)\n",
    "    \n",
    "    def search(self, query: str, limit: int = 4, **kwargs):\n",
    "        \"\"\"Perform semantic search (alias for semantic_search)\"\"\"\n",
    "        return self.semantic_search(query, limit=limit, return_dict=True, **kwargs)\n",
    "    \n",
    "    def semantic_search(self, query: str, limit: int = 4, \n",
    "                       filters: Optional[Dict[str, str]] = None,\n",
    "                       return_dict: bool = False, **kwargs):\n",
    "        \"\"\"Perform semantic search using dense vectors\"\"\"\n",
    "        if not hasattr(self, 'embeddings'):\n",
    "            raise ValueError(\"Embeddings not initialized. Cannot perform semantic search.\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Use script_score to get actual cosine similarity scores\n",
    "        script_query = {\n",
    "            \"size\": limit,\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},  # Start with all documents\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, '\" + self.dense_vector_field + \"') + 1.0\",\n",
    "                        \"params\": {\n",
    "                            \"query_vector\": query_embedding\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add filters if provided\n",
    "        if filters:\n",
    "            filter_clauses = []\n",
    "            for k, v in filters.items():\n",
    "                filter_type = \"terms\" if isinstance(v, list) else \"term\"\n",
    "                filter_clauses.append({filter_type: {k: v}})\n",
    "            script_query[\"query\"][\"script_score\"][\"query\"] = {\n",
    "                \"bool\": {\n",
    "                    \"filter\": filter_clauses\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Execute search\n",
    "        response = self.es_store.es.search(index=self.es_store.index_name, body=script_query)\n",
    "        \n",
    "        # Process results\n",
    "        hits = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            doc_dict = hit['_source'].copy()\n",
    "            # Convert script_score back to actual cosine similarity\n",
    "            # script_score returns (cosineSimilarity + 1.0), so subtract 1.0\n",
    "            elasticsearch_score = hit['_score']\n",
    "            cosine_similarity = elasticsearch_score - 1.0\n",
    "            doc_dict['score'] = cosine_similarity\n",
    "            hits.append(doc_dict)\n",
    "        \n",
    "        total_hits = response['hits']['total']['value']\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'hits': hits, 'total_hits': total_hits}\n",
    "        else:\n",
    "            from ..helpers import doc_from_dict\n",
    "            return [doc_from_dict(hit) for hit in hits]\n",
    "    \n",
    "    # get_db() method removed - use store methods instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e1d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.exists\n",
       "\n",
       ">      ChromaStore.exists ()\n",
       "\n",
       "*Returns True if vector store has been initialized and contains documents.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.exists\n",
       "\n",
       ">      ChromaStore.exists ()\n",
       "\n",
       "*Returns True if vector store has been initialized and contains documents.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3d3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L140){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.add_documents\n",
       "\n",
       ">      ChromaStore.add_documents (documents, batch_size:int=41000)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L140){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.add_documents\n",
       "\n",
       ">      ChromaStore.add_documents (documents, batch_size:int=41000)\n",
       "\n",
       "*Stores instances of `langchain_core.documents.base.Document` in vectordb*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e190b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.remove_document\n",
       "\n",
       ">      ChromaStore.remove_document (id_to_delete)\n",
       "\n",
       "*Remove a single document with ID, `id_to_delete`.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.remove_document\n",
       "\n",
       ">      ChromaStore.remove_document (id_to_delete)\n",
       "\n",
       "*Remove a single document with ID, `id_to_delete`.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.remove_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05dd5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L182){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.remove_source\n",
       "\n",
       ">      ChromaStore.remove_source (source:str)\n",
       "\n",
       "*Deletes all documents in a Chroma collection whose `source` metadata field starts with the given prefix.\n",
       "The `source` argument can either be a full path to a document or a prefix (e.g., parent folder).\n",
       "\n",
       "**Args:**\n",
       "- *source*: The source value or prefix\n",
       "\n",
       "**Returns:**\n",
       "- The number of documents deleted*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L182){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.remove_source\n",
       "\n",
       ">      ChromaStore.remove_source (source:str)\n",
       "\n",
       "*Deletes all documents in a Chroma collection whose `source` metadata field starts with the given prefix.\n",
       "The `source` argument can either be a full path to a document or a prefix (e.g., parent folder).\n",
       "\n",
       "**Args:**\n",
       "- *source*: The source value or prefix\n",
       "\n",
       "**Returns:**\n",
       "- The number of documents deleted*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.remove_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc071c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L210){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.update_documents\n",
       "\n",
       ">      ChromaStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update a set of documents (doc in index with same ID will be over-written)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| doc_dicts | dict | dictionary with keys 'page_content', 'source', 'id', etc. |\n",
       "| kwargs | VAR_KEYWORD |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L210){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.update_documents\n",
       "\n",
       ">      ChromaStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update a set of documents (doc in index with same ID will be over-written)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| doc_dicts | dict | dictionary with keys 'page_content', 'source', 'id', etc. |\n",
       "| kwargs | VAR_KEYWORD |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.update_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d4265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L225){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.get_all_docs\n",
       "\n",
       ">      ChromaStore.get_all_docs ()\n",
       "\n",
       "*Returns all docs*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L225){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.get_all_docs\n",
       "\n",
       ">      ChromaStore.get_all_docs ()\n",
       "\n",
       "*Returns all docs*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.get_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5af1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.get_doc\n",
       "\n",
       ">      ChromaStore.get_doc (id)\n",
       "\n",
       "*Retrieve a record by ID*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.get_doc\n",
       "\n",
       ">      ChromaStore.get_doc (id)\n",
       "\n",
       "*Retrieve a record by ID*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.get_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ca5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L244){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.get_size\n",
       "\n",
       ">      ChromaStore.get_size ()\n",
       "\n",
       "*Get total number of records*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L244){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.get_size\n",
       "\n",
       ">      ChromaStore.get_size ()\n",
       "\n",
       "*Get total number of records*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.get_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730c1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L252){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.erase\n",
       "\n",
       ">      ChromaStore.erase (confirm=True)\n",
       "\n",
       "*Resets collection and removes and stored documents*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L252){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.erase\n",
       "\n",
       ">      ChromaStore.erase (confirm=True)\n",
       "\n",
       "*Resets collection and removes and stored documents*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d441e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L270){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.query\n",
       "\n",
       ">      ChromaStore.query (query:str, k:int=4,\n",
       ">                         filters:Optional[Dict[str,str]]=None,\n",
       ">                         where_document:Optional[Dict[str,str]]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | query string |\n",
       "| k | int | 4 | max number of results to return |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L270){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.query\n",
       "\n",
       ">      ChromaStore.query (query:str, k:int=4,\n",
       ">                         filters:Optional[Dict[str,str]]=None,\n",
       ">                         where_document:Optional[Dict[str,str]]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | query string |\n",
       "| k | int | 4 | max number of results to return |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | filter sources by document content in Chroma syntax (e.g., {\"$contains\": \"Canada\"}) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd459a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L292){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.semantic_search\n",
       "\n",
       ">      ChromaStore.semantic_search (*args, **kwargs)\n",
       "\n",
       "*Semantic search is equivalent to queries in this class*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dense.py#L292){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChromaStore.semantic_search\n",
       "\n",
       ">      ChromaStore.semantic_search (*args, **kwargs)\n",
       "\n",
       "*Semantic search is equivalent to queries in this class*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc61b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChromaStore.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec97191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afebe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563794fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "tempfolder = temp_dir.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19942e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /tmp/tmpmftvr854\n",
      "Loading documents from tests/sample_data/ktrain_paper/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  7.85it/s]\n",
      "Processing and chunking 6 new documents: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 985.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 41 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "store = DenseStore.create(tempfolder)\n",
    "store.ingest(\"tests/sample_data/ktrain_paper/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774d532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.ChromaStore"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "type(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d193d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "store.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d64924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "a_document = store.get_all_docs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d300f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "store.remove_document(a_document['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f962bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "store.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcdb10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
