{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.dual\n",
    "\n",
    "> Dual vector store implementation for ingesting documents into both sparse and dense stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Callable, Dict, Sequence, Union\n",
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from onprem.ingest.base import VectorStore\n",
    "from onprem.ingest.stores.dense import DenseStore\n",
    "from onprem.ingest.stores.sparse import SparseStore, ElasticsearchStore\n",
    "from onprem.ingest.helpers import doc_from_dict\n",
    "\n",
    "class DualStore(VectorStore):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dense_kind:str='chroma',\n",
    "        dense_persist_directory: Optional[str] = None,\n",
    "        sparse_kind:str='whoosh',\n",
    "        sparse_persist_directory: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a dual vector store that manages both dense and sparse stores.\n",
    "        \n",
    "        **Args**:\n",
    "        \n",
    "          - *dense_persist_directory*: Path to dense vector database (created if it doesn't exist).\n",
    "          - *sparse_persist_directory*: Path to sparse vector database (created if it doesn't exist).\n",
    "          - *embedding_model_name*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        \"\"\"\n",
    "        self.init_embedding_model(**kwargs)  # stored in self.embeddings\n",
    "        \n",
    "        # Initialize both stores\n",
    "        self.dense_store = DenseStore.create(\n",
    "            kind=dense_kind,\n",
    "            persist_directory=dense_persist_directory,\n",
    "            embedding_model_name=kwargs.get('embedding_model_name'),\n",
    "            embedding_model_kwargs=kwargs.get('embedding_model_kwargs'),\n",
    "            embedding_encode_kwargs=kwargs.get('embedding_encode_kwargs')\n",
    "        )\n",
    "        self.sparse_store = SparseStore.create(\n",
    "            kind=sparse_kind,\n",
    "            persist_directory=sparse_persist_directory,\n",
    "            embedding_model_name=kwargs.get('embedding_model_name'),\n",
    "            embedding_model_kwargs=kwargs.get('embedding_model_kwargs'),\n",
    "            embedding_encode_kwargs=kwargs.get('embedding_encode_kwargs')\n",
    "        )\n",
    "        \n",
    "        # For compatibility with the VectorStore interface\n",
    "        self.persist_directory = dense_persist_directory\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, dense_kind='chroma', sparse_kind='whoosh', persist_directory=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Factory method to construct a DualStore instance.\n",
    "        \n",
    "        Args:\n",
    "            dense_kind: type of dense store ('chroma', 'elasticsearch')\n",
    "            sparse_kind: type of sparse store ('whoosh', 'elasticsearch')\n",
    "            persist_directory: base directory for stores or Elasticsearch URL\n",
    "            **kwargs: additional arguments passed to store initialization\n",
    "            \n",
    "        For traditional dual stores (different dense_kind and sparse_kind):\n",
    "            dense_persist_directory: directory for dense store  \n",
    "            sparse_persist_directory: directory for sparse store\n",
    "            \n",
    "        For unified Elasticsearch dual store (both kinds are 'elasticsearch'):\n",
    "            index_name: name of Elasticsearch index\n",
    "            dense_vector_field: field name for dense vectors\n",
    "            vector_dims: dimension of dense vectors\n",
    "            basic_auth: authentication credentials\n",
    "            verify_certs: SSL verification\n",
    "            ca_certs: CA certificate path\n",
    "            timeout: connection timeout\n",
    "            \n",
    "        Returns:\n",
    "            DualStore instance\n",
    "        \"\"\"\n",
    "        # If both dense and sparse are elasticsearch, use unified ElasticsearchDualStore\n",
    "        if dense_kind == 'elasticsearch' and sparse_kind == 'elasticsearch':\n",
    "            return ElasticsearchDualStore(persist_directory=persist_directory, **kwargs)\n",
    "        \n",
    "        # Otherwise, use traditional dual store approach\n",
    "        return cls(\n",
    "            dense_kind=dense_kind,\n",
    "            sparse_kind=sparse_kind,\n",
    "            dense_persist_directory=persist_directory,\n",
    "            sparse_persist_directory=persist_directory,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_dense_db(self):\n",
    "        \"\"\"\n",
    "        Returns the dense store's database instance.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_db()\n",
    "    \n",
    "    def get_sparse_db(self):\n",
    "        \"\"\"\n",
    "        Returns the sparse store's database instance.\n",
    "        \"\"\"\n",
    "        return self.sparse_store.get_db()\n",
    "\n",
    "\n",
    "    #------------------------------\n",
    "    # overrides of abstract methods\n",
    "    # -----------------------------\n",
    "  \n",
    "    \n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if either store exists.\n",
    "        \"\"\"\n",
    "        return self.dense_store.exists() or self.sparse_store.exists()\n",
    "    \n",
    "    def add_documents(self, documents: Sequence[Document], batch_size: int = 1000, **kwargs):\n",
    "        \"\"\"\n",
    "        Add documents to both dense and sparse stores.\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        # Add to dense store\n",
    "        self.dense_store.add_documents(documents, batch_size=batch_size, **kwargs)\n",
    "        \n",
    "        # Add to sparse store\n",
    "        self.sparse_store.add_documents(documents, **kwargs)\n",
    "\n",
    "   \n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a document from both stores.\n",
    "        \"\"\"\n",
    "        self.dense_store.remove_document(id_to_delete)\n",
    "        self.sparse_store.remove_document(id_to_delete)\n",
    "\n",
    "    def remove_source(self, source:str):\n",
    "        \"\"\"\n",
    "        Remove a document by source from both stores.\n",
    "\n",
    "        The `source` can either be the full path to a document\n",
    "        or a parent folder.  Returns the number of records deleted.\n",
    "        \"\"\"\n",
    "        num_deleted_1 = self.dense_store.remove_source(source)\n",
    "        num_deleted_2 = self.sparse_store.remove_source(source)\n",
    "        return num_deleted_1\n",
    "    \n",
    "\n",
    "    def update_documents(self, doc_dicts: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        Update documents in both stores.\n",
    "        \"\"\"\n",
    "        self.dense_store.update_documents(doc_dicts, **kwargs)\n",
    "        self.sparse_store.update_documents(doc_dicts, **kwargs)\n",
    "    \n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Get all documents from the dense store.\n",
    "        For simplicity, we only return documents from one store since they should be the same.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_all_docs()\n",
    "    \n",
    "    def get_doc(self, id):\n",
    "        \"\"\"\n",
    "        Get a document by ID from the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_doc(id)\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get the size of the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_size()\n",
    "    \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Erase both stores.\n",
    "        \"\"\"\n",
    "        dense_erased = self.dense_store.erase(confirm=confirm)\n",
    "        sparse_erased = self.sparse_store.erase(confirm=False)  # Second confirmation not needed\n",
    "        return dense_erased and sparse_erased\n",
    "    \n",
    "    def query(self, q: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Query using the sparse store.\n",
    "        \"\"\"\n",
    "        return self.sparse_store.query(q, **kwargs)\n",
    "    \n",
    "    def semantic_search(self, query: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform semantic search using the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.semantic_search(query, **kwargs)\n",
    "    \n",
    "    def hybrid_search(self, query: str, limit: int = 10, weights: Union[List[float], float] = 0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform hybrid search combining dense and sparse results.\n",
    "        \n",
    "        **Args**:\n",
    "        - *query*: Search query string\n",
    "        - *limit*: Maximum number of results to return\n",
    "        - *weights*: Weights for combining dense and sparse scores. \n",
    "                    If float, represents dense weight (sparse = 1 - dense).\n",
    "                    If list, [dense_weight, sparse_weight]\n",
    "        - *kwargs*: Additional arguments passed to individual search methods\n",
    "        \n",
    "        **Returns**:\n",
    "        Dictionary with 'hits' and 'total_hits' keys\n",
    "        \"\"\"\n",
    "        # Create weights array if single number passed\n",
    "        if isinstance(weights, (int, float)):\n",
    "            weights = [weights, 1 - weights]\n",
    "        \n",
    "        # Get expanded results from both stores\n",
    "        search_limit = limit * 10  # Get more candidates for better fusion\n",
    "        \n",
    "        # Get dense results\n",
    "        dense_results = self.dense_store.query(query, limit=search_limit, return_dict=True, **kwargs)\n",
    "        dense_hits = dense_results.get('hits', [])\n",
    "        \n",
    "        # Get sparse results  \n",
    "        sparse_results = self.sparse_store.query(query, limit=search_limit, return_dict=True, **kwargs)\n",
    "        sparse_hits = sparse_results.get('hits', [])\n",
    "        \n",
    "        # Combine scores using hybrid approach\n",
    "        uids = {}\n",
    "        \n",
    "        # Process dense results (similarity-based scores)\n",
    "        if weights[0] > 0:\n",
    "            for dense_doc in dense_hits:\n",
    "                uid = dense_doc.get('id')\n",
    "                if uid:\n",
    "                    score = dense_doc.get('score', 0.0)\n",
    "                    uids[uid] = score * weights[0]\n",
    "        \n",
    "        # Process sparse results (use RRF since sparse doesn't have normalized scores)\n",
    "        if weights[1] > 0:\n",
    "            for rank, sparse_doc in enumerate(sparse_hits):\n",
    "                uid = sparse_doc.get('id')\n",
    "                if uid:\n",
    "                    # Use Reciprocal Rank Fusion (RRF) for sparse results\n",
    "                    rrf_score = 1.0 / (rank + 1)\n",
    "                    if uid in uids:\n",
    "                        uids[uid] += rrf_score * weights[1]\n",
    "                    else:\n",
    "                        uids[uid] = rrf_score * weights[1]\n",
    "        \n",
    "        # Sort by combined score and limit results\n",
    "        sorted_results = sorted(uids.items(), key=lambda x: x[1], reverse=True)[:limit]\n",
    "        \n",
    "        # Convert back to result dictionaries\n",
    "        results = []\n",
    "        # Create a lookup dict for efficient document retrieval\n",
    "        doc_lookup = {}\n",
    "        for doc in dense_hits + sparse_hits:\n",
    "            doc_lookup[doc.get('id')] = doc\n",
    "        \n",
    "        for uid, combined_score in sorted_results:\n",
    "            if uid in doc_lookup:\n",
    "                doc_dict = doc_lookup[uid].copy()\n",
    "                doc_dict['score'] = combined_score  # Update with combined score\n",
    "                results.append(doc_dict)\n",
    "        \n",
    "        return {'hits': results, 'total_hits': len(results)}\n",
    "\n",
    "\n",
    "class ElasticsearchDualStore(ElasticsearchStore):\n",
    "    \"\"\"\n",
    "    A unified Elasticsearch-based dual store that supports both dense vector searches\n",
    "    and sparse text searches in a single index. Extends ElasticsearchStore with dense vector capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                dense_vector_field: str = 'dense_vector',\n",
    "                vector_dims: int = 384,  # Default for sentence-transformers models\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes unified Elasticsearch dual store with both dense and sparse capabilities.\n",
    "        \n",
    "        **Args:**\n",
    "        - *dense_vector_field*: field name for dense vectors (default: 'dense_vector')\n",
    "        - *vector_dims*: dimension of dense vectors (default: 384)\n",
    "        - All other args are passed to ElasticsearchStore (persist_directory, index_name, basic_auth, etc.)\n",
    "        \"\"\"\n",
    "        self.dense_vector_field = dense_vector_field\n",
    "        self.vector_dims = vector_dims\n",
    "        \n",
    "        # Initialize parent ElasticsearchStore\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _create_index(self):\n",
    "        \"\"\"Create Elasticsearch index with both text and vector mappings\"\"\"\n",
    "        # Get the standard mapping from parent class\n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    # Text fields for sparse search (same as parent)\n",
    "                    \"page_content\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \"id\": {\"type\": \"keyword\"},\n",
    "                    \"source\": {\"type\": \"keyword\"},\n",
    "                    \"source_search\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \"filepath\": {\"type\": \"keyword\"},\n",
    "                    \"filepath_search\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \"filename\": {\"type\": \"keyword\"},\n",
    "                    \"ocr\": {\"type\": \"boolean\"},\n",
    "                    \"table\": {\"type\": \"boolean\"},\n",
    "                    \"markdown\": {\"type\": \"boolean\"},\n",
    "                    \"page\": {\"type\": \"integer\"},\n",
    "                    \"document_title\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \"md5\": {\"type\": \"keyword\"},\n",
    "                    \"mimetype\": {\"type\": \"keyword\"},\n",
    "                    \"extension\": {\"type\": \"keyword\"},\n",
    "                    \"filesize\": {\"type\": \"integer\"},\n",
    "                    \"createdate\": {\"type\": \"date\"},\n",
    "                    \"modifydate\": {\"type\": \"date\"},\n",
    "                    \"tags\": {\"type\": \"keyword\"},\n",
    "                    \"notes\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \"msg\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                    \n",
    "                    # Dense vector field for semantic search\n",
    "                    self.dense_vector_field: {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": self.vector_dims,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.es.indices.create(index=self.index_name, body=mapping)\n",
    "\n",
    "    def doc2dict(self, doc: Document, include_vector: bool = True):\n",
    "        \"\"\"Convert LangChain Document to expected format with optional vector embedding\"\"\"\n",
    "        # Get the standard dict from parent class\n",
    "        d = super().doc2dict(doc)\n",
    "        \n",
    "        # Add dense vector embedding if requested\n",
    "        if include_vector and hasattr(self, 'embeddings'):\n",
    "            try:\n",
    "                # Generate embedding for the document text\n",
    "                embedding = self.embeddings.embed_documents([doc.page_content])[0]\n",
    "                d[self.dense_vector_field] = embedding\n",
    "            except Exception as e:\n",
    "                # If embedding fails, continue without it\n",
    "                pass\n",
    "                \n",
    "        return d\n",
    "\n",
    "\n",
    "    def semantic_search(self,\n",
    "                       query: str,\n",
    "                       limit: int = 4,\n",
    "                       filters: Optional[Dict[str, str]] = None,\n",
    "                       return_dict: bool = True,\n",
    "                       **kwargs):\n",
    "        \"\"\"Perform semantic search using dense vectors (equivalent to ChromaStore.semantic_search)\"\"\"\n",
    "        if not hasattr(self, 'embeddings'):\n",
    "            raise ValueError(\"Embeddings not initialized. Cannot perform semantic search.\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Build Elasticsearch KNN query\n",
    "        knn_query = {\n",
    "            \"knn\": {\n",
    "                \"field\": self.dense_vector_field,\n",
    "                \"query_vector\": query_embedding,\n",
    "                \"k\": limit,\n",
    "                \"num_candidates\": limit * 10  # More candidates for better results\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add filters if provided\n",
    "        if filters:\n",
    "            filter_clauses = []\n",
    "            for k, v in filters.items():\n",
    "                filter_clauses.append({\"term\": {k: v}})\n",
    "            knn_query[\"knn\"][\"filter\"] = filter_clauses\n",
    "        \n",
    "        # Execute search\n",
    "        response = self.es.search(index=self.index_name, body=knn_query, size=limit)\n",
    "        \n",
    "        # Process results\n",
    "        hits = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            doc_dict = hit['_source'].copy()\n",
    "            # Convert Elasticsearch score to similarity score (higher is better)\n",
    "            doc_dict['score'] = hit['_score']\n",
    "            hits.append(doc_dict)\n",
    "        \n",
    "        total_hits = response['hits']['total']['value']\n",
    "        \n",
    "        if return_dict:\n",
    "            return {'hits': hits, 'total_hits': total_hits}\n",
    "        else:\n",
    "            return [doc_from_dict(hit) for hit in hits]\n",
    "\n",
    "\n",
    "    def hybrid_search(self,\n",
    "                     query: str,\n",
    "                     limit: int = 10,\n",
    "                     weights: Union[List[float], float] = 0.5,\n",
    "                     filters: Optional[Dict[str, str]] = None,\n",
    "                     **kwargs):\n",
    "        \"\"\"Perform hybrid search combining dense vector and sparse text search using Reciprocal Rank Fusion (RRF)\"\"\"\n",
    "        # Create weights array if single number passed\n",
    "        if isinstance(weights, (int, float)):\n",
    "            weights = [weights, 1 - weights]\n",
    "        \n",
    "        # Get expanded results from both search types\n",
    "        search_limit = limit * 10  # Get more candidates for better fusion\n",
    "        \n",
    "        # Get dense vector results\n",
    "        dense_results = []\n",
    "        if weights[0] > 0:\n",
    "            try:\n",
    "                dense_results = self.semantic_search(\n",
    "                    query, \n",
    "                    limit=search_limit, \n",
    "                    filters=filters, \n",
    "                    return_dict=True\n",
    "                )['hits']\n",
    "            except Exception:\n",
    "                # If dense search fails, continue with sparse only\n",
    "                pass\n",
    "        \n",
    "        # Get sparse text results  \n",
    "        sparse_results = []\n",
    "        if weights[1] > 0:\n",
    "            try:\n",
    "                sparse_results = self.query(\n",
    "                    query, \n",
    "                    limit=search_limit, \n",
    "                    filters=filters, \n",
    "                    return_dict=True\n",
    "                )['hits']\n",
    "            except Exception:\n",
    "                # If sparse search fails, continue with dense only\n",
    "                pass\n",
    "        \n",
    "        # Combine scores using Reciprocal Rank Fusion (RRF)\n",
    "        doc_scores = {}\n",
    "        \n",
    "        # Process dense results (use actual similarity scores)\n",
    "        if weights[0] > 0:\n",
    "            for rank, doc in enumerate(dense_results):\n",
    "                doc_id = doc.get('id')\n",
    "                if doc_id:\n",
    "                    # Use RRF: 1 / (k + rank) where k=60 is standard\n",
    "                    rrf_score = 1.0 / (60 + rank + 1)\n",
    "                    doc_scores[doc_id] = {\n",
    "                        'doc': doc,\n",
    "                        'score': rrf_score * weights[0]\n",
    "                    }\n",
    "        \n",
    "        # Process sparse results (use RRF for ranking)\n",
    "        if weights[1] > 0:\n",
    "            for rank, doc in enumerate(sparse_results):\n",
    "                doc_id = doc.get('id')\n",
    "                if doc_id:\n",
    "                    # Use RRF: 1 / (k + rank) where k=60 is standard\n",
    "                    rrf_score = 1.0 / (60 + rank + 1)\n",
    "                    if doc_id in doc_scores:\n",
    "                        # Combine scores if document appears in both results\n",
    "                        doc_scores[doc_id]['score'] += rrf_score * weights[1]\n",
    "                    else:\n",
    "                        doc_scores[doc_id] = {\n",
    "                            'doc': doc,\n",
    "                            'score': rrf_score * weights[1]\n",
    "                        }\n",
    "        \n",
    "        # Sort by combined score and limit results\n",
    "        sorted_results = sorted(\n",
    "            doc_scores.items(), \n",
    "            key=lambda x: x[1]['score'], \n",
    "            reverse=True\n",
    "        )[:limit]\n",
    "        \n",
    "        # Convert back to result format\n",
    "        hits = []\n",
    "        for doc_id, result in sorted_results:\n",
    "            doc_dict = result['doc'].copy()\n",
    "            doc_dict['score'] = result['score']  # Update with combined RRF score\n",
    "            hits.append(doc_dict)\n",
    "        \n",
    "        return {'hits': hits, 'total_hits': len(hits)}\n",
    "\n",
    "    def get_dense_db(self):\n",
    "        \"\"\"Returns the Elasticsearch client for dense operations\"\"\"\n",
    "        return self.es\n",
    "    \n",
    "    def get_sparse_db(self):\n",
    "        \"\"\"Returns the Elasticsearch client for sparse operations\"\"\"\n",
    "        return self.es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.exists\n",
       "\n",
       ">      DualStore.exists ()\n",
       "\n",
       "*Returns True if either store exists.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.exists\n",
       "\n",
       ">      DualStore.exists ()\n",
       "\n",
       "*Returns True if either store exists.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.add_documents\n",
       "\n",
       ">      DualStore.add_documents\n",
       ">                               (documents:Sequence[langchain_core.documents.bas\n",
       ">                               e.Document], batch_size:int=1000, **kwargs)\n",
       "\n",
       "*Add documents to both dense and sparse stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.add_documents\n",
       "\n",
       ">      DualStore.add_documents\n",
       ">                               (documents:Sequence[langchain_core.documents.bas\n",
       ">                               e.Document], batch_size:int=1000, **kwargs)\n",
       "\n",
       "*Add documents to both dense and sparse stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L93){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_document\n",
       "\n",
       ">      DualStore.remove_document (id_to_delete)\n",
       "\n",
       "*Remove a document from both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L93){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_document\n",
       "\n",
       ">      DualStore.remove_document (id_to_delete)\n",
       "\n",
       "*Remove a document from both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.remove_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L100){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_source\n",
       "\n",
       ">      DualStore.remove_source (source:str)\n",
       "\n",
       "*Remove a document by source from both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L100){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_source\n",
       "\n",
       ">      DualStore.remove_source (source:str)\n",
       "\n",
       "*Remove a document by source from both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.remove_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.update_documents\n",
       "\n",
       ">      DualStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update documents in both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.update_documents\n",
       "\n",
       ">      DualStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update documents in both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.update_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_all_docs\n",
       "\n",
       ">      DualStore.get_all_docs ()\n",
       "\n",
       "*Get all documents from the dense store.\n",
       "For simplicity, we only return documents from one store since they should be the same.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_all_docs\n",
       "\n",
       ">      DualStore.get_all_docs ()\n",
       "\n",
       "*Get all documents from the dense store.\n",
       "For simplicity, we only return documents from one store since they should be the same.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_doc\n",
       "\n",
       ">      DualStore.get_doc (id)\n",
       "\n",
       "*Get a document by ID from the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_doc\n",
       "\n",
       ">      DualStore.get_doc (id)\n",
       "\n",
       "*Get a document by ID from the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_size\n",
       "\n",
       ">      DualStore.get_size ()\n",
       "\n",
       "*Get the size of the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_size\n",
       "\n",
       ">      DualStore.get_size ()\n",
       "\n",
       "*Get the size of the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.erase\n",
       "\n",
       ">      DualStore.erase (confirm=True)\n",
       "\n",
       "*Erase both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.erase\n",
       "\n",
       ">      DualStore.erase (confirm=True)\n",
       "\n",
       "*Erase both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.query\n",
       "\n",
       ">      DualStore.query (q:str, **kwargs)\n",
       "\n",
       "*Query using the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.query\n",
       "\n",
       ">      DualStore.query (q:str, **kwargs)\n",
       "\n",
       "*Query using the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.semantic_search\n",
       "\n",
       ">      DualStore.semantic_search (query:str, **kwargs)\n",
       "\n",
       "*Perform semantic search using the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.semantic_search\n",
       "\n",
       ">      DualStore.semantic_search (query:str, **kwargs)\n",
       "\n",
       "*Perform semantic search using the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
