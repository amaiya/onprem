{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.dual\n",
    "\n",
    "> Dual vector store implementation for ingesting documents into both sparse and dense stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Callable, Dict, Sequence, Union\n",
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from onprem.ingest.base import VectorStore\n",
    "from onprem.ingest.stores.dense import DenseStore\n",
    "from onprem.ingest.stores.sparse import SparseStore\n",
    "from onprem.ingest.helpers import doc_from_dict\n",
    "\n",
    "class DualStore(VectorStore):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dense_persist_directory: Optional[str] = None,\n",
    "        sparse_persist_directory: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a dual vector store that manages both dense and sparse stores.\n",
    "        \n",
    "        **Args**:\n",
    "        \n",
    "          - *dense_persist_directory*: Path to dense vector database (created if it doesn't exist).\n",
    "          - *sparse_persist_directory*: Path to sparse vector database (created if it doesn't exist).\n",
    "          - *embedding_model_name*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        \"\"\"\n",
    "        self.init_embedding_model(**kwargs)  # stored in self.embeddings\n",
    "        \n",
    "        # Initialize both stores\n",
    "        self.dense_store = DenseStore(\n",
    "            persist_directory=dense_persist_directory,\n",
    "            embedding_model_name=kwargs.get('embedding_model_name'),\n",
    "            embedding_model_kwargs=kwargs.get('embedding_model_kwargs'),\n",
    "            embedding_encode_kwargs=kwargs.get('embedding_encode_kwargs')\n",
    "        )\n",
    "        self.sparse_store = SparseStore(\n",
    "            persist_directory=sparse_persist_directory,\n",
    "            embedding_model_name=kwargs.get('embedding_model_name'),\n",
    "            embedding_model_kwargs=kwargs.get('embedding_model_kwargs'),\n",
    "            embedding_encode_kwargs=kwargs.get('embedding_encode_kwargs')\n",
    "        )\n",
    "        \n",
    "        # For compatibility with the VectorStore interface\n",
    "        self.persist_directory = dense_persist_directory\n",
    "\n",
    "    def keyword_search(self, query: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform keyword search using the sparse store.\n",
    "        \"\"\"\n",
    "        return self.sparse_store.query(query, **kwargs)\n",
    "\n",
    "    def get_dense_db(self):\n",
    "        \"\"\"\n",
    "        Returns the dense store's database instance.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_db()\n",
    "    \n",
    "    def get_sparse_db(self):\n",
    "        \"\"\"\n",
    "        Returns the sparse store's database instance.\n",
    "        \"\"\"\n",
    "        return self.sparse_store.get_db()\n",
    "\n",
    "\n",
    "    #------------------------------\n",
    "    # overrides of abstract methods\n",
    "    # -----------------------------\n",
    "  \n",
    "    \n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if either store exists.\n",
    "        \"\"\"\n",
    "        return self.dense_store.exists() or self.sparse_store.exists()\n",
    "    \n",
    "    def add_documents(self, documents: Sequence[Document], batch_size: int = 1000, **kwargs):\n",
    "        \"\"\"\n",
    "        Add documents to both dense and sparse stores.\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        # Add to dense store\n",
    "        self.dense_store.add_documents(documents, batch_size=batch_size, **kwargs)\n",
    "        \n",
    "        # Add to sparse store\n",
    "        self.sparse_store.add_documents(documents, **kwargs)\n",
    "\n",
    "   \n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a document from both stores.\n",
    "        \"\"\"\n",
    "        self.dense_store.remove_document(id_to_delete)\n",
    "        self.sparse_store.remove_document(id_to_delete)\n",
    "\n",
    "    def remove_source(self, source:str):\n",
    "        \"\"\"\n",
    "        Remove a document by source from both stores.\n",
    "\n",
    "        The `source` can either be the full path to a document\n",
    "        or a parent folder.  Returns the number of records deleted.\n",
    "        \"\"\"\n",
    "        num_deleted_1 = self.dense_store.remove_source(source)\n",
    "        num_deleted_2 = self.sparse_store.remove_source(source)\n",
    "        return num_deleted_1\n",
    "    \n",
    "\n",
    "    def update_documents(self, doc_dicts: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        Update documents in both stores.\n",
    "        \"\"\"\n",
    "        self.dense_store.update_documents(doc_dicts, **kwargs)\n",
    "        self.sparse_store.update_documents(doc_dicts, **kwargs)\n",
    "    \n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Get all documents from the dense store.\n",
    "        For simplicity, we only return documents from one store since they should be the same.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_all_docs()\n",
    "    \n",
    "    def get_doc(self, id):\n",
    "        \"\"\"\n",
    "        Get a document by ID from the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_doc(id)\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get the size of the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_size()\n",
    "    \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Erase both stores.\n",
    "        \"\"\"\n",
    "        dense_erased = self.dense_store.erase(confirm=confirm)\n",
    "        sparse_erased = self.sparse_store.erase(confirm=False)  # Second confirmation not needed\n",
    "        return dense_erased and sparse_erased\n",
    "    \n",
    "    def query(self, q: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Query using the sparse store.\n",
    "        \"\"\"\n",
    "        return self.sparse_store.query(q, **kwargs)\n",
    "    \n",
    "    def semantic_search(self, query: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform semantic search using the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.semantic_search(query, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.exists\n",
       "\n",
       ">      DualStore.exists ()\n",
       "\n",
       "*Returns True if either store exists.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.exists\n",
       "\n",
       ">      DualStore.exists ()\n",
       "\n",
       "*Returns True if either store exists.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.add_documents\n",
       "\n",
       ">      DualStore.add_documents\n",
       ">                               (documents:Sequence[langchain_core.documents.bas\n",
       ">                               e.Document], batch_size:int=1000, **kwargs)\n",
       "\n",
       "*Add documents to both dense and sparse stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.add_documents\n",
       "\n",
       ">      DualStore.add_documents\n",
       ">                               (documents:Sequence[langchain_core.documents.bas\n",
       ">                               e.Document], batch_size:int=1000, **kwargs)\n",
       "\n",
       "*Add documents to both dense and sparse stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L93){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_document\n",
       "\n",
       ">      DualStore.remove_document (id_to_delete)\n",
       "\n",
       "*Remove a document from both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L93){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_document\n",
       "\n",
       ">      DualStore.remove_document (id_to_delete)\n",
       "\n",
       "*Remove a document from both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.remove_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L100){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_source\n",
       "\n",
       ">      DualStore.remove_source (source:str)\n",
       "\n",
       "*Remove a document by source from both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L100){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_source\n",
       "\n",
       ">      DualStore.remove_source (source:str)\n",
       "\n",
       "*Remove a document by source from both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.remove_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.update_documents\n",
       "\n",
       ">      DualStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update documents in both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.update_documents\n",
       "\n",
       ">      DualStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update documents in both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.update_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_all_docs\n",
       "\n",
       ">      DualStore.get_all_docs ()\n",
       "\n",
       "*Get all documents from the dense store.\n",
       "For simplicity, we only return documents from one store since they should be the same.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_all_docs\n",
       "\n",
       ">      DualStore.get_all_docs ()\n",
       "\n",
       "*Get all documents from the dense store.\n",
       "For simplicity, we only return documents from one store since they should be the same.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_doc\n",
       "\n",
       ">      DualStore.get_doc (id)\n",
       "\n",
       "*Get a document by ID from the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_doc\n",
       "\n",
       ">      DualStore.get_doc (id)\n",
       "\n",
       "*Get a document by ID from the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_size\n",
       "\n",
       ">      DualStore.get_size ()\n",
       "\n",
       "*Get the size of the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_size\n",
       "\n",
       ">      DualStore.get_size ()\n",
       "\n",
       "*Get the size of the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.erase\n",
       "\n",
       ">      DualStore.erase (confirm=True)\n",
       "\n",
       "*Erase both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.erase\n",
       "\n",
       ">      DualStore.erase (confirm=True)\n",
       "\n",
       "*Erase both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.query\n",
       "\n",
       ">      DualStore.query (q:str, **kwargs)\n",
       "\n",
       "*Query using the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.query\n",
       "\n",
       ">      DualStore.query (q:str, **kwargs)\n",
       "\n",
       "*Query using the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.semantic_search\n",
       "\n",
       ">      DualStore.semantic_search (query:str, **kwargs)\n",
       "\n",
       "*Perform semantic search using the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.semantic_search\n",
       "\n",
       ">      DualStore.semantic_search (query:str, **kwargs)\n",
       "\n",
       "*Perform semantic search using the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
