{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.dual\n",
    "\n",
    "> Dual vector store implementation for ingesting documents into both sparse and dense stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Callable, Dict, Sequence, Union\n",
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from onprem.ingest.stores.base import VectorStore\n",
    "from onprem.ingest.stores.dense import DenseStore\n",
    "from onprem.ingest.stores.sparse import SparseStore, ElasticsearchSparseStore, ELASTICSEARCH_INSTALLED\n",
    "from onprem.ingest.helpers import doc_from_dict\n",
    "\n",
    "class DualStore(VectorStore):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dense_kind:str='chroma',\n",
    "        dense_persist_location: Optional[str] = None,\n",
    "        sparse_kind:str='whoosh',\n",
    "        sparse_persist_location: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a dual vector store that manages both dense and sparse stores.\n",
    "        \n",
    "        **Args**:\n",
    "        \n",
    "          - *dense_persist_location*: Path to dense vector database (created if it doesn't exist).\n",
    "          - *sparse_persist_location*: Path to sparse vector database (created if it doesn't exist).\n",
    "          - *embedding_model_name*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        \"\"\"\n",
    "        self.init_embedding_model(**kwargs)  # stored in self.embeddings\n",
    "        \n",
    "        # Initialize both stores\n",
    "        self.dense_store = DenseStore.create(\n",
    "            kind=dense_kind,\n",
    "            persist_location=dense_persist_location,\n",
    "            embedding_model_name=kwargs.get('embedding_model_name'),\n",
    "            embedding_model_kwargs=kwargs.get('embedding_model_kwargs'),\n",
    "            embedding_encode_kwargs=kwargs.get('embedding_encode_kwargs')\n",
    "        )\n",
    "        self.sparse_store = SparseStore.create(\n",
    "            kind=sparse_kind,\n",
    "            persist_location=sparse_persist_location,\n",
    "            embedding_model_name=kwargs.get('embedding_model_name'),\n",
    "            embedding_model_kwargs=kwargs.get('embedding_model_kwargs'),\n",
    "            embedding_encode_kwargs=kwargs.get('embedding_encode_kwargs')\n",
    "        )\n",
    "        \n",
    "        # For compatibility with the VectorStore interface\n",
    "        self.persist_location = dense_persist_location\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, dense_kind='chroma', sparse_kind='whoosh', persist_location=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Factory method to construct a DualStore instance.\n",
    "        \n",
    "        Args:\n",
    "            dense_kind: type of dense store ('chroma', 'elasticsearch')\n",
    "            sparse_kind: type of sparse store ('whoosh', 'elasticsearch')\n",
    "            persist_location: base directory for stores or Elasticsearch URL\n",
    "            **kwargs: additional arguments passed to store initialization\n",
    "            \n",
    "        For traditional dual stores (different dense_kind and sparse_kind):\n",
    "            dense_persist_location: directory for dense store  \n",
    "            sparse_persist_location: directory for sparse store\n",
    "            \n",
    "        For unified Elasticsearch dual store (both kinds are 'elasticsearch'):\n",
    "            index_name: name of Elasticsearch index\n",
    "            dense_vector_field: field name for dense vectors\n",
    "            basic_auth: authentication credentials\n",
    "            verify_certs: SSL verification\n",
    "            ca_certs: CA certificate path\n",
    "            timeout: connection timeout\n",
    "            \n",
    "        Returns:\n",
    "            DualStore instance\n",
    "        \"\"\"\n",
    "        # If both dense and sparse are elasticsearch, use unified ElasticsearchDualStore\n",
    "        if dense_kind == 'elasticsearch' and sparse_kind == 'elasticsearch':\n",
    "            if not ELASTICSEARCH_INSTALLED:\n",
    "                raise ImportError('Please install elasticsearch packages: pip install onprem[elasticsearch]')\n",
    "            return ElasticsearchStore(persist_location=persist_location, **kwargs)\n",
    "        \n",
    "        # Otherwise, use traditional dual store approach\n",
    "        return cls(\n",
    "            dense_kind=dense_kind,\n",
    "            sparse_kind=sparse_kind,\n",
    "            dense_persist_location=persist_location,\n",
    "            sparse_persist_location=persist_location,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "    # get_dense_db() and get_sparse_db() methods removed - use store methods instead\n",
    "\n",
    "\n",
    "    #------------------------------\n",
    "    # overrides of abstract methods\n",
    "    # -----------------------------\n",
    "  \n",
    "    \n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if either store exists.\n",
    "        \"\"\"\n",
    "        return self.dense_store.exists() or self.sparse_store.exists()\n",
    "    \n",
    "    def add_documents(self, documents: Sequence[Document], batch_size: int = 1000, **kwargs):\n",
    "        \"\"\"\n",
    "        Add documents to both dense and sparse stores.\n",
    "        If both stores use the same persist_location, only add once.\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return\n",
    "        \n",
    "        # Check if both stores are using the same persist_location\n",
    "        if self._stores_share_same_location():\n",
    "            # Use the dense store since it typically includes both text and vectors\n",
    "            self.dense_store.add_documents(documents, batch_size=batch_size, **kwargs)\n",
    "        else:\n",
    "            # Traditional dual store approach - add to both stores\n",
    "            self.dense_store.add_documents(documents, batch_size=batch_size, **kwargs)\n",
    "            self.sparse_store.add_documents(documents, **kwargs)\n",
    "    \n",
    "    def _stores_share_same_location(self):\n",
    "        \"\"\"\n",
    "        Check if the dense and sparse stores are using the same persist_location.\n",
    "        This indicates they're using the same underlying storage/index.\n",
    "        \"\"\"\n",
    "        dense_location = getattr(self.dense_store, 'persist_location', None)\n",
    "        sparse_location = getattr(self.sparse_store, 'persist_location', None)\n",
    "        \n",
    "        return (dense_location is not None and \n",
    "                sparse_location is not None and \n",
    "                dense_location == sparse_location)\n",
    "\n",
    "   \n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a document from both stores.\n",
    "        \"\"\"\n",
    "        self.dense_store.remove_document(id_to_delete)\n",
    "        self.sparse_store.remove_document(id_to_delete)\n",
    "\n",
    "    def remove_source(self, source:str):\n",
    "        \"\"\"\n",
    "        Remove a document by source from both stores.\n",
    "\n",
    "        The `source` can either be the full path to a document\n",
    "        or a parent folder.  Returns the number of records deleted.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            num_deleted_1 = self.dense_store.remove_source(source)\n",
    "            num_deleted_2 = self.sparse_store.remove_source(source)\n",
    "            return num_deleted_1\n",
    "        except ValueError as e:\n",
    "            # Re-raise with more context if source field is not configured\n",
    "            if \"no source field configured\" in str(e):\n",
    "                raise ValueError(\"Cannot remove by source: neither dense nor sparse store has a source field configured. Set source_field parameter when creating the store.\")\n",
    "            raise\n",
    "    \n",
    "\n",
    "    def update_documents(self, doc_dicts: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        Update documents in both stores.\n",
    "        \"\"\"\n",
    "        self.dense_store.update_documents(doc_dicts, **kwargs)\n",
    "        self.sparse_store.update_documents(doc_dicts, **kwargs)\n",
    "    \n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Get all documents from the dense store.\n",
    "        For simplicity, we only return documents from one store since they should be the same.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_all_docs()\n",
    "    \n",
    "    def get_doc(self, id):\n",
    "        \"\"\"\n",
    "        Get a document by ID from the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_doc(id)\n",
    "    \n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get the size of the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.get_size()\n",
    "    \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Erase both stores.\n",
    "        \"\"\"\n",
    "        dense_erased = self.dense_store.erase(confirm=confirm)\n",
    "        sparse_erased = self.sparse_store.erase(confirm=False)  # Second confirmation not needed\n",
    "        return dense_erased and sparse_erased\n",
    "    \n",
    "    def query(self, q: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Query using the sparse store.\n",
    "        \"\"\"\n",
    "        return self.sparse_store.query(q, **kwargs)\n",
    "    \n",
    "    def semantic_search(self, query: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform semantic search using the dense store.\n",
    "        \"\"\"\n",
    "        return self.dense_store.semantic_search(query, **kwargs)\n",
    "    \n",
    "    def hybrid_search(self, query: str, limit: int = 10, weights: Union[List[float], float] = 0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform hybrid search combining dense and sparse results.\n",
    "        \n",
    "        **Args**:\n",
    "        - *query*: Search query string\n",
    "        - *limit*: Maximum number of results to return\n",
    "        - *weights*: Weights for combining dense and sparse scores. \n",
    "                    If float, represents dense weight (sparse = 1 - dense).\n",
    "                    If list, [dense_weight, sparse_weight]\n",
    "        - *kwargs*: Additional arguments passed to individual search methods\n",
    "        \n",
    "        **Returns**:\n",
    "        Dictionary with 'hits' and 'total_hits' keys\n",
    "        \"\"\"\n",
    "        # Create weights array if single number passed\n",
    "        if isinstance(weights, (int, float)):\n",
    "            weights = [weights, 1 - weights]\n",
    "        \n",
    "        # Get expanded results from both stores\n",
    "        search_limit = limit * 10  # Get more candidates for better fusion\n",
    "        \n",
    "        # Get dense results\n",
    "        dense_results = self.dense_store.query(query, limit=search_limit, **kwargs)\n",
    "        dense_hits = dense_results.get('hits', [])\n",
    "        \n",
    "        # Get sparse results  \n",
    "        sparse_results = self.sparse_store.query(query, limit=search_limit, return_dict=True, **kwargs)\n",
    "        sparse_hits = sparse_results.get('hits', [])\n",
    "        \n",
    "        # Combine scores using hybrid approach\n",
    "        uids = {}\n",
    "        \n",
    "        # Process dense results (similarity-based scores)\n",
    "        if weights[0] > 0:\n",
    "            for dense_doc in dense_hits:\n",
    "                uid = dense_doc.get('id')\n",
    "                if uid:\n",
    "                    score = dense_doc.get('score', 0.0)\n",
    "                    uids[uid] = score * weights[0]\n",
    "        \n",
    "        # Process sparse results (use RRF since sparse doesn't have normalized scores)\n",
    "        if weights[1] > 0:\n",
    "            for rank, sparse_doc in enumerate(sparse_hits):\n",
    "                uid = sparse_doc.get('id')\n",
    "                if uid:\n",
    "                    # Use Reciprocal Rank Fusion (RRF) for sparse results\n",
    "                    rrf_score = 1.0 / (rank + 1)\n",
    "                    if uid in uids:\n",
    "                        uids[uid] += rrf_score * weights[1]\n",
    "                    else:\n",
    "                        uids[uid] = rrf_score * weights[1]\n",
    "        \n",
    "        # Sort by combined score and limit results\n",
    "        sorted_results = sorted(uids.items(), key=lambda x: x[1], reverse=True)[:limit]\n",
    "        \n",
    "        # Convert back to result dictionaries\n",
    "        results = []\n",
    "        # Create a lookup dict for efficient document retrieval\n",
    "        doc_lookup = {}\n",
    "        for doc in dense_hits + sparse_hits:\n",
    "            doc_lookup[doc.get('id')] = doc\n",
    "        \n",
    "        for uid, combined_score in sorted_results:\n",
    "            if uid in doc_lookup:\n",
    "                doc_dict = doc_lookup[uid].copy()\n",
    "                doc_dict['score'] = combined_score  # Update with combined score\n",
    "                results.append(doc_dict)\n",
    "        \n",
    "        return {'hits': results, 'total_hits': len(results)}\n",
    "\n",
    "\n",
    "class ElasticsearchStore(DualStore):\n",
    "    \"\"\"\n",
    "    A unified Elasticsearch-based dual store that supports both dense vector searches\n",
    "    and sparse text searches in a single index. Uses composition to manage both stores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                dense_vector_field: str = 'dense_vector',\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes unified Elasticsearch dual store with both dense and sparse capabilities.\n",
    "        \n",
    "        **Args:**\n",
    "        - *dense_vector_field*: field name for dense vectors (default: 'dense_vector')\n",
    "        - All other args are passed to both stores (persist_location, index_name, basic_auth, etc.)\n",
    "        \"\"\"\n",
    "        # Initialize embedding model (required by DualStore interface)\n",
    "        self.init_embedding_model(**kwargs)\n",
    "        \n",
    "        # Create both stores pointing to the same Elasticsearch index\n",
    "        from .dense import ElasticsearchDenseStore\n",
    "        self.dense_store = ElasticsearchDenseStore(\n",
    "            dense_vector_field=dense_vector_field, **kwargs)\n",
    "        self.sparse_store = ElasticsearchSparseStore(**kwargs)\n",
    "        \n",
    "        # For compatibility with the VectorStore interface\n",
    "        self.persist_location = kwargs.get('persist_location')\n",
    "\n",
    "    # get_dense_db() and get_sparse_db() methods removed - use store methods instead\n",
    "\n",
    "    # hybrid_search method is inherited from DualStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.exists\n",
       "\n",
       ">      DualStore.exists ()\n",
       "\n",
       "*Returns True if either store exists.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.exists\n",
       "\n",
       ">      DualStore.exists ()\n",
       "\n",
       "*Returns True if either store exists.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.add_documents\n",
       "\n",
       ">      DualStore.add_documents\n",
       ">                               (documents:Sequence[langchain_core.documents.bas\n",
       ">                               e.Document], batch_size:int=1000, **kwargs)\n",
       "\n",
       "*Add documents to both dense and sparse stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L81){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.add_documents\n",
       "\n",
       ">      DualStore.add_documents\n",
       ">                               (documents:Sequence[langchain_core.documents.bas\n",
       ">                               e.Document], batch_size:int=1000, **kwargs)\n",
       "\n",
       "*Add documents to both dense and sparse stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L93){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_document\n",
       "\n",
       ">      DualStore.remove_document (id_to_delete)\n",
       "\n",
       "*Remove a document from both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L93){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_document\n",
       "\n",
       ">      DualStore.remove_document (id_to_delete)\n",
       "\n",
       "*Remove a document from both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.remove_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L100){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_source\n",
       "\n",
       ">      DualStore.remove_source (source:str)\n",
       "\n",
       "*Remove a document by source from both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/dual.py#L100){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DualStore.remove_source\n",
       "\n",
       ">      DualStore.remove_source (source:str)\n",
       "\n",
       "*Remove a document by source from both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.remove_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.update_documents\n",
       "\n",
       ">      DualStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update documents in both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.update_documents\n",
       "\n",
       ">      DualStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update documents in both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.update_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_all_docs\n",
       "\n",
       ">      DualStore.get_all_docs ()\n",
       "\n",
       "*Get all documents from the dense store.\n",
       "For simplicity, we only return documents from one store since they should be the same.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_all_docs\n",
       "\n",
       ">      DualStore.get_all_docs ()\n",
       "\n",
       "*Get all documents from the dense store.\n",
       "For simplicity, we only return documents from one store since they should be the same.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_doc\n",
       "\n",
       ">      DualStore.get_doc (id)\n",
       "\n",
       "*Get a document by ID from the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_doc\n",
       "\n",
       ">      DualStore.get_doc (id)\n",
       "\n",
       "*Get a document by ID from the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.get_size\n",
       "\n",
       ">      DualStore.get_size ()\n",
       "\n",
       "*Get the size of the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.get_size\n",
       "\n",
       ">      DualStore.get_size ()\n",
       "\n",
       "*Get the size of the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.get_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.erase\n",
       "\n",
       ">      DualStore.erase (confirm=True)\n",
       "\n",
       "*Erase both stores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.erase\n",
       "\n",
       ">      DualStore.erase (confirm=True)\n",
       "\n",
       "*Erase both stores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.query\n",
       "\n",
       ">      DualStore.query (q:str, **kwargs)\n",
       "\n",
       "*Query using the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.query\n",
       "\n",
       ">      DualStore.query (q:str, **kwargs)\n",
       "\n",
       "*Query using the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DualStore.semantic_search\n",
       "\n",
       ">      DualStore.semantic_search (query:str, **kwargs)\n",
       "\n",
       "*Perform semantic search using the dense store.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DualStore.semantic_search\n",
       "\n",
       ">      DualStore.semantic_search (query:str, **kwargs)\n",
       "\n",
       "*Perform semantic search using the dense store.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DualStore.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
