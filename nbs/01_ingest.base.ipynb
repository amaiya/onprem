{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.base\n",
    "\n",
    "> functionality for text extraction and document ingestion into a vector database for question-answering and other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from onprem.llm.helpers import summarize_tables, extract_title\n",
    "from onprem.utils import batch_list, filtered_generator\n",
    "from onprem.ingest import helpers\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters.base import Language\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    TextLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredExcelLoader,\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from typing import List, Optional, Callable\n",
    "import multiprocessing\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logger = logging.getLogger('OnPrem.LLM-ingest')\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 500\n",
    "DEFAULT_CHUNK_OVERLAP = 50\n",
    "TABLE_CHUNK_SIZE = 2000\n",
    "CHROMA_MAX = 41000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if \"text/html content not found in email\" in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"] = \"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "        return doc\n",
    "\n",
    "\n",
    "class MyUnstructuredPDFLoader(UnstructuredPDFLoader):\n",
    "    \"\"\"Custom PDF Loader\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper UnstructuredPDFLoader\"\"\"\n",
    "        try:\n",
    "            docs = UnstructuredPDFLoader.load(self)\n",
    "            if not docs:\n",
    "                raise Exception('Document had no content. ')\n",
    "            tables = [d.metadata['text_as_html'] for d in docs if d.metadata.get('text_as_html', None) is not None]\n",
    "            texts = [d.page_content for d in docs if d.metadata.get('text_as_html', None) is None]\n",
    "\n",
    "            page_content = '\\n'.join(texts)\n",
    "            docs = [helpers.create_document(page_content, self.file_path)]\n",
    "            table_docs = [helpers.create_document(t, source=self.file_path, table=True) for t in tables]\n",
    "            docs.extend(table_docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n",
    "class _PyMuPDFLoader(PyMuPDFLoader):\n",
    "    \"\"\"Custom PyMUPDF Loader with optional support for inferring table structure\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            # PyMuPDFLoader complains when you add custom flags to text_kwargs,\n",
    "            # so delete before loading\n",
    "            infer_table_structure = self.parser.text_kwargs.get('infer_table_structure', False)\n",
    "            if 'infer_table_structure' in self.parser.text_kwargs:\n",
    "                del self.parser.text_kwargs['infer_table_structure']\n",
    "            docs = PyMuPDFLoader.load(self)\n",
    "            if infer_table_structure:\n",
    "                docs = helpers.extract_tables(docs=docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n",
    "class PDF2MarkdownLoader(_PyMuPDFLoader):\n",
    "    \"\"\"Custom PDF to Markdown Loader\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        import pymupdf4llm\n",
    "        try:\n",
    "            md_text = pymupdf4llm.to_markdown(self.file_path, show_progress=False)\n",
    "            if not md_text.strip():\n",
    "                raise Exception('Document had no content. ')\n",
    "            doc = helpers.create_document(md_text, self.file_path, markdown=True)\n",
    "            docs = [doc]\n",
    "            if self.parser.text_kwargs.get('infer_table_structure', False):\n",
    "                docs = helpers.extract_tables(docs=docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Map file extensions to document loaders and their arguments\n",
    "PDFOCR = 'pdfOCR'\n",
    "PDFMD = 'pdfMD'\n",
    "PDF = 'pdf'\n",
    "PDF_EXTS = [PDF, PDFOCR, PDFMD]\n",
    "OCR_CHAR_THRESH = 32\n",
    "LOADER_MAPPING = {\n",
    "    \"csv\": (CSVLoader, {}),\n",
    "    \"doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \"docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \"xlsx\": (UnstructuredExcelLoader, {}),\n",
    "    \"enex\": (EverNoteLoader, {}),\n",
    "    \"eml\": (MyElmLoader, {}),\n",
    "    \"epub\": (UnstructuredEPubLoader, {}),\n",
    "    \"html\": (UnstructuredHTMLLoader, {}),\n",
    "    \"htm\": (UnstructuredHTMLLoader, {}),\n",
    "    \"md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \"odt\": (UnstructuredODTLoader, {}),\n",
    "    \"ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \"pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \"txt\": (TextLoader, {\"autodetect_encoding\": True}),\n",
    "    \"json\": (TextLoader, {\"autodetect_encoding\": True}),\n",
    "    PDF   : (_PyMuPDFLoader, {}),\n",
    "    PDFMD: (PDF2MarkdownLoader, {}),\n",
    "    PDFOCR: (MyUnstructuredPDFLoader, {\"infer_table_structure\":False, \"mode\":\"elements\", \"strategy\":\"hi_res\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "def _update_metadata(docs:List[Document], metadata:dict):\n",
    "    \"\"\"\n",
    "    Update metadata in docs with supplied metadata dictionary\n",
    "    \"\"\"\n",
    "    for doc in docs:\n",
    "        doc.metadata.update(metadata)\n",
    "    return docs\n",
    "\n",
    "def _apply_text_callables(docs:List[Document], text_callables:dict):\n",
    "    \"\"\"\n",
    "    Invokes text_callables on entire text of document.\n",
    "\n",
    "    Returns a dictionary with values containing results from callables for each key\n",
    "    \"\"\"\n",
    "    if not text_callables: return {}\n",
    "        \n",
    "    text = '\\n\\n'.join([d.page_content for d in docs])\n",
    "    results = {}\n",
    "    for k,v in text_callables.items():\n",
    "        results[k] = v(text)\n",
    "    return results\n",
    "\n",
    "def _apply_file_callables(file_path:str, file_callables:dict):\n",
    "    \"\"\"\n",
    "    Invokes file_callables on file path.\n",
    "\n",
    "    Returns a dictionary with values containing results from callables for each key\n",
    "    \"\"\"\n",
    "    if not file_callables: return {}\n",
    "        \n",
    "    if not os.path.exists(file_path):\n",
    "        raise ValueError('file_path does not exist: {file_path}')\n",
    "    \n",
    "    results = {}\n",
    "    for k,v in file_callables.items():\n",
    "        results[k] = v(file_path)\n",
    "    return results\n",
    "\n",
    "    \n",
    "def load_single_document(file_path: str, # path to file\n",
    "                         pdf_unstructured:bool=False, # use unstructured for PDF extraction if True (will also OCR if necessary)\n",
    "                         pdf_markdown:bool = False, # Convert PDFs to Markdown instead of plain text if True.\n",
    "                         store_md5:bool=False, # Extract and store MD5 of document in metadata\n",
    "                         store_mimetype:bool=False, # Guess and store mime type of document in metadata\n",
    "                         store_file_dates:bool=False, # Extract snd store file dates in metadata\n",
    "                         file_callables:Optional[dict]=None, # optional dict with  keys and functions called with filepath as argument. Results stored as metadata.\n",
    "                         text_callables:Optional[dict]=None, # optional dict with  keys and functions called with file text as argument. Results stored as metadata.\n",
    "                         **kwargs,\n",
    "                         ) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extract text from a single document. Will attempt to OCR PDFs, if necessary.\n",
    "\n",
    "\n",
    "    Note that extra kwargs can be supplied to configure the behavior of PDF loaders.\n",
    "    For instance, supplying `infer_table_structure` will cause `load_single_document` to try and\n",
    "    infer and extract tables from PDFs. When `pdf_unstructured=True` and `infer_table_structure=True`,\n",
    "    tables are represented as HTML within the main body of extracted text. In all other cases, inferred tables\n",
    "    are represented as Markdown and appended to the end of the extracted text when `infer_table_structure=True`.\n",
    "    \"\"\"\n",
    "    if pdf_unstructured and pdf_markdown:\n",
    "        raise ValueError('pdf_unstructured and pdf_markdown cannot both be True.')\n",
    "    file_callables = {} if not file_callables else file_callables\n",
    "    text_callables = {} if not text_callables else text_callables\n",
    "    \n",
    "    # Normalize path for consistent handling - first get absolute path\n",
    "    file_path = os.path.abspath(file_path)\n",
    "    # Then normalize and standardize path separators to forward slashes for cross-platform consistency\n",
    "    file_path = os.path.normpath(file_path).replace('\\\\', '/')\n",
    "\n",
    "\n",
    "    # extract metadata\n",
    "    file_metadata = {}\n",
    "    if store_md5:\n",
    "        file_metadata['md5'] = helpers.md5sum(file_path)\n",
    "    if store_mimetype:\n",
    "        file_metadata['mimetype'], _, _ = helpers.extract_mimetype(file_path)\n",
    "    if store_file_dates:\n",
    "        file_metadata['createdate'], file_metadata['modifydate'] = helpers.extract_file_dates(file_path)\n",
    "    ext = helpers.extract_extension(file_path)\n",
    "    file_metadata['extension'] = ext\n",
    "    file_metadata.update(_apply_file_callables(file_path, file_callables))\n",
    "        \n",
    "    # load file\n",
    "    if ext in LOADER_MAPPING:\n",
    "        try:\n",
    "            if ext == PDF:\n",
    "                if pdf_unstructured:\n",
    "                    ext = PDFOCR\n",
    "                elif pdf_markdown:\n",
    "                    ext = PDFMD\n",
    "            loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "            loader_args = loader_args.copy() # copy so any supplied kwargs do not persist across calls\n",
    "            if ext in PDF_EXTS:\n",
    "                loader_args.update(kwargs)\n",
    "            loader = loader_class(file_path, **loader_args)\n",
    "            if ext in PDF_EXTS and ext != PDFOCR:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "                    docs = loader.load()\n",
    "                    file_metadata.update(_apply_text_callables(docs, text_callables))\n",
    "                    docs = _update_metadata(docs, file_metadata)\n",
    "                if not docs or len('\\n'.join([d.page_content.strip() for d in docs]).strip()) < OCR_CHAR_THRESH:\n",
    "                    loader_class, loader_args = LOADER_MAPPING[PDFOCR]\n",
    "                    loader = loader_class(file_path, **loader_args)\n",
    "                    file_metadata['ocr'] = True\n",
    "                    docs = loader.load()\n",
    "                    for doc in docs:\n",
    "                        doc.metadata['source'] = file_path\n",
    "                    file_metadata.update(_apply_text_callables(docs, text_callables))\n",
    "                    docs = _update_metadata(docs, file_metadata)\n",
    "            else:\n",
    "                docs = loader.load()\n",
    "                file_metadata.update(_apply_text_callables(docs, text_callables))                \n",
    "                docs = _update_metadata(docs, file_metadata)\n",
    "            extra_keys = list(file_metadata.keys() | text_callables.keys())\n",
    "            return helpers.set_metadata_defaults(docs, extra_keys=extra_keys)\n",
    "        except Exception as e:\n",
    "            logger.warning(f'\\nSkipping {file_path} due to error: {str(e)}')\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    else:\n",
    "        logger.warning(f\"\\nSkipping {file_path} due to unsupported file extension: '{ext}'\")\n",
    "\n",
    "\n",
    "def _ignore_file(file_path, ignored_files:List[str]=[], ignore_fn:Optional[Callable]=None):\n",
    "    # Normalize path for consistent handling - first get absolute path, then normalize path separators\n",
    "    file_path = os.path.abspath(file_path)\n",
    "    file_path = os.path.normpath(file_path).replace('\\\\', '/')\n",
    "    \n",
    "    # Normalize ignored_files as well for consistent comparison\n",
    "    normalized_ignored_files = [os.path.normpath(f).replace('\\\\', '/') for f in ignored_files]\n",
    "    \n",
    "    return file_path in normalized_ignored_files or \\\n",
    "            os.path.basename(file_path).startswith('~$') or \\\n",
    "            (ignore_fn is not None and ignore_fn(file_path))\n",
    "\n",
    "\n",
    "def load_documents(source_dir: str, # path to folder containing documents\n",
    "                   ignored_files: List[str] = [], # list of filepaths to ignore\n",
    "                   ignore_fn:Optional[Callable] = None, # callable that accepts file path and returns True for ignored files\n",
    "                   caption_tables:bool=False,# If True, agument table text with summaries of tables if infer_table_structure is True.\n",
    "                   extract_document_titles:bool=False, # If True, infer document title and attach to individual chunks\n",
    "                   llm=None, # a reference to the LLM (used by `caption_tables` and `extract_document_titles`\n",
    "                   n_proc:Optional[int]=None, # number of CPU cores to use for text extraction. If None, use maximum for system.\n",
    "                   verbose:bool=True, # verbosity\n",
    "                   preserve_paragraphs:bool=False, # This is not used here and is only included to mask it from being forwarded to `load_single_document`.\n",
    "                   **kwargs\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files.\n",
    "    Extra kwargs fed to `ingest.load_single_document`.\n",
    "\n",
    "    Returns a generator over documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def keep(file_path):\n",
    "        return not _ignore_file(file_path, ignored_files, ignore_fn)\n",
    "    def all_files():\n",
    "        for f in helpers.extract_files(source_dir, LOADER_MAPPING):\n",
    "            yield f\n",
    "    filtered_files = filtered_generator(all_files(), criteria=[keep])\n",
    "    total = sum(1 for _ in filtered_generator(all_files(), criteria=[keep]))\n",
    "    load_args = kwargs.copy()\n",
    "\n",
    "    if n_proc==1:\n",
    "        prog_msg = 'Loading documents...'\n",
    "        mybar = None\n",
    "        for i, filtered_file in enumerate(tqdm(filtered_files, total=total)):\n",
    "            docs = load_single_document(filtered_file, **load_args)\n",
    "            if docs is None: continue\n",
    "            if llm and caption_tables:\n",
    "                summarize_tables(docs, llm=llm, **kwargs)\n",
    "            if llm and extract_document_titles:\n",
    "                title = extract_title(docs, llm=llm, **kwargs)\n",
    "                for doc in docs:\n",
    "                    if title:\n",
    "                        doc.metadata['document_title'] = title\n",
    "            yield from docs\n",
    "    else:\n",
    "        if kwargs.get('infer_table_structure', False):\n",
    "            # Use \"spawn\" if using TableTransformers\n",
    "            # Reference: https://github.com/pytorch/pytorch/issues/40403\n",
    "            multiprocessing.set_start_method('spawn', force=True)\n",
    "            # call helpers.extract_tables sequentially below instead of in load_single_document if n_proc>1\n",
    "            # because helpers.extract_tables is not well-suited to multiprocessing even with line above\n",
    "            if not n_proc or n_proc>1:\n",
    "                load_args = {k:load_args[k] for k in load_args if k!='infer_table_structure'}\n",
    "        with multiprocessing.Pool(processes=n_proc if n_proc else os.cpu_count()) as pool:\n",
    "            results = []\n",
    "            with tqdm(\n",
    "                total=total, desc=\"Loading new documents\", ncols=80, disable=not verbose\n",
    "            ) as pbar:\n",
    "                for i, docs in enumerate(\n",
    "                    pool.imap_unordered(functools.partial(load_single_document, **load_args),\n",
    "                                                          filtered_files)\n",
    "                ):\n",
    "                    pbar.update()\n",
    "                    if docs is None: continue\n",
    "                    if kwargs.get('infer_table_structure', False):\n",
    "                        docs = helpers.extract_tables(docs=docs)\n",
    "                    if llm and caption_tables:\n",
    "                        summarize_tables(docs, llm=llm, **kwargs)\n",
    "                    if llm and extract_document_titles:\n",
    "                        title = extract_title(docs, llm=llm, **kwargs)\n",
    "                        for doc in docs:\n",
    "                            if title:\n",
    "                                doc.metadata['document_title'] = title\n",
    "                    yield from docs\n",
    "            # Make sure to close the pool when done\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "\n",
    "def process_folder(\n",
    "    source_directory: str, # path to folder containing document store\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    ignored_files: List[str] = [], # list of files to ignore\n",
    "    ignore_fn:Optional[Callable] = None, # Callable that accepts the file path (including file name) as input and ignores if returns True\n",
    "    batch_size:int=CHROMA_MAX, # batch size used when processing documents\n",
    "    **kwargs\n",
    "\n",
    "\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from folder, extract text from them, split texts into chunks.\n",
    "    Extra kwargs fed to `ingest.load_documents` and `ingest.load_single_document`.\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory,\n",
    "                              ignored_files, ignore_fn=ignore_fn,\n",
    "                              **kwargs)\n",
    "    documents = list(documents)\n",
    "\n",
    "    if not documents:\n",
    "        print(\"No new documents to process\")\n",
    "        return\n",
    "\n",
    "    batches = batch_list(documents, batch_size)\n",
    "    total = sum(1 for _ in batch_list(documents, batch_size))\n",
    "    for docs in tqdm(batches, total=total,\n",
    "                     desc=f'Processing and chunking {len(documents)} new documents'):\n",
    "        yield from chunk_documents(docs,\n",
    "                                  chunk_size = chunk_size,\n",
    "                                  chunk_overlap = chunk_overlap,\n",
    "                                  **kwargs)\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "    documents: list, # list of LangChain Documents\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    infer_table_structure:bool = False, # This should be set to True if `documents` may contain contain tables (i.e., `doc.metadata['table']=True`).\n",
    "    preserve_paragraphs:bool=False, # If True, strictly chunk by paragraph and only split if paragraph exceeds `chunk_size`. If False, small paragraphs will be accumulated into a single chunk until `chunk_size` is exceeded.\n",
    "    **kwargs\n",
    "\n",
    "\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process list of Documents by splitting into chunks.\n",
    "    \"\"\"\n",
    "    # remove tables before chunking\n",
    "    if infer_table_structure and not kwargs.get('pdf_unstructured', False):\n",
    "        tables = [d for d in documents if d.metadata.get('table', False)]\n",
    "        docs = [d for d in documents if not d.metadata.get('table', False)]\n",
    "    else:\n",
    "        tables = []\n",
    "        docs = documents\n",
    "\n",
    "    # initialize the splitter\n",
    "    contains_markdown = kwargs.get('pdf_markdown', False)\n",
    "    if contains_markdown:\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.MARKDOWN,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap)\n",
    "    elif preserve_paragraphs:\n",
    "        text_splitter = helpers.ParagraphTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    # split non-table texts\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    # attempt to remove text chunks containing mangled tables\n",
    "    texts = [d for d in texts if not helpers.includes_caption(d)]\n",
    "\n",
    "    # split table texts\n",
    "    if tables:\n",
    "        table_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.MARKDOWN,\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=0)\n",
    "        table_texts = table_splitter.split_documents(tables)\n",
    "        texts.extend(table_texts)\n",
    "\n",
    "    # attach document title to each chunk (where title was extracted earlier by `load_documents`)\n",
    "    if kwargs.get('extract_document_titles', False):\n",
    "        for text in texts:\n",
    "            if text.metadata.get('document_title', ''):\n",
    "                text.page_content = f'The content below is from a document titled, \\\"{text.metadata[\"document_title\"]}\\\"\\n\\n{text.page_content}'\n",
    "    return texts\n",
    "\n",
    "\n",
    "def does_vectorstore_exist(db) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if not db.get()[\"documents\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def batchify_chunks(texts, batch_size=CHROMA_MAX):\n",
    "    \"\"\"\n",
    "    split texts into batches specifically for Chroma\n",
    "    \"\"\"\n",
    "    split_docs_chunked = batch_list(texts, batch_size)\n",
    "    total_chunks = sum(1 for _ in batch_list(texts, batch_size))\n",
    "    return split_docs_chunked, total_chunks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class VectorStore(ABC):\n",
    "\n",
    "    def ingest(\n",
    "        self,\n",
    "        source_directory: str, # path to folder containing document store\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n",
    "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        ignore_fn:Optional[Callable] = None, # Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested.\n",
    "        batch_size:int=CHROMA_MAX, # batch size used when processing documents\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_directory` (previously-ingested documents are\n",
    "        ignored). When retrieved, the\n",
    "        [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
    "        objects will each have a `metadata` dict with the absolute path to the file\n",
    "        in `metadata[\"source\"]`.\n",
    "        Extra kwargs fed to `ingest.load_single_document`.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(source_directory):\n",
    "            raise ValueError(\"The source_directory does not exist.\")\n",
    "        elif os.path.isfile(source_directory):\n",
    "            raise ValueError(\n",
    "                \"The source_directory argument must be a folder, not a file.\"\n",
    "            )\n",
    "        texts = None\n",
    "        if self.exists():\n",
    "            # Update and store locally vectorstore\n",
    "            print(f\"Appending to existing vectorstore at {self.persist_directory}\")\n",
    "            # Get existing sources and normalize them for consistent comparison\n",
    "            ignored_files = set([os.path.normpath(d['source']).replace('\\\\', '/') \n",
    "                                for d in self.get_all_docs()])\n",
    "        else:\n",
    "            print(f\"Creating new vectorstore at {self.persist_directory}\")\n",
    "            ignored_files = []\n",
    "\n",
    "        texts = process_folder(\n",
    "            source_directory,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            ignored_files=ignored_files,\n",
    "            ignore_fn=ignore_fn,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs\n",
    "\n",
    "        )\n",
    "\n",
    "        texts = list(texts)\n",
    "        print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} chars each for text; max. {TABLE_CHUNK_SIZE} chars for tables)\")\n",
    "\n",
    "        self.add_documents(texts, batch_size=batch_size)\n",
    "\n",
    "        if texts:\n",
    "            print(\n",
    "                \"Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\"\n",
    "            )\n",
    "        db = None\n",
    "        return\n",
    "    \n",
    "    def init_embedding_model(self, \n",
    "                             embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                             embedding_model_kwargs: Optional[dict] = None,\n",
    "                             embedding_encode_kwargs: dict = {\"normalize_embeddings\": False},\n",
    "                             **kwargs\n",
    "                             ):\n",
    "        \"\"\"\n",
    "        Instantiate embedding model\n",
    "        \"\"\"\n",
    "        if not embedding_model_kwargs:\n",
    "            import torch\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            embedding_model_kwargs = {\"device\": device}          \n",
    "        self.embeddings =  HuggingFaceEmbeddings(model_name=embedding_model_name, \n",
    "                                     model_kwargs=embedding_model_kwargs,\n",
    "                                     encode_kwargs=embedding_encode_kwargs)\n",
    "\n",
    "    def get_embedding_model(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance\n",
    "        \"\"\"\n",
    "        return self.embeddings\n",
    "\n",
    "\n",
    "    def compute_similarity(self, query:str, texts:list):\n",
    "        \"\"\"\n",
    "        Computes semantic similarity between a query and a list of texts\n",
    "        \"\"\"\n",
    "        from sentence_transformers import util\n",
    "        import torch\n",
    "\n",
    "        embeddings = self.get_embedding_model()\n",
    "\n",
    "        # Compute embeddings\n",
    "        query_emb = torch.tensor(embeddings.embed_query(query)).unsqueeze(0)  # Shape (1, embedding_dim)\n",
    "        text_embs = torch.tensor(embeddings.embed_documents(texts))  # Shape (len(texts), embedding_dim)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cos_scores = util.pytorch_cos_sim(query_emb, text_embs).squeeze(0).tolist()  # Shape (len(texts),)\n",
    "\n",
    "        return cos_scores\n",
    "\n",
    "\n",
    "    def check(self):\n",
    "        \"\"\"\n",
    "        Raise exception if `VectorStore.exists()` returns False\n",
    "        \"\"\"\n",
    "        if not self.exists():\n",
    "            raise Exception('The vector store is either empty or does not yet exist. '+\\\n",
    "                            'Please invoke the `ingest` method or `add_document` method.')\n",
    "\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if vector store has been initialized and contains documents.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Stores instances of `langchain_core.documents.base.Document` in vector store\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    @abstractmethod\n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a single document.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def remove_source(self, source):\n",
    "        \"\"\"\n",
    "        Remove documents by source\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update_documents(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Update a set of documents.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a list of files previously added to vector store.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_doc(self, id):\n",
    "        \"\"\"\n",
    "        Retrieve a document by ID\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get total number of records added to vector store\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def erase(self):\n",
    "        \"\"\"\n",
    "        Removes all documents in vector store\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def query(self, query, limit=4):\n",
    "        \"\"\"\n",
    "        Queries the vector store.\n",
    "        For sparse stores, this is simply a keyword-search.\n",
    "        For dense stores, this is equivalent to semantic_search except results\n",
    "        are in the form of dictionary with keys 'hits' and 'total_hits'.\n",
    "        Method must include 'query' as first positional argument and \"limit\" as keyword argument.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def semantic_search(self):\n",
    "        \"\"\"\n",
    "        Semantic search of vector store.\n",
    "        Expected to return a list of Langchain Document objects.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def search(self, query: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Generic search method that invokes the store's query method.\n",
    "        This provides a consistent interface across all store types.\n",
    "        \"\"\"\n",
    "        return self.query(query, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
