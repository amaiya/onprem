{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.base\n",
    "\n",
    "> functionality for text extraction and document ingestion into a vector database for question-answering and other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from onprem.llm.helpers import summarize_tables, extract_title\n",
    "from onprem.utils import batch_list, filtered_generator\n",
    "from onprem.ingest import helpers\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters.base import Language\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    TextLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredExcelLoader,\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from typing import List, Optional, Callable\n",
    "import multiprocessing\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logger = logging.getLogger('OnPrem.LLM-ingest')\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 500\n",
    "DEFAULT_CHUNK_OVERLAP = 50\n",
    "TABLE_CHUNK_SIZE = 2000\n",
    "CHROMA_MAX = 41000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if \"text/html content not found in email\" in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"] = \"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "        return doc\n",
    "\n",
    "\n",
    "class MyUnstructuredPDFLoader(UnstructuredPDFLoader):\n",
    "    \"\"\"Custom PDF Loader\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper UnstructuredPDFLoader\"\"\"\n",
    "        try:\n",
    "            docs = UnstructuredPDFLoader.load(self)\n",
    "            if not docs:\n",
    "                raise Exception('Document had no content. ')\n",
    "            tables = [d.metadata['text_as_html'] for d in docs if d.metadata.get('text_as_html', None) is not None]\n",
    "            texts = [d.page_content for d in docs if d.metadata.get('text_as_html', None) is None]\n",
    "\n",
    "            page_content = '\\n'.join(texts)\n",
    "            docs = [helpers.create_document(page_content, source=self.file_path)]\n",
    "            table_docs = [helpers.create_document(t, source=self.file_path, table=True) for t in tables]\n",
    "            docs.extend(table_docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n",
    "class _PyMuPDFLoader(PyMuPDFLoader):\n",
    "    \"\"\"Custom PyMUPDF Loader with optional support for inferring table structure\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            # PyMuPDFLoader complains when you add custom flags to text_kwargs,\n",
    "            # so delete before loading\n",
    "            infer_table_structure = self.parser.text_kwargs.get('infer_table_structure', False)\n",
    "            if 'infer_table_structure' in self.parser.text_kwargs:\n",
    "                del self.parser.text_kwargs['infer_table_structure']\n",
    "            docs = PyMuPDFLoader.load(self)\n",
    "            if infer_table_structure:\n",
    "                docs = helpers.extract_tables(docs=docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n",
    "class PDF2MarkdownLoader(_PyMuPDFLoader):\n",
    "    \"\"\"Custom PDF to Markdown Loader\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        import pymupdf4llm\n",
    "        try:\n",
    "            md_text = pymupdf4llm.to_markdown(self.file_path, show_progress=False)\n",
    "            if not md_text.strip():\n",
    "                raise Exception('Document had no content. ')\n",
    "            doc = helpers.create_document(md_text, source=self.file_path, markdown=True)\n",
    "            docs = [doc]\n",
    "            if self.parser.text_kwargs.get('infer_table_structure', False):\n",
    "                docs = helpers.extract_tables(docs=docs)\n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise Exception(f'{self.file_path} : {e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Map file extensions to document loaders and their arguments\n",
    "PDFOCR = 'pdfOCR'\n",
    "PDFMD = 'pdfMD'\n",
    "PDF = 'pdf'\n",
    "PDF_EXTS = [PDF, PDFOCR, PDFMD]\n",
    "OCR_CHAR_THRESH = 32\n",
    "LOADER_MAPPING = {\n",
    "    \"csv\": (CSVLoader, {}),\n",
    "    \"doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \"docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \"xlsx\": (UnstructuredExcelLoader, {}),\n",
    "    \"enex\": (EverNoteLoader, {}),\n",
    "    \"eml\": (MyElmLoader, {}),\n",
    "    \"epub\": (UnstructuredEPubLoader, {}),\n",
    "    \"html\": (UnstructuredHTMLLoader, {}),\n",
    "    \"htm\": (UnstructuredHTMLLoader, {}),\n",
    "    \"md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \"odt\": (UnstructuredODTLoader, {}),\n",
    "    \"ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \"pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \"txt\": (TextLoader, {\"autodetect_encoding\": True}),\n",
    "    \"json\": (TextLoader, {\"autodetect_encoding\": True}),\n",
    "    PDF   : (_PyMuPDFLoader, {}),\n",
    "    PDFMD: (PDF2MarkdownLoader, {}),\n",
    "    PDFOCR: (MyUnstructuredPDFLoader, {\"infer_table_structure\":False, \"mode\":\"elements\", \"strategy\":\"hi_res\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "def _update_metadata(docs:List[Document], metadata:dict):\n",
    "    \"\"\"\n",
    "    Update metadata in docs with supplied metadata dictionary\n",
    "    \"\"\"\n",
    "    for doc in docs:\n",
    "        doc.metadata.update(metadata)\n",
    "    return docs\n",
    "\n",
    "def _apply_text_callables(docs:List[Document], text_callables:dict):\n",
    "    \"\"\"\n",
    "    Invokes text_callables on entire text of document.\n",
    "\n",
    "    Returns a dictionary with values containing results from callables for each key\n",
    "    \"\"\"\n",
    "    if not text_callables: return {}\n",
    "        \n",
    "    text = '\\n\\n'.join([d.page_content for d in docs])\n",
    "    results = {}\n",
    "    for k,v in text_callables.items():\n",
    "        results[k] = v(text)\n",
    "    return results\n",
    "\n",
    "def _apply_file_callables(file_path:str, file_callables:dict):\n",
    "    \"\"\"\n",
    "    Invokes file_callables on file path.\n",
    "\n",
    "    Returns a dictionary with values containing results from callables for each key\n",
    "    \"\"\"\n",
    "    if not file_callables: return {}\n",
    "        \n",
    "    if not os.path.exists(file_path):\n",
    "        raise ValueError('file_path does not exist: {file_path}')\n",
    "    \n",
    "    results = {}\n",
    "    for k,v in file_callables.items():\n",
    "        results[k] = v(file_path)\n",
    "    return results\n",
    "\n",
    "    \n",
    "def load_single_document(file_path: str, # path to file\n",
    "                         pdf_unstructured:bool=False, # use unstructured for PDF extraction if True (will also OCR if necessary)\n",
    "                         pdf_markdown:bool = False, # Convert PDFs to Markdown instead of plain text if True.\n",
    "                         store_md5:bool=False, # Extract and store MD5 of document in metadata\n",
    "                         store_mimetype:bool=False, # Guess and store mime type of document in metadata\n",
    "                         store_file_dates:bool=False, # Extract snd store file dates in metadata\n",
    "                         file_callables:Optional[dict]=None, # optional dict with  keys and functions called with filepath as argument. Results stored as metadata.\n",
    "                         text_callables:Optional[dict]=None, # optional dict with  keys and functions called with file text as argument. Results stored as metadata.\n",
    "                         **kwargs,\n",
    "                         ) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extract text from a single document. Will attempt to OCR PDFs, if necessary.\n",
    "\n",
    "\n",
    "    Note that extra kwargs can be supplied to configure the behavior of PDF loaders.\n",
    "    For instance, supplying `infer_table_structure` will cause `load_single_document` to try and\n",
    "    infer and extract tables from PDFs. When `pdf_unstructured=True` and `infer_table_structure=True`,\n",
    "    tables are represented as HTML within the main body of extracted text. In all other cases, inferred tables\n",
    "    are represented as Markdown and appended to the end of the extracted text when `infer_table_structure=True`.\n",
    "    \"\"\"\n",
    "    if pdf_unstructured and pdf_markdown:\n",
    "        raise ValueError('pdf_unstructured and pdf_markdown cannot both be True.')\n",
    "    file_callables = {} if not file_callables else file_callables\n",
    "    text_callables = {} if not text_callables else text_callables\n",
    "    \n",
    "    # Normalize path for consistent handling - first get absolute path\n",
    "    file_path = os.path.abspath(file_path)\n",
    "    # Then normalize and standardize path separators to forward slashes for cross-platform consistency\n",
    "    file_path = os.path.normpath(file_path).replace('\\\\', '/')\n",
    "\n",
    "\n",
    "    # extract metadata\n",
    "    file_metadata = {}\n",
    "    if store_md5:\n",
    "        file_metadata['md5'] = helpers.md5sum(file_path)\n",
    "    if store_mimetype:\n",
    "        file_metadata['mimetype'], _, _ = helpers.extract_mimetype(file_path)\n",
    "    if store_file_dates:\n",
    "        file_metadata['createdate'], file_metadata['modifydate'] = helpers.extract_file_dates(file_path)\n",
    "    ext = helpers.extract_extension(file_path)\n",
    "    file_metadata['extension'] = ext\n",
    "    file_metadata.update(_apply_file_callables(file_path, file_callables))\n",
    "        \n",
    "    # load file\n",
    "    if ext in LOADER_MAPPING:\n",
    "        try:\n",
    "            if ext == PDF:\n",
    "                if pdf_unstructured:\n",
    "                    ext = PDFOCR\n",
    "                elif pdf_markdown:\n",
    "                    ext = PDFMD\n",
    "            loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "            loader_args = loader_args.copy() # copy so any supplied kwargs do not persist across calls\n",
    "            if ext in PDF_EXTS:\n",
    "                loader_args.update(kwargs)\n",
    "            loader = loader_class(file_path, **loader_args)\n",
    "            if ext in PDF_EXTS and ext != PDFOCR:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "                    docs = loader.load()\n",
    "                    file_metadata.update(_apply_text_callables(docs, text_callables))\n",
    "                    docs = _update_metadata(docs, file_metadata)\n",
    "                if not docs or len('\\n'.join([d.page_content.strip() for d in docs]).strip()) < OCR_CHAR_THRESH:\n",
    "                    loader_class, loader_args = LOADER_MAPPING[PDFOCR]\n",
    "                    loader = loader_class(file_path, **loader_args)\n",
    "                    file_metadata['ocr'] = True\n",
    "                    docs = loader.load()\n",
    "                    for doc in docs:\n",
    "                        doc.metadata['source'] = file_path\n",
    "                    file_metadata.update(_apply_text_callables(docs, text_callables))\n",
    "                    docs = _update_metadata(docs, file_metadata)\n",
    "            else:\n",
    "                docs = loader.load()\n",
    "                file_metadata.update(_apply_text_callables(docs, text_callables))                \n",
    "                docs = _update_metadata(docs, file_metadata)\n",
    "            extra_keys = list(file_metadata.keys() | text_callables.keys())\n",
    "            return helpers.set_metadata_defaults(docs, extra_keys=extra_keys)\n",
    "        except Exception as e:\n",
    "            logger.warning(f'\\nSkipping {file_path} due to error: {str(e)}')\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    else:\n",
    "        logger.warning(f\"\\nSkipping {file_path} due to unsupported file extension: '{ext}'\")\n",
    "\n",
    "\n",
    "def _ignore_file(file_path, ignored_files:List[str]=[], ignore_fn:Optional[Callable]=None):\n",
    "    # Normalize path for consistent handling - first get absolute path, then normalize path separators\n",
    "    file_path = os.path.abspath(file_path)\n",
    "    file_path = os.path.normpath(file_path).replace('\\\\', '/')\n",
    "    \n",
    "    # Normalize ignored_files as well for consistent comparison\n",
    "    normalized_ignored_files = [os.path.normpath(f).replace('\\\\', '/') for f in ignored_files]\n",
    "    \n",
    "    return file_path in normalized_ignored_files or \\\n",
    "            os.path.basename(file_path).startswith('~$') or \\\n",
    "            (ignore_fn is not None and ignore_fn(file_path))\n",
    "\n",
    "\n",
    "def load_documents(source_dir: str, # path to folder containing documents\n",
    "                   ignored_files: List[str] = [], # list of filepaths to ignore\n",
    "                   ignore_fn:Optional[Callable] = None, # callable that accepts file path and returns True for ignored files\n",
    "                   caption_tables:bool=False,# If True, agument table text with summaries of tables if infer_table_structure is True.\n",
    "                   extract_document_titles:bool=False, # If True, infer document title and attach to individual chunks\n",
    "                   llm=None, # a reference to the LLM (used by `caption_tables` and `extract_document_titles`\n",
    "                   n_proc:Optional[int]=None, # number of CPU cores to use for text extraction. If None, use maximum for system.\n",
    "                   verbose:bool=True, # verbosity\n",
    "                   preserve_paragraphs:bool=False, # This is not used here and is only included to mask it from being forwarded to `load_single_document`.\n",
    "                   **kwargs\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files.\n",
    "    Extra kwargs fed to `ingest.load_single_document`.\n",
    "\n",
    "    Returns a generator over documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def keep(file_path):\n",
    "        return not _ignore_file(file_path, ignored_files, ignore_fn)\n",
    "    def all_files():\n",
    "        for f in helpers.extract_files(source_dir, LOADER_MAPPING):\n",
    "            yield f\n",
    "    filtered_files = filtered_generator(all_files(), criteria=[keep])\n",
    "    total = sum(1 for _ in filtered_generator(all_files(), criteria=[keep]))\n",
    "    load_args = kwargs.copy()\n",
    "\n",
    "    if n_proc==1:\n",
    "        prog_msg = 'Loading documents...'\n",
    "        mybar = None\n",
    "        for i, filtered_file in enumerate(tqdm(filtered_files, total=total)):\n",
    "            docs = load_single_document(filtered_file, **load_args)\n",
    "            if docs is None: continue\n",
    "            if llm and caption_tables:\n",
    "                summarize_tables(docs, llm=llm, **kwargs)\n",
    "            if llm and extract_document_titles:\n",
    "                title = extract_title(docs, llm=llm, **kwargs)\n",
    "                for doc in docs:\n",
    "                    if title:\n",
    "                        doc.metadata['document_title'] = title\n",
    "            yield from docs\n",
    "    else:\n",
    "        if kwargs.get('infer_table_structure', False):\n",
    "            # Use \"spawn\" if using TableTransformers\n",
    "            # Reference: https://github.com/pytorch/pytorch/issues/40403\n",
    "            multiprocessing.set_start_method('spawn', force=True)\n",
    "            # call helpers.extract_tables sequentially below instead of in load_single_document if n_proc>1\n",
    "            # because helpers.extract_tables is not well-suited to multiprocessing even with line above\n",
    "            if not n_proc or n_proc>1:\n",
    "                load_args = {k:load_args[k] for k in load_args if k!='infer_table_structure'}\n",
    "        with multiprocessing.Pool(processes=n_proc if n_proc else os.cpu_count()) as pool:\n",
    "            results = []\n",
    "            with tqdm(\n",
    "                total=total, desc=\"Loading new documents\", ncols=80, disable=not verbose\n",
    "            ) as pbar:\n",
    "                for i, docs in enumerate(\n",
    "                    pool.imap_unordered(functools.partial(load_single_document, **load_args),\n",
    "                                                          filtered_files)\n",
    "                ):\n",
    "                    pbar.update()\n",
    "                    if docs is None: continue\n",
    "                    if kwargs.get('infer_table_structure', False):\n",
    "                        docs = helpers.extract_tables(docs=docs)\n",
    "                    if llm and caption_tables:\n",
    "                        summarize_tables(docs, llm=llm, **kwargs)\n",
    "                    if llm and extract_document_titles:\n",
    "                        title = extract_title(docs, llm=llm, **kwargs)\n",
    "                        for doc in docs:\n",
    "                            if title:\n",
    "                                doc.metadata['document_title'] = title\n",
    "                    yield from docs\n",
    "            # Make sure to close the pool when done\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "\n",
    "def process_folder(\n",
    "    source_directory: str, # path to folder containing document store\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    ignored_files: List[str] = [], # list of files to ignore\n",
    "    ignore_fn:Optional[Callable] = None, # Callable that accepts the file path (including file name) as input and ignores if returns True\n",
    "    batch_size:int=CHROMA_MAX, # batch size used when processing documents\n",
    "    **kwargs\n",
    "\n",
    "\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from folder, extract text from them, split texts into chunks.\n",
    "    Extra kwargs fed to `ingest.load_documents` and `ingest.load_single_document`.\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory,\n",
    "                              ignored_files, ignore_fn=ignore_fn,\n",
    "                              **kwargs)\n",
    "    documents = list(documents)\n",
    "\n",
    "    if not documents:\n",
    "        print(\"No new documents to process\")\n",
    "        return\n",
    "\n",
    "    batches = batch_list(documents, batch_size)\n",
    "    total = sum(1 for _ in batch_list(documents, batch_size))\n",
    "    for docs in tqdm(batches, total=total,\n",
    "                     desc=f'Processing and chunking {len(documents)} new documents'):\n",
    "        yield from chunk_documents(docs,\n",
    "                                  chunk_size = chunk_size,\n",
    "                                  chunk_overlap = chunk_overlap,\n",
    "                                  **kwargs)\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "    documents: list, # list of LangChain Documents or list of text strings\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "    infer_table_structure:bool = False, # This should be set to True if `documents` may contain contain tables (i.e., `doc.metadata['table']=True`).\n",
    "    preserve_paragraphs:bool=False, # If True, strictly chunk by paragraph and only split if paragraph exceeds `chunk_size`. If False, small paragraphs will be accumulated into a single chunk until `chunk_size` is exceeded.\n",
    "    **kwargs\n",
    "\n",
    "\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Process list of Documents or text strings by splitting into chunks.\n",
    "    If text strings are provided, they will be converted to Document objects internally.\n",
    "    \"\"\"\n",
    "    # Convert text strings to Documents if needed\n",
    "    if documents and isinstance(documents[0], str):\n",
    "        documents = [Document(page_content=text, metadata={}) for text in documents]\n",
    "    # remove tables before chunking\n",
    "    if infer_table_structure and not kwargs.get('pdf_unstructured', False):\n",
    "        tables = [d for d in documents if d.metadata.get('table', False)]\n",
    "        docs = [d for d in documents if not d.metadata.get('table', False)]\n",
    "    else:\n",
    "        tables = []\n",
    "        docs = documents\n",
    "\n",
    "    # initialize the splitter\n",
    "    contains_markdown = kwargs.get('pdf_markdown', False)\n",
    "    if contains_markdown:\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.MARKDOWN,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap)\n",
    "    elif preserve_paragraphs:\n",
    "        text_splitter = helpers.ParagraphTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    # split non-table texts\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    # attempt to remove text chunks containing mangled tables\n",
    "    texts = [d for d in texts if not helpers.includes_caption(d)]\n",
    "\n",
    "    # split table texts\n",
    "    if tables:\n",
    "        table_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.MARKDOWN,\n",
    "            chunk_size=2000,\n",
    "            chunk_overlap=0)\n",
    "        table_texts = table_splitter.split_documents(tables)\n",
    "        texts.extend(table_texts)\n",
    "\n",
    "    # attach document title to each chunk (where title was extracted earlier by `load_documents`)\n",
    "    if kwargs.get('extract_document_titles', False):\n",
    "        for text in texts:\n",
    "            if text.metadata.get('document_title', ''):\n",
    "                text.page_content = f'The content below is from a document titled, \\\"{text.metadata[\"document_title\"]}\\\"\\n\\n{text.page_content}'\n",
    "    return texts\n",
    "\n",
    "\n",
    "def does_vectorstore_exist(db) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if not db.get()[\"documents\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def batchify_chunks(texts, batch_size=CHROMA_MAX):\n",
    "    \"\"\"\n",
    "    split texts into batches specifically for Chroma\n",
    "    \"\"\"\n",
    "    split_docs_chunked = batch_list(texts, batch_size)\n",
    "    total_chunks = sum(1 for _ in batch_list(texts, batch_size))\n",
    "    return split_docs_chunked, total_chunks\n",
    "\n",
    "\n",
    "def load_web_document(url, username=None, password=None):\n",
    "    \"\"\"\n",
    "    Download and extract text from a web document using load_single_document.\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to download from\n",
    "        username: Optional username for authentication (e.g., for SharePoint)\n",
    "        password: Optional password for authentication (e.g., for SharePoint)\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects\n",
    "    \"\"\"\n",
    "    import tempfile\n",
    "    import os\n",
    "    import requests\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    # Parse URL to get file extension if available\n",
    "    parsed_url = urlparse(url)\n",
    "    path_parts = parsed_url.path.split('/')\n",
    "    filename = path_parts[-1] if path_parts else 'document'\n",
    "    \n",
    "    # Set up authentication if credentials provided\n",
    "    auth = None\n",
    "    if username and password:\n",
    "        try:\n",
    "            from requests_ntlm import HttpNtlmAuth\n",
    "            auth = HttpNtlmAuth(username, password)\n",
    "        except ImportError:\n",
    "            # Fall back to basic auth if requests_ntlm not available\n",
    "            from requests.auth import HTTPBasicAuth\n",
    "            auth = HTTPBasicAuth(username, password)\n",
    "    \n",
    "    # If no extension, try to determine from Content-Type header\n",
    "    if '.' not in filename:\n",
    "        try:\n",
    "            response = requests.head(url, timeout=10, auth=auth)\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "            if 'pdf' in content_type:\n",
    "                filename += '.pdf'\n",
    "            elif 'html' in content_type:\n",
    "                filename += '.html'\n",
    "            elif 'text' in content_type:\n",
    "                filename += '.txt'\n",
    "            else:\n",
    "                filename += '.html'  # Default fallback\n",
    "        except:\n",
    "            filename += '.html'  # Default fallback\n",
    "    \n",
    "    # Download the file to a temporary location\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as temp_file:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30, auth=auth)\n",
    "            response.raise_for_status()\n",
    "            temp_file.write(response.content)\n",
    "            temp_path = temp_file.name\n",
    "            \n",
    "            # Use load_single_document to extract text\n",
    "            docs = load_single_document(temp_path)\n",
    "            \n",
    "            # Update source to original URL\n",
    "            if docs:\n",
    "                for doc in docs:\n",
    "                    doc.metadata['source'] = url\n",
    "                    doc.metadata['original_source'] = url\n",
    "            \n",
    "            return docs\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            try:\n",
    "                os.unlink(temp_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "def load_spreadsheet_documents(file_path, text_column, metadata_columns=None, sheet_name=None):\n",
    "    \"\"\"\n",
    "    Load documents from a spreadsheet where each row becomes a document.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the spreadsheet file (.xlsx, .xls, .csv)\n",
    "        text_column: Name of the column containing the text content\n",
    "        metadata_columns: List of column names to include as metadata (default: all other columns)\n",
    "        sheet_name: For Excel files, name of the sheet to read (default: first sheet)\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects, one per row\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except ImportError:\n",
    "        raise ImportError(\"pandas is required for spreadsheet loading. Install with: pip install pandas\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Spreadsheet file not found: {file_path}\")\n",
    "    \n",
    "    # Determine file type and load accordingly\n",
    "    file_ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        if file_ext == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_ext in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            # Handle case where pd.read_excel returns a dict of DataFrames\n",
    "            if isinstance(df, dict):\n",
    "                if sheet_name and sheet_name in df:\n",
    "                    df = df[sheet_name]\n",
    "                else:\n",
    "                    # Take the first sheet if sheet_name not specified or not found\n",
    "                    df = list(df.values())[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_ext}. Supported formats: .csv, .xlsx, .xls\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read spreadsheet: {str(e)}\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(f\"Spreadsheet is empty: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Validate text column exists\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Text column '{text_column}' not found. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Determine metadata columns\n",
    "    if metadata_columns is None:\n",
    "        # Use all columns except the text column\n",
    "        metadata_columns = [col for col in df.columns if col != text_column]\n",
    "    else:\n",
    "        # Validate specified metadata columns exist\n",
    "        missing_cols = [col for col in metadata_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Metadata columns not found: {missing_cols}. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Get text content\n",
    "        text_content = str(row[text_column]) if pd.notna(row[text_column]) else \"\"\n",
    "        \n",
    "        if not text_content.strip():\n",
    "            logger.warning(f\"Row {idx + 1}: Empty text content in column '{text_column}', skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Build metadata\n",
    "        metadata = {\n",
    "            'source': file_path,\n",
    "            'row_number': idx + 1,  # 1-based row numbering\n",
    "            'text_column': text_column,\n",
    "        }\n",
    "        \n",
    "        # Add metadata from other columns\n",
    "        for col in metadata_columns:\n",
    "            value = row[col]\n",
    "            # Handle NaN values\n",
    "            if pd.isna(value):\n",
    "                metadata[col] = None\n",
    "            else:\n",
    "                # Convert numpy/pandas types to Python types for JSON serialization\n",
    "                if hasattr(value, 'item'):  # numpy scalar\n",
    "                    metadata[col] = value.item()\n",
    "                else:\n",
    "                    metadata[col] = value\n",
    "        \n",
    "        # Create document\n",
    "        doc = Document(\n",
    "            page_content=text_content,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(documents)} documents from spreadsheet: {file_path}\")\n",
    "    return documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
