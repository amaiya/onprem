{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.helpers\n",
    "\n",
    "> helper utilities for ingesting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import List, Union, Optional\n",
    "import warnings\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "from hashlib import md5\n",
    "from functools import partial\n",
    "import datetime\n",
    "from langchain_core.documents import Document\n",
    "from onprem.utils import contains_sentence\n",
    "\n",
    "CAPTION_DELIMITER = '||CAPTION||'\n",
    "def includes_caption(d:Document):\n",
    "    \"\"\"\n",
    "    Returns True if content of supplied Document includes a table caption\n",
    "    \"\"\"\n",
    "    table_captions = d.metadata.get('table_captions', '')\n",
    "    if not table_captions: return False\n",
    "    table_captions = table_captions.split(CAPTION_DELIMITER)\n",
    "    for c in table_captions:\n",
    "        if contains_sentence(c, d.page_content):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_tables(filepath:Optional[str]=None, docs:Optional[List[Document]]=[]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extract tables from PDF and append to end of supplied Document list.\n",
    "    Accepts either a `filepath` or a list of LangChain `Document` objects \n",
    "    all from a **single** file. If `filepath` is empty, the file path of interest \n",
    "    is extracted from `docs`. \n",
    "\n",
    "    Returns an updated list of Document objects appended with extracted tables.\n",
    "    \"\"\"\n",
    "    # Lazy import PDFTables to avoid slow module loading on Windows\n",
    "    from onprem.ingest.pdftables import PDFTables\n",
    "\n",
    "    # get filepath of document under consideration\n",
    "    docs = [] if not docs else docs\n",
    "    if not filepath and not docs:\n",
    "        raise ValueError('filepath and docs cannot both be empty.')\n",
    "    if filepath and docs:\n",
    "        raise ValueError('filepath and docs are mutually exclusive.')\n",
    "    if docs:\n",
    "        filepath = None if not docs else docs[0].metadata['source']\n",
    "    if not filepath: return docs\n",
    "        \n",
    "    if extract_extension(filepath) != \"pdf\": return docs\n",
    "    pdftab = PDFTables.from_file(filepath, verbose=False)\n",
    "    md_tables = pdftab.get_markdown_tables()\n",
    "\n",
    "    # tag document objects that contain extracted tables\n",
    "    captions = pdftab.get_captions()\n",
    "    for c in captions:\n",
    "        c = c.strip()\n",
    "        if len(c) < 8: continue\n",
    "        for d in docs:\n",
    "            if contains_sentence(c, d.page_content):\n",
    "                table_captions = d.metadata.get('table_captions', [])\n",
    "                if isinstance(table_captions, str):\n",
    "                    table_captions = table_captions.split(CAPTION_DELIMITER)\n",
    "                table_captions.append(c)\n",
    "                d.metadata['table_captions'] = CAPTION_DELIMITER.join(table_captions)\n",
    "\n",
    "    # augment docs with extracted tables\n",
    "    tabledocs = []\n",
    "    for md_table in md_tables:\n",
    "        tabledoc = Document(page_content=md_table,\n",
    "                metadata={'source':filepath, 'markdown':True, 'table':True})\n",
    "        tabledocs.append(tabledoc)\n",
    "    docs.extend(tabledocs)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def extract_files(source_dir:str, follow_links=False, extensions:Optional[Union[dict,list]]=None):\n",
    "    extensions = list(extensions.keys()) if isinstance(extensions, dict) else extensions\n",
    "    if os.listdir(source_dir) == []:\n",
    "        raise ValueError(\"%s: path is empty\" % source_dir)\n",
    "    walk = os.walk\n",
    "    for root, _, filenames in walk(source_dir, followlinks=follow_links):\n",
    "        for filename in filenames:\n",
    "            if extensions and extract_extension(filename) not in extensions:\n",
    "                continue\n",
    "            try:\n",
    "                yield os.path.join(root, filename)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "def extract_extension(filepath:str, include_dot=False):\n",
    "    \"\"\"\n",
    "    Extracts file extension (including dot) from file path\n",
    "    \"\"\"\n",
    "    dot = \".\" if include_dot else \"\"\n",
    "    return dot + filepath.rsplit(\".\", 1)[-1].lower()\n",
    "\n",
    "\n",
    "\n",
    "def extract_file_dates(filepath):\n",
    "    \"\"\"\n",
    "    Takes a file path and returns an ISO datetime string of last-modified\n",
    "    and create date of file.\n",
    "\n",
    "    Returns tuple of the form (create-date, last-modify-date)\n",
    "    \"\"\"\n",
    "    from os.path import getmtime, getctime\n",
    "    mtimestamp = getmtime(filepath)\n",
    "    ctimestamp = getctime(filepath)\n",
    "    mdate = datetime.datetime.fromtimestamp(mtimestamp).isoformat()\n",
    "    cdate = datetime.datetime.fromtimestamp(ctimestamp).isoformat()\n",
    "    return cdate, mdate\n",
    "\n",
    "def iso2date(s):\n",
    "    return datetime.datetime.fromisoformat(s)\n",
    "\n",
    "\n",
    "def date2iso(d):\n",
    "    return d.isoformat()\n",
    "\n",
    "\n",
    "def md5sum(filepath):\n",
    "    \"\"\"\n",
    "    Perform an MD5 hash of a file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"rb\") as fobj:\n",
    "        data = md5()\n",
    "        for buf in iter(partial(fobj.read, 128), b\"\"):\n",
    "            data.update(buf)\n",
    "    return data.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import mimetypes\n",
    "try:\n",
    "    import magic\n",
    "\n",
    "    get_magic = magic._get_magic_type(True)\n",
    "    MAGIC_INSTALLED = True\n",
    "except Exception as e:\n",
    "    get_magic = None\n",
    "    MAGIC_INSTALLED = False\n",
    "    #warnings.warn(\n",
    "        #\"using mimetypes, since magic could not be imported (perhaps try python-magic-bin if on Windows):  %s\"\n",
    "        #% (e)\n",
    "    #)\n",
    "\n",
    "def get_mimetype(filepath):\n",
    "    with open(filepath, \"rb\") as fobj:\n",
    "        buf = fobj.read(4096)\n",
    "    return get_magic.from_buffer(buf) if isinstance(get_magic, magic.Magic) else \"\"\n",
    "\n",
    "\n",
    "def extract_mimetype(filepath):\n",
    "    \"\"\"\n",
    "    Extract mimetype.\n",
    "    Returns a tuple with extracted mimetype, type, subtype.\n",
    "    \"\"\"\n",
    "\n",
    "    if MAGIC_INSTALLED:\n",
    "        mtype = get_mimetype(filepath)\n",
    "    else:\n",
    "        mtype = mimetypes.guess_type(filepath)[0]\n",
    "    mtype1 = None\n",
    "    mtype2 = None\n",
    "    if isinstance(mtype, str):\n",
    "        try:\n",
    "            mtype1, mtype2 = mtype.split(\"/\")\n",
    "            # Reading from buffer does not work for old office formats\n",
    "            if \"CDFV2\" in mtype2 and isinstance(get_magic, magic.Magic):\n",
    "                mtype = get_magic.from_file(filepath)\n",
    "                if isinstance(mtype, str):\n",
    "                    mtype1, mtype2 = mtype.split(\"/\")\n",
    "        except Exception:\n",
    "            mtype = None\n",
    "            mtype1 = None\n",
    "            mtype2 = None\n",
    "    return (mtype, mtype1, mtype2)\n",
    "\n",
    "\n",
    "def is_random_plaintext(extension, mimetype):\n",
    "    \"\"\"\n",
    "    Check mimetype for plain text\n",
    "    \"\"\"\n",
    "    mimetype1, mimetype2 = mimetype.split('/')\n",
    "    return mimetype1 == \"text\" or mimetype2 == \"xml\" or extension in {\"txt\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text_s_or_b):\n",
    "    \"\"\"\n",
    "    convert to string and strip.\n",
    "    \"\"\"\n",
    "    text = (\n",
    "        text_s_or_b.decode(errors=\"ignore\")\n",
    "        if isinstance(text_s_or_b, bytes)\n",
    "        else text_s_or_b\n",
    "    )\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "METADATA = {'source':None,\n",
    "            'ocr':False,\n",
    "            'table' : False,\n",
    "            'markdown' :False,\n",
    "            'page' :-1,\n",
    "            'table_captions':'',\n",
    "            'document_title' : '',\n",
    "           }\n",
    "\n",
    "def doc_from_dict(d:dict, content_field='page_content'):\n",
    "    \"\"\"\n",
    "    Create LangChain Document from dicationary\n",
    "    \"\"\"\n",
    "    metadata = d.copy()\n",
    "    page_content = metadata.pop(content_field, '')\n",
    "    return create_document(page_content, only_required_metadata=False, **metadata)\n",
    "\n",
    "\n",
    "def dict_from_doc(doc, content_field = 'page_content'):\n",
    "    \"\"\"\n",
    "    Create dictinoary from LangChain Document\n",
    "    \"\"\"\n",
    "    metadata_with_content = doc.metadata.copy()\n",
    "    metadata_with_content[content_field] = doc.page_content\n",
    "    return metadata_with_content\n",
    "\n",
    "\n",
    "def create_document(page_content:str, \n",
    "                    only_required_metadata:bool=True,\n",
    "                   **kwargs):\n",
    "    \"\"\"\n",
    "    Create document with required metadata keys from `METADATA`.\n",
    "    \"\"\"\n",
    "    metadata = {k: v for k, v in kwargs.items() if k in METADATA} if only_required_metadata else kwargs\n",
    "    if 'source' not in metadata: metadata['source'] = ''\n",
    "    return Document(page_content=page_content, \n",
    "                    metadata=metadata)\n",
    "\n",
    "def set_metadata_defaults(docs:List[Document], extra_keys:list=[]):\n",
    "    \"\"\"\n",
    "    Sets Document metadata defaults\n",
    "    \"\"\"\n",
    "    for doc in docs:\n",
    "        # Keep only keys that are in METADATA or extra_keys\n",
    "        doc.metadata = {k: v for k, v in doc.metadata.items() if k in METADATA or k in extra_keys}\n",
    "        # Add keys from METADATA if they don't exist in doc.metadata\n",
    "        doc.metadata.update({k: v for k, v in METADATA.items() if k not in doc.metadata})\n",
    "    return docs\n",
    "\n",
    "\n",
    "def _apply_file_callables(file_path:str, file_callables:dict):\n",
    "    \"\"\"\n",
    "    Invokes file_callables on file path.\n",
    "\n",
    "    Returns a dictionary with values containing results from callables for each key\n",
    "    \"\"\"\n",
    "    if not file_callables: return {}\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise ValueError('file_path does not exist: {file_path}')\n",
    "\n",
    "    results = {}\n",
    "    for k,v in file_callables.items():\n",
    "        results[k] = v(file_path)\n",
    "    return results\n",
    "\n",
    "FILE_METADATA = ['md5', 'mimetype', 'createdate', 'modifydate', 'extension']\n",
    "\n",
    "def extract_file_metadata(file_path:str,\n",
    "                          store_md5:bool=True,\n",
    "                          store_mimetype:bool=True,\n",
    "                          store_file_dates:bool=True,\n",
    "                          file_callables:dict={}):\n",
    "    \"\"\"\n",
    "    Extract file metadata\n",
    "    \"\"\"\n",
    "    # extract metadata\n",
    "    file_metadata = {}\n",
    "    if store_md5:\n",
    "        file_metadata['md5'] = md5sum(file_path)\n",
    "    if store_mimetype:\n",
    "        file_metadata['mimetype'], _, _ = extract_mimetype(file_path)\n",
    "    if store_file_dates:\n",
    "        file_metadata['createdate'], file_metadata['modifydate'] = extract_file_dates(file_path)\n",
    "    ext = extract_extension(file_path)\n",
    "    file_metadata['extension'] = ext\n",
    "    file_metadata.update(_apply_file_callables(file_path, file_callables))\n",
    "    return file_metadata\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from langchain.text_splitter import TextSplitter\n",
    "import re\n",
    "class ParagraphTextSplitter(TextSplitter):\n",
    "    def __init__(self, chunk_size: int = 5000, chunk_overlap: int = 0):\n",
    "        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text) # Paragraphs assumed to be separated by double newlines\n",
    "\n",
    "        chunks = []\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            if len(para) <= self._chunk_size:   # <- notice: _chunk_size (with underscore)\n",
    "                chunks.append(para)\n",
    "            else:\n",
    "                chunks.extend(self._split_large_paragraph(para))\n",
    "        return chunks\n",
    "\n",
    "    def _split_large_paragraph(self, paragraph: str) -> List[str]:\n",
    "        # Split a long paragraph into smaller chunks\n",
    "        splits = []\n",
    "        start = 0\n",
    "        while start < len(paragraph):\n",
    "            end = start + self._chunk_size\n",
    "            splits.append(paragraph[start:end])\n",
    "            start = end\n",
    "        return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
