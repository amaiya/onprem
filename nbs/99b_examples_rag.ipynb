{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example of [OnPrem.LLM](https://github.com/amaiya/onprem) demonstrates retrieval augmented generation or RAG.\n",
    "\n",
    "## Basic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Setup the `LLM` instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first example, we will use a model called **[Zephyr-7B-beta](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF)**.  When selecting a model, it is important to inspect the model's home page and identify the correct prompt format.  The prompt format for this model is [located here](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF#prompt-template-zephyr), and we will supply it directly to the `LLM` constructor along with the URL to the specific model file we want (i.e., *zephyr-7b-beta.Q4_K_M.gguf*).  We will offload layers to our GPU(s) to speed up inference using the `n_gpu_layers` parameter. (For more information on GPU acceleration, see [here](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu).) For the purposes of this notebook, we also supply `temperature=0` so that there is no variability in outputs.  You can increase this value for more creativity in the outputs. Finally, we will choose a non-default location for our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM, utils as U\n",
    "import tempfile\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (3904) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "vectordb_path = tempfile.mkdtemp()\n",
    "\n",
    "llm = LLM(model_url='https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf', \n",
    "          prompt_template= \"<|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\",\n",
    "          n_gpu_layers=-1,\n",
    "          temperature=0,\n",
    "          store_type='dense',\n",
    "          vectordb_path=vectordb_path,\n",
    "         verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since OnPrem.LLM includes built-in support for Zephyr, an easier way to instantiate the LLM with Zephyr is as follows:\n",
    "\n",
    "```python\n",
    "llm = LLM(default_model='zephyr', \n",
    "          n_gpu_layers=-1,\n",
    "          temperature=0,\n",
    "          store_type='dense',\n",
    "          vectordb_path=vectordb_path)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Ingest Documents\n",
    "\n",
    "When ingesting documents, they can be stored in one of two ways:\n",
    "1. a **dense** vector store:  a conventional vector database like Chroma\n",
    "2. a **sparse** vector store: a keyword-search engine\n",
    "\n",
    "Sparse vector stores compute embeddings on-the-fly at inference time. As a result, sparse vector stores sacrifice a small amount of inference speed for significant speed ups in ingestion speed.  This makes it better suited for larger document sets.  Note that sparse vector stores include the contraint that any passages considered as sources for answers should have at least one word in common with the question being asked. You can specify the kind of vector store by supplying either `store_type=\"dense\"` or `store_type=\"sparse\"` when creating the `LLM` above.  We use a dense vector store in this example, as shown above.\n",
    "\n",
    "For this example, we will download the 2024 National Defense Autorization Act (NDAA) report and ingest it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "U.download('https://www.congress.gov/118/crpt/hrpt125/CRPT-118hrpt125.pdf', '/tmp/ndaa/ndaa.pdf', verify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /tmp/tmpmnt6g6l8/dense\n",
      "Loading documents from /tmp/ndaa/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Processing and chunking 672 new documents: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 5202 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:17<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "llm.ingest(\"/tmp/ndaa/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Asking Questions to Your Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The context provided discusses the implementation of an AI education strategy required by Section 256 of the National Defense Authorization Act for Fiscal Year 2020. The strategy aims to educate servicemembers in relevant occupational fields, with a focus on data literacy across a broader population within the Department of Defense. The committee encourages the Air Force and Space Force to leverage government-owned training platforms informed by private sector expertise to accelerate learning and career path development. Additionally, the committee suggests expanding existing mobile enabled platforms to train and develop the cyber workforce of the Air Force and Space Force. Overall, there is a recognition that AI continues to be central to warfighting and that proper implementation of these new technologies requires a focus on education and training."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "result = llm.ask(\"What is said about artificial intelligence training and education?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is stored in `results['answer']`. The documents retrieved from the vector store used to generate the answer are stored in `results['source_documents']` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " The context provided discusses the implementation of an AI education\n",
      "strategy required by Section 256 of the National Defense Authorization\n",
      "Act for Fiscal Year 2020. The strategy aims to educate servicemembers\n",
      "in relevant occupational fields, with a focus on data literacy across\n",
      "a broader population within the Department of Defense. The committee\n",
      "encourages the Air Force and Space Force to leverage government-owned\n",
      "training platforms informed by private sector expertise to accelerate\n",
      "learning and career path development. Additionally, the committee\n",
      "suggests expanding existing mobile enabled platforms to train and\n",
      "develop the cyber workforce of the Air Force and Space Force. Overall,\n",
      "there is a recognition that AI continues to be central to warfighting\n",
      "and that proper implementation of these new technologies requires a\n",
      "focus on education and training.\n",
      "\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "On Page 359 in /tmp/ndaa/ndaa.pdf:\n",
      "‘‘servicemembers in relevant occupational fields on matters relating \n",
      "to artificial intelligence.’’ \n",
      "Given the continued centrality of AI to warfighting, the com-\n",
      "mittee directs the Chief Digital and Artificial Intelligence Officer of \n",
      "the Department of Defense to provide a briefing to the House Com-\n",
      "mittee on Armed Services not later than March 31, 2024, on the \n",
      "implementation status of the AI education strategy, with emphasis \n",
      "on current efforts underway, such as the AI Primer course within\n",
      "----------------------------------------\n",
      "\n",
      "On Page 359 in /tmp/ndaa/ndaa.pdf:\n",
      "intelligence (AI) and machine learning capabilities available within \n",
      "the Department of Defense. To ensure the proper implementation \n",
      "of these new technologies, there must be a focus on data literacy \n",
      "across a broader population within the Department. Section 256 of \n",
      "the National Defense Authorization Act for Fiscal Year 2020 (Pub-\n",
      "lic Law 116–92) required the Department of Defense to develop an \n",
      "AI education strategy, with the stated objective to educate\n",
      "----------------------------------------\n",
      "\n",
      "On Page 102 in /tmp/ndaa/ndaa.pdf:\n",
      "tificial intelligence and machine learning (AI/ML), and cloud com-\n",
      "puting. The committee encourages the Air Force and Space Force \n",
      "to leverage government owned training platforms with curricula in-\n",
      "formed by private sector expertise to accelerate learning and career \n",
      "path development. \n",
      "To that end, the committee encourages the Secretary of the Air \n",
      "Force to expand existing mobile enabled platforms to train and de-\n",
      "velop the cyber workforce of Air Force and Space Force. To better\n",
      "----------------------------------------\n",
      "\n",
      "On Page 109 in /tmp/ndaa/ndaa.pdf:\n",
      "70 \n",
      "role of senior official with principal responsibility for artificial intel-\n",
      "ligence and machine learning. In February 2022, the Department \n",
      "stood up the Chief Digital and Artificial Intelligence Office to accel-\n",
      "erate the Department’s adoption of AI. The committee encourages \n",
      "the Department to build upon this progress and sustain efforts to \n",
      "research, develop, test, and where appropriate, operationalize AI \n",
      "capabilities. \n",
      "Artificial intelligence capabilities of foreign adversaries\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "print('ANSWER:')\n",
    "print(\"\\n\".join(wrap(result['answer'])))\n",
    "print()\n",
    "print()\n",
    "print('REFERENCES')\n",
    "print()\n",
    "for d in result['source_documents']:\n",
    "    print(f\"On Page {d.metadata['page']} in {d.metadata['source']}:\")\n",
    "    print(d.page_content)\n",
    "    print('----------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: NSF Awards\n",
    "\n",
    "The example above employed the use of the default dense vector store, Chroma.  By supplying `store_type=\"sparse\"` to `LLM`, a sparse vector store (i.e., keyword search engine) is used instead.  Sparse vector stores index documents faster but requires keyword matches between sources containing answers and the question or query.  Semantic search is still supported through on-demand dense vectorization in OnPrem.LLM.\n",
    "\n",
    "In this example, we will instantiate a spare store directly and customize the ingestion process to include custom fields using a dataset of 2024 NSF Awards.\n",
    "\n",
    "### STEP 1: Download the Pre-Process the NSF Data\n",
    "\n",
    "NSF awards data stores as thousands of JSON files. The code below downloads and parses each JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file already exists.\n",
      "Already extracted.\n",
      "Processing 11687 JSON files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab62febcd8304ab7b192f5df7f41cb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All JSON files processed and saved to list of dictionaries.\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Step 1: Download the ZIP file\n",
    "url = \"https://www.nsf.gov/awardsearch/download?DownloadFileName=2024&All=true&isJson=true\"\n",
    "zip_path = \"/tmp/nsf_awards_2024.zip\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading NSF data...\")\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"ZIP file already exists.\")\n",
    "\n",
    "# Step 2: Unzip the file\n",
    "extract_dir = \"nsf_awards_2024\"\n",
    "\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(\"Extracting ZIP file...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Already extracted.\")\n",
    "\n",
    "# Step 3: Function to extract fields from JSON\n",
    "def extract_fields(data):\n",
    "    title = data.get(\"awd_titl_txt\", \"N/A\")\n",
    "    abstract = data.get(\"awd_abstract_narration\", \"N/A\")\n",
    "    \n",
    "    pgm_ele = data.get(\"pgm_ele\")\n",
    "    if isinstance(pgm_ele, list) and pgm_ele:\n",
    "        category = pgm_ele[0].get(\"pgm_ele_name\", \"N/A\")\n",
    "    else:\n",
    "        category = \"N/A\"\n",
    "\n",
    "    # Authors\n",
    "    authors = []\n",
    "    for pi in data.get(\"pi\", []):\n",
    "        full_name = pi.get(\"pi_full_name\", \"\")\n",
    "        if full_name:\n",
    "            authors.append(full_name)\n",
    "    authors_str = \", \".join(authors) if authors else \"N/A\"\n",
    "\n",
    "    # Affiliation\n",
    "    affiliation = data.get(\"inst\", {}).get(\"inst_name\", \"N/A\")\n",
    "\n",
    "    # Amount\n",
    "    raw_amount = data.get(\"awd_amount\", data.get(\"tot_intn_awd_amt\", None))\n",
    "    try:\n",
    "        amount = float(raw_amount)\n",
    "    except (TypeError, ValueError):\n",
    "        amount = None\n",
    "\n",
    "    return {\n",
    "        \"title\": title or '',\n",
    "        \"abstract\": f'{title or \"\"}' + '\\n\\n' + f'{abstract or \"\"}',\n",
    "        \"category\": category,\n",
    "        \"authors\": authors_str,\n",
    "        \"affiliation\": affiliation,\n",
    "        \"amount\": amount\n",
    "    }\n",
    "\n",
    "# Step 4: Process all JSON files and write results to .txt\n",
    "output_dir = \"/tmp/nsf_text_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "json_files = list(Path(extract_dir).glob(\"*.json\"))\n",
    "\n",
    "print(f\"Processing {len(json_files)} JSON files...\")\n",
    "\n",
    "nsf_data = []\n",
    "for json_file in tqdm(json_files):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # skip bad files\n",
    "\n",
    "    fields = extract_fields(data)\n",
    "    fields['source'] = str(json_file)\n",
    "    nsf_data.append(fields)\n",
    "\n",
    "print(\"All JSON files processed and saved to list of dictionaries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Ingest Documents\n",
    "\n",
    "Let's now store these NSF awards data in a Whoosh-backed sparse vector store.  This is equivalent to supplying `store_type=\"sparse\"` to `LLM`. However, we will explicitly create the SparseStore instance to customize the ingestion process for NSF data. See the [vectorstore factory example](https://amaiya.github.io/onprem/examples_vectorstore_factory.html) for more information.\n",
    "\n",
    "Since award abstracts are relatively short, we will skip document chunking (whether using `onprem.ingest.chunk_documents` or an external tool like [chonkie](https://github.com/chonkie-inc/chonkie)) and instead store each award as a single record in the index. By accessing the vector store directly, any custom chunking strategy can be applied at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem.ingest import VectorStoreFactory, helpers, chunk_documents\n",
    "store = VectorStoreFactory.create(\n",
    "    kind='whoosh', # you can change to kind=\"chroma\" to use ChromaDB.\n",
    "    persist_location='/tmp/nsf_store'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the default embedding model is: `sentence-transformers/all-MiniLM-L6-v2`.  You can change it by supplying `embedding_model_name` to `VectorStoreFactory.create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 11687/11687 [00:10<00:00, 1119.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "docs = []\n",
    "for d in nsf_data:\n",
    "    doc = helpers.doc_from_dict(d, content_field='abstract')\n",
    "    docs.append(doc)\n",
    "store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the total number of awards stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11687"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "store.get_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Explore NSF Awards\n",
    "\n",
    "We can explore NSF awards by either using an LLM or querying the vector store directly.\n",
    "\n",
    "The NSF buckets awards into different catgories.  Let's examine all the material-related categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIOMATERIALS PROGRAM',\n",
       " 'ELECTRONIC/PHOTONIC MATERIALS',\n",
       " 'Mechanics of Materials and Str',\n",
       " 'SOLID STATE & MATERIALS CHEMIS'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "set([d['category'] for d in store.search('category:*material*', limit=100)['hits']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see  how many of the material-related awards mention AI. \n",
    "\n",
    "One of the advantages of sparse vector stores is the ability to easily use complex boolean queries to target specific documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "store.search('(\"machine learning\" OR \"artificial intelligence\") AND category:*material*', limit=100)['total_hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil now use an LLM to summarize how AI is utilized in this research.\n",
    "\n",
    "Since NSF awards data are publicly-available, we will use OpenAI's GPT-4o-mini, a cloud LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) and machine learning (ML) are utilized in various research projects described in the provided context in several ways:\n",
      "\n",
      "1. **Data-Driven Approaches**: Many projects leverage AI techniques to analyze complex datasets and identify patterns that are not easily discernible through traditional methods. For example, in the project on engineered photonic materials, AI is used to develop new materials with tailored properties by consolidating information on material compositions and geometries.\n",
      "\n",
      "2. **Model Development and Prediction**: AI and ML are employed to create predictive models that can simulate the behavior of materials under different conditions. The project on recycled polymers utilizes AI to predict deformation and failure mechanisms in recyclates, enhancing their mechanical performance.\n",
      "\n",
      "3. **Optimization**: Machine learning algorithms are used for optimizing the design and synthesis processes of materials. In the project focused on luminescent biomaterials, iterative Bayesian optimization is applied to screen proteins for desired luminescent properties, facilitating the discovery of new materials.\n",
      "\n",
      "4. **Enhanced Characterization**: In the study of nanofibers, machine learning aids in understanding and mitigating defects that compromise strength by predicting stress fields around nanovoids and assessing the impact of atomic crosslinks on material properties.\n",
      "\n",
      "5. **Multiscale Simulations**: AI enhances multiscale modeling approaches, where simulations at different scales (from atomic to macro) are integrated to provide insights into material performance. For instance, the project on 3D printed polymer composites combines computational modeling with experimental data to understand fracture mechanisms.\n",
      "\n",
      "6. **Education and Workforce Development**: Several projects include educational components that involve teaching AI and ML techniques to students from diverse backgrounds, thereby preparing the next generation of engineers and scientists with skills relevant to emerging technologies.\n",
      "\n",
      "Overall, AI and ML are essential tools in these projects, facilitating advancements in material science, improving predictive capabilities, and enhancing the understanding of complex physical phenomena."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "llm = LLM('openai/gpt-4o-mini')\n",
    "llm.load_vectorstore(custom_vectorstore=store)\n",
    "result = llm.ask('How is articial intelligene and machine learning used in these research projects?', \n",
    "                 limit=16,\n",
    "                  where_document='(\"machine learning\" OR \"artificial intelligence\") AND category:*material*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awards used to answer the question are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference: Uncertainty Quantification for Machine Learning Integrated Physics Modeling (UQ-MLIP 2024); Arlington, Virginia; 12-14 August 2024\n",
      "Collaborative Research: DMREF: Accelerating the Design and Development of Engineered Photonic Materials based on a Data-Driven Deep Learning Approach\n",
      "Collaborative Research: DMREF: Accelerating the Design and Development of Engineered Photonic Materials based on a Data-Driven Deep Learning Approach\n",
      "EAGER: Generative AI for Learning Emergent Complexity in  Mechanics-driven Coupled Physics Problems\n",
      "CAREER: Investigating the Role of Microstructure in the High Strain Rate Behavior of Stable Nanocrystalline Alloys\n",
      "Conference: 10th International Conference on Spectroscopic Ellipsometry\n",
      "CAREER: Informed Testing — From Full-Field Characterization of Mechanically Graded Soft Materials to Student Equity in the Classroom\n",
      "2024 Solid State Chemistry Gordon Research Conference and Gordon Research Seminar\n",
      "Designing Luminescent Biomaterials from First Principles\n",
      "CAREER: Recycled Polymers of Enhanced Strength and Toughness: Predicting Failure and Unraveling Deformation to Enable Circular Transitions\n",
      "Collaborative Research: DMREF: Organic Materials Architectured for Researching Vibronic Excitations with Light in the Infrared (MARVEL-IR)\n",
      "Designing Pyrolyzed Nanofibers at the Atomic Level: Toward Synthesis of Ultra-high-strength Nano-carbon\n",
      "CAREER: Design and synthesis of functional van der Waals magnets\n",
      "Integrated Multiscale Computational and Experimental Investigations on Fracture of Additively Manufactured Polymer Composites\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "for d in result['source_documents']:\n",
    "    print(d.metadata['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#|hide\n",
    "\n",
    "## Additional Tips\n",
    "\n",
    "The `LLM.ask`and `LLM.ingest` methods include many options for more complex scenarios.  \n",
    "\n",
    "#### LLM.ingest options\n",
    "\n",
    "- If supplying `infer_table_structure=True` to `LLM.ingest`, the `LLM.ask` method will automatically consider tables within PDFs when answering questions. This behavior can be controlled with the `table_k` and `table_score_threshold` parameters in `LLM.ask`.\n",
    "- If suppyling `extract_document_titles=True` to `LLM.ingest`, the title of each document will be inferred and added to each document chunk for potentially better retrieval.\n",
    "- If supplying `caption_tables=True`, an LLM-generated caption will be added to every extracted table for potentially better table retrieval.\n",
    "- Increasing chunk size of sources for more answer context\n",
    "#### LLM.ask options\n",
    "- If supplying `selfask=True` as an argument, a [Self-Ask prompting strategy](https://learnprompting.org/docs/advanced/few_shot/self_ask) will be used to decompose the question into subquestions.\n",
    "- Adjusting prompts for QA with `prompt_template` argument to `LLM.ask`\n",
    "- Increasing number of sources to consider (`k` parameter to `LLM.ask`)\n",
    "- Filtering sources with `filters` and `where_document`\n",
    "- Adding a score threshold for sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
