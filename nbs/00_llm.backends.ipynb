{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm backends\n",
    "\n",
    "> Support classes for different LLM backends (e.g., AWS GovCloud LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp llm.backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Custom LangChain Chat classes for various frameworks and cloud providers.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "\n",
    "from pydantic import Field, BaseModel\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "\n",
    "\n",
    "def _pydantic_to_bedrock_tool_schema(pydantic_model: BaseModel, tool_name: str = \"structured_output\") -> Dict:\n",
    "    \"\"\"\n",
    "    Convert a Pydantic model to Bedrock tool schema format.\n",
    "    \n",
    "    Args:\n",
    "        pydantic_model: Pydantic model class\n",
    "        tool_name: Name for the tool\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary in Bedrock tool format\n",
    "    \"\"\"\n",
    "    schema = pydantic_model.model_json_schema()\n",
    "    \n",
    "    # Extract properties and required fields from the Pydantic schema\n",
    "    properties = schema.get(\"properties\", {})\n",
    "    required = schema.get(\"required\", [])\n",
    "    description = schema.get(\"description\", f\"Structured output for {pydantic_model.__name__}\")\n",
    "    \n",
    "    return {\n",
    "        \"name\": tool_name,\n",
    "        \"description\": description,\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": properties,\n",
    "            \"required\": required\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "class ChatGovCloudBedrock(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Custom LangChain Chat model for AWS GovCloud Bedrock.\n",
    "    \n",
    "    This class provides integration with Amazon Bedrock running in AWS GovCloud regions,\n",
    "    supporting custom VPC endpoints and GovCloud-specific configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_id: str = Field(description=\"The Bedrock model ID or inference profile ARN\")\n",
    "    region_name: str = Field(default=\"us-gov-east-1\", description=\"AWS GovCloud region\")\n",
    "    endpoint_url: Optional[str] = Field(default=None, description=\"Custom endpoint URL for VPC endpoints\")\n",
    "    aws_access_key_id: Optional[str] = Field(default=None, description=\"AWS access key ID\")\n",
    "    aws_secret_access_key: Optional[str] = Field(default=None, description=\"AWS secret access key\")\n",
    "    max_tokens: int = Field(default=512, description=\"Maximum tokens to generate\")\n",
    "    temperature: float = Field(default=0.7, description=\"Sampling temperature\")\n",
    "    streaming: bool = Field(default=False, description=\"Enable streaming responses\")\n",
    "    client: Any = Field(default=None, exclude=True, description=\"Boto3 client instance\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        region_name: str = \"us-gov-east-1\",\n",
    "        endpoint_url: Optional[str] = None,\n",
    "        aws_access_key_id: Optional[str] = None,\n",
    "        aws_secret_access_key: Optional[str] = None,\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        streaming: bool = False,\n",
    "        callbacks: Optional[List] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ChatGovCloudBedrock.\n",
    "        \n",
    "        Args:\n",
    "            model_id: The Bedrock model ID or inference profile ARN\n",
    "            region_name: AWS GovCloud region (us-gov-east-1 or us-gov-west-1)\n",
    "            endpoint_url: Custom endpoint URL for VPC endpoints\n",
    "            aws_access_key_id: AWS access key ID (or use environment variables)\n",
    "            aws_secret_access_key: AWS secret access key (or use environment variables)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            streaming: Enable streaming responses\n",
    "            callbacks: LangChain callbacks\n",
    "            **kwargs: Additional parameters\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model_id=model_id,\n",
    "            region_name=region_name,\n",
    "            endpoint_url=endpoint_url,\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            streaming=streaming,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def model_post_init(self, __context: Any) -> None:\n",
    "        \"\"\"Initialize boto3 client after model validation.\"\"\"\n",
    "        # Initialize boto3 client\n",
    "        client_kwargs = {\n",
    "            \"service_name\": \"bedrock-runtime\",\n",
    "            \"region_name\": self.region_name,\n",
    "        }\n",
    "        \n",
    "        if self.endpoint_url:\n",
    "            client_kwargs[\"endpoint_url\"] = self.endpoint_url\n",
    "            \n",
    "        if self.aws_access_key_id:\n",
    "            client_kwargs[\"aws_access_key_id\"] = self.aws_access_key_id\n",
    "        else:\n",
    "            client_kwargs[\"aws_access_key_id\"] = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "            \n",
    "        if self.aws_secret_access_key:\n",
    "            client_kwargs[\"aws_secret_access_key\"] = self.aws_secret_access_key\n",
    "        else:\n",
    "            client_kwargs[\"aws_secret_access_key\"] = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "        \n",
    "        try:\n",
    "            import boto3\n",
    "        except ImportError:\n",
    "            raise ImportError('Please install boto3: pip install boto3')\n",
    "\n",
    "        self.client = boto3.client(**client_kwargs)\n",
    "\n",
    "    def _convert_messages_to_bedrock_format(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Convert LangChain messages to Bedrock API format.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of LangChain BaseMessage objects\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries in Bedrock API format\n",
    "        \"\"\"\n",
    "        bedrock_messages = []\n",
    "        \n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": message.content\n",
    "                })\n",
    "            elif isinstance(message, AIMessage):\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": message.content\n",
    "                })\n",
    "            elif isinstance(message, SystemMessage):\n",
    "                # For Claude models, system messages can be handled as user messages\n",
    "                # or through the system parameter in the request body\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"System: {message.content}\"\n",
    "                })\n",
    "            else:\n",
    "                # Default to user role for unknown message types\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": str(message.content)\n",
    "                })\n",
    "        \n",
    "        return bedrock_messages\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"\n",
    "        Generate chat response using AWS Bedrock.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of chat messages\n",
    "            stop: List of stop sequences\n",
    "            run_manager: Callback manager\n",
    "            **kwargs: Additional parameters\n",
    "            \n",
    "        Returns:\n",
    "            ChatResult containing the response\n",
    "        \"\"\"\n",
    "        # If streaming is enabled, use streaming and collect all chunks\n",
    "        if self.streaming:\n",
    "            chunks = []\n",
    "            full_content = \"\"\n",
    "            for chunk in self._stream(messages, stop=stop, run_manager=run_manager, **kwargs):\n",
    "                chunks.append(chunk)\n",
    "                if chunk.message.content:\n",
    "                    full_content += chunk.message.content\n",
    "            \n",
    "            # Return final result with complete content\n",
    "            final_message = AIMessage(content=full_content)\n",
    "            return ChatResult(generations=[ChatGeneration(message=final_message)])\n",
    "        \n",
    "        # Non-streaming path\n",
    "        # Convert messages to Bedrock format\n",
    "        bedrock_messages = self._convert_messages_to_bedrock_format(messages)\n",
    "        \n",
    "        # Prepare request body\n",
    "        body = {\n",
    "            \"messages\": bedrock_messages,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }\n",
    "        \n",
    "        # Add stop sequences if provided\n",
    "        if stop:\n",
    "            body[\"stop_sequences\"] = stop\n",
    "            \n",
    "        try:\n",
    "            # Make the API call\n",
    "            response = self.client.invoke_model(\n",
    "                modelId=self.model_id,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "                body=json.dumps(body).encode(\"utf-8\")\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            response_body = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
    "            \n",
    "            # Extract the generated text\n",
    "            if \"content\" in response_body and len(response_body[\"content\"]) > 0:\n",
    "                generated_text = response_body[\"content\"][0][\"text\"]\n",
    "            else:\n",
    "                generated_text = \"\"\n",
    "            \n",
    "            # Create AIMessage\n",
    "            message = AIMessage(content=generated_text)\n",
    "            \n",
    "            # Create ChatGeneration\n",
    "            generation = ChatGeneration(message=message)\n",
    "            \n",
    "            return ChatResult(generations=[generation])\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error calling AWS Bedrock: {str(e)}\")\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGeneration]:\n",
    "        \"\"\"\n",
    "        Stream chat response using AWS Bedrock.\n",
    "        \"\"\"\n",
    "        # Convert messages to Bedrock format\n",
    "        bedrock_messages = self._convert_messages_to_bedrock_format(messages)\n",
    "        \n",
    "        # Prepare request body\n",
    "        body = {\n",
    "            \"messages\": bedrock_messages,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }\n",
    "        \n",
    "        # Add stop sequences if provided\n",
    "        if stop:\n",
    "            body[\"stop_sequences\"] = stop\n",
    "            \n",
    "        try:\n",
    "            # Use streaming API\n",
    "            response = self.client.invoke_model_with_response_stream(\n",
    "                modelId=self.model_id,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "                body=json.dumps(body).encode(\"utf-8\")\n",
    "            )\n",
    "            \n",
    "            # Process streaming response\n",
    "            for event in response[\"body\"]:\n",
    "                chunk = json.loads(event[\"chunk\"][\"bytes\"].decode(\"utf-8\"))\n",
    "                \n",
    "                if chunk.get(\"type\") == \"content_block_delta\":\n",
    "                    if \"delta\" in chunk and \"text\" in chunk[\"delta\"]:\n",
    "                        text = chunk[\"delta\"][\"text\"]\n",
    "                        if run_manager:\n",
    "                            # Check if run_manager is async or sync\n",
    "                            if hasattr(run_manager.on_llm_new_token, '__call__'):\n",
    "                                try:\n",
    "                                    import asyncio\n",
    "                                    if asyncio.iscoroutinefunction(run_manager.on_llm_new_token):\n",
    "                                        # Skip async callback in sync context for now\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        run_manager.on_llm_new_token(text)\n",
    "                                except:\n",
    "                                    run_manager.on_llm_new_token(text)\n",
    "                            else:\n",
    "                                run_manager.on_llm_new_token(text)\n",
    "                        yield ChatGeneration(message=AIMessage(content=text))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            # Fallback to non-streaming if streaming fails\n",
    "            result = self._generate(messages, stop=stop, run_manager=run_manager, **kwargs)\n",
    "            yield result.generations[0]\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"\n",
    "        Async generate chat response using AWS Bedrock.\n",
    "        \"\"\"\n",
    "        # For now, run the sync version in a thread pool\n",
    "        import asyncio\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(\n",
    "            None, \n",
    "            lambda: self._generate(messages, stop, run_manager, **kwargs)\n",
    "        )\n",
    "\n",
    "    async def _astream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGeneration]:\n",
    "        \"\"\"\n",
    "        Async stream chat response using AWS Bedrock.\n",
    "        \"\"\"\n",
    "        # For now, run the sync version in a thread pool\n",
    "        import asyncio\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def sync_stream():\n",
    "            return list(self._stream(messages, stop, run_manager, **kwargs))\n",
    "        \n",
    "        chunks = await loop.run_in_executor(None, sync_stream)\n",
    "        for chunk in chunks:\n",
    "            yield chunk\n",
    "\n",
    "    def structured_generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage], \n",
    "        pydantic_model: BaseModel,\n",
    "        tool_name: str = \"structured_output\",\n",
    "        **kwargs: Any\n",
    "    ) -> BaseModel:\n",
    "        \"\"\"\n",
    "        Generate structured output using Bedrock tool calling.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of chat messages\n",
    "            pydantic_model: Pydantic model class to structure the output\n",
    "            tool_name: Name for the tool\n",
    "            **kwargs: Additional parameters\n",
    "            \n",
    "        Returns:\n",
    "            Instance of the pydantic_model with parsed data\n",
    "        \"\"\"\n",
    "        # Convert Pydantic model to Bedrock tool schema\n",
    "        tool_schema = _pydantic_to_bedrock_tool_schema(pydantic_model, tool_name)\n",
    "        \n",
    "        # Convert messages to Bedrock format\n",
    "        bedrock_messages = self._convert_messages_to_bedrock_format(messages)\n",
    "        \n",
    "        # Build request body\n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"messages\": bedrock_messages,\n",
    "            \"tools\": [tool_schema],\n",
    "            \"tool_choice\": {\"type\": \"tool\", \"name\": tool_name}\n",
    "        }\n",
    "        \n",
    "        # Filter out parameters that aren't supported by Bedrock tool calling\n",
    "        bedrock_kwargs = {}\n",
    "        supported_params = {'system', 'anthropic_version'}\n",
    "        for key, value in kwargs.items():\n",
    "            if key in supported_params:\n",
    "                bedrock_kwargs[key] = value\n",
    "        \n",
    "        # Add supported kwargs to request body  \n",
    "        request_body.update(bedrock_kwargs)\n",
    "        \n",
    "        try:\n",
    "            # Call Bedrock\n",
    "            response = self.client.invoke_model(\n",
    "                modelId=self.model_id,\n",
    "                body=json.dumps(request_body)\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            \n",
    "            # Extract tool use from response\n",
    "            content = response_body.get('content', [])\n",
    "            for item in content:\n",
    "                if item.get('type') == 'tool_use' and item.get('name') == tool_name:\n",
    "                    tool_input = item.get('input', {})\n",
    "                    # Parse the tool input into the Pydantic model\n",
    "                    return pydantic_model(**tool_input)\n",
    "            \n",
    "            # If no tool use found, try to parse the text content as JSON\n",
    "            for item in content:\n",
    "                if item.get('type') == 'text':\n",
    "                    try:\n",
    "                        data = json.loads(item.get('text', '{}'))\n",
    "                        return pydantic_model(**data)\n",
    "                    except (json.JSONDecodeError, ValueError):\n",
    "                        continue\n",
    "            \n",
    "            raise ValueError(\"No valid structured output found in response\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error in structured generation: {str(e)}\")\n",
    "\n",
    "    def with_structured_output(self, pydantic_model: BaseModel, **kwargs):\n",
    "        \"\"\"\n",
    "        Return a wrapper that uses native structured output.\n",
    "        This method makes ChatGovCloudBedrock compatible with LangChain's structured output interface.\n",
    "        \"\"\"\n",
    "        class StructuredOutputWrapper:\n",
    "            def __init__(self, chat_model, pydantic_model):\n",
    "                self.chat_model = chat_model\n",
    "                self.pydantic_model = pydantic_model\n",
    "                \n",
    "            def invoke(self, input_data, **invoke_kwargs):\n",
    "                # Handle different input formats\n",
    "                if hasattr(input_data, 'messages'):\n",
    "                    messages = input_data.messages\n",
    "                elif isinstance(input_data, list):\n",
    "                    messages = input_data\n",
    "                elif isinstance(input_data, str):\n",
    "                    from langchain_core.messages import HumanMessage\n",
    "                    messages = [HumanMessage(content=input_data)]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported input type: {type(input_data)}\")\n",
    "                \n",
    "                return self.chat_model.structured_generate(\n",
    "                    messages=messages,\n",
    "                    pydantic_model=self.pydantic_model,\n",
    "                    **invoke_kwargs\n",
    "                )\n",
    "        \n",
    "        return StructuredOutputWrapper(self, pydantic_model)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of chat model.\"\"\"\n",
    "        return \"govcloud-bedrock\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"region_name\": self.region_name,\n",
    "            \"endpoint_url\": self.endpoint_url,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "This example shows how to use **OnPrem.LLM** with cloud LLMs served from **AWS GovCloud**.\n",
    "\n",
    "The example below assumes you have set both `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` as environment variables.\n",
    "You can adjust the `inference_arn`, `endpoint_url`, and `region_name` based on your application scenario.\n",
    "\n",
    "```python\n",
    "from onprem import LLM\n",
    "\n",
    "inference_arn = \"YOUR INFERENCE ARN\"\n",
    "endpoint_url = \"YOUR ENDPOINT URL\"\n",
    "region_name = \"us-gov-east-1\" # replace as necessary\n",
    "\n",
    "# set up LLM connection to Bedrock on AWS GovCloud\n",
    "llm = LLM(\n",
    "  f\"govcloud-bedrock://{inference_arn}\",\n",
    "  region_name=region_name,\n",
    "  endpoint_url=endpoint_url,\n",
    ")\n",
    "\n",
    "# send prompt to LLM\n",
    "response = llm.prompt(\"Write a haiku about the moon.\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs with AWS GovCloud Bedrock\n",
    "\n",
    "AWS GovCloud Bedrock natively supports structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|notest\n",
    "\n",
    "from onprem import LLM\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Sarah Johnson\n",
      "Age: 28\n",
      "City: Seattle\n",
      "Occupation: software engineer\n",
      "Type: <class '__main__.PersonInfo'>\n"
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "\n",
    "inference_arn = \"YOUR INFERENCE ARN\"\n",
    "endpoint = \"YOUR ENDPOINT URL\"\n",
    "region = \"us-gov-east-1\" # replace as necessary\n",
    "\n",
    "llm = LLM(\n",
    "  f\"govcloud-bedrock://{inference_arn}\",\n",
    "  region_name=region,\n",
    "  endpoint_url=endpoint,\n",
    ")\n",
    "\n",
    "# send prompt to LLM\n",
    "\n",
    "\n",
    "# Define a Pydantic model for structured output\n",
    "class PersonInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    city: str\n",
    "    occupation: str\n",
    "prompt = \"\"\"\n",
    "  Extract the following information from this text:\n",
    "  \"Hi, I'm Sarah Johnson, I'm 28 years old, live in Seattle, and work as a software engineer.\"\n",
    "\"\"\"\n",
    "result = llm.prompt(prompt, response_format=PersonInfo)\n",
    "\n",
    "# Print the structured result\n",
    "print(f\"Name: {result.name}\")\n",
    "print(f\"Age: {result.age}\")\n",
    "print(f\"City: {result.city}\")\n",
    "print(f\"Occupation: {result.occupation}\")\n",
    "print(f\"Type: {type(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
