{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm backends\n",
    "\n",
    "> Support classes for different LLM backends (e.g., AWS GovCloud LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp llm.backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Custom LangChain Chat classes for various frameworks and cloud providers.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "\n",
    "from pydantic import Field\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "\n",
    "\n",
    "class ChatGovCloudBedrock(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Custom LangChain Chat model for AWS GovCloud Bedrock.\n",
    "    \n",
    "    This class provides integration with Amazon Bedrock running in AWS GovCloud regions,\n",
    "    supporting custom VPC endpoints and GovCloud-specific configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_id: str = Field(description=\"The Bedrock model ID or inference profile ARN\")\n",
    "    region_name: str = Field(default=\"us-gov-east-1\", description=\"AWS GovCloud region\")\n",
    "    endpoint_url: Optional[str] = Field(default=None, description=\"Custom endpoint URL for VPC endpoints\")\n",
    "    aws_access_key_id: Optional[str] = Field(default=None, description=\"AWS access key ID\")\n",
    "    aws_secret_access_key: Optional[str] = Field(default=None, description=\"AWS secret access key\")\n",
    "    max_tokens: int = Field(default=512, description=\"Maximum tokens to generate\")\n",
    "    temperature: float = Field(default=0.7, description=\"Sampling temperature\")\n",
    "    streaming: bool = Field(default=False, description=\"Enable streaming responses\")\n",
    "    client: Any = Field(default=None, exclude=True, description=\"Boto3 client instance\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        region_name: str = \"us-gov-east-1\",\n",
    "        endpoint_url: Optional[str] = None,\n",
    "        aws_access_key_id: Optional[str] = None,\n",
    "        aws_secret_access_key: Optional[str] = None,\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        streaming: bool = False,\n",
    "        callbacks: Optional[List] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ChatGovCloudBedrock.\n",
    "        \n",
    "        Args:\n",
    "            model_id: The Bedrock model ID or inference profile ARN\n",
    "            region_name: AWS GovCloud region (us-gov-east-1 or us-gov-west-1)\n",
    "            endpoint_url: Custom endpoint URL for VPC endpoints\n",
    "            aws_access_key_id: AWS access key ID (or use environment variables)\n",
    "            aws_secret_access_key: AWS secret access key (or use environment variables)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            streaming: Enable streaming responses\n",
    "            callbacks: LangChain callbacks\n",
    "            **kwargs: Additional parameters\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model_id=model_id,\n",
    "            region_name=region_name,\n",
    "            endpoint_url=endpoint_url,\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            streaming=streaming,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def model_post_init(self, __context: Any) -> None:\n",
    "        \"\"\"Initialize boto3 client after model validation.\"\"\"\n",
    "        # Initialize boto3 client\n",
    "        client_kwargs = {\n",
    "            \"service_name\": \"bedrock-runtime\",\n",
    "            \"region_name\": self.region_name,\n",
    "        }\n",
    "        \n",
    "        if self.endpoint_url:\n",
    "            client_kwargs[\"endpoint_url\"] = self.endpoint_url\n",
    "            \n",
    "        if self.aws_access_key_id:\n",
    "            client_kwargs[\"aws_access_key_id\"] = self.aws_access_key_id\n",
    "        else:\n",
    "            client_kwargs[\"aws_access_key_id\"] = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "            \n",
    "        if self.aws_secret_access_key:\n",
    "            client_kwargs[\"aws_secret_access_key\"] = self.aws_secret_access_key\n",
    "        else:\n",
    "            client_kwargs[\"aws_secret_access_key\"] = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "        \n",
    "        try:\n",
    "            import boto3\n",
    "        except ImportError:\n",
    "            raise ImportError('Please install boto3: pip install boto3')\n",
    "\n",
    "        self.client = boto3.client(**client_kwargs)\n",
    "\n",
    "    def _convert_messages_to_bedrock_format(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Convert LangChain messages to Bedrock API format.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of LangChain BaseMessage objects\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries in Bedrock API format\n",
    "        \"\"\"\n",
    "        bedrock_messages = []\n",
    "        \n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": message.content\n",
    "                })\n",
    "            elif isinstance(message, AIMessage):\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": message.content\n",
    "                })\n",
    "            elif isinstance(message, SystemMessage):\n",
    "                # For Claude models, system messages can be handled as user messages\n",
    "                # or through the system parameter in the request body\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"System: {message.content}\"\n",
    "                })\n",
    "            else:\n",
    "                # Default to user role for unknown message types\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": str(message.content)\n",
    "                })\n",
    "        \n",
    "        return bedrock_messages\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"\n",
    "        Generate chat response using AWS Bedrock.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of chat messages\n",
    "            stop: List of stop sequences\n",
    "            run_manager: Callback manager\n",
    "            **kwargs: Additional parameters\n",
    "            \n",
    "        Returns:\n",
    "            ChatResult containing the response\n",
    "        \"\"\"\n",
    "        # If streaming is enabled, use streaming and collect all chunks\n",
    "        if self.streaming:\n",
    "            chunks = []\n",
    "            full_content = \"\"\n",
    "            for chunk in self._stream(messages, stop=stop, run_manager=run_manager, **kwargs):\n",
    "                chunks.append(chunk)\n",
    "                if chunk.message.content:\n",
    "                    full_content += chunk.message.content\n",
    "            \n",
    "            # Return final result with complete content\n",
    "            final_message = AIMessage(content=full_content)\n",
    "            return ChatResult(generations=[ChatGeneration(message=final_message)])\n",
    "        \n",
    "        # Non-streaming path\n",
    "        # Convert messages to Bedrock format\n",
    "        bedrock_messages = self._convert_messages_to_bedrock_format(messages)\n",
    "        \n",
    "        # Prepare request body\n",
    "        body = {\n",
    "            \"messages\": bedrock_messages,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }\n",
    "        \n",
    "        # Add stop sequences if provided\n",
    "        if stop:\n",
    "            body[\"stop_sequences\"] = stop\n",
    "            \n",
    "        try:\n",
    "            # Make the API call\n",
    "            response = self.client.invoke_model(\n",
    "                modelId=self.model_id,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "                body=json.dumps(body).encode(\"utf-8\")\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            response_body = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
    "            \n",
    "            # Extract the generated text\n",
    "            if \"content\" in response_body and len(response_body[\"content\"]) > 0:\n",
    "                generated_text = response_body[\"content\"][0][\"text\"]\n",
    "            else:\n",
    "                generated_text = \"\"\n",
    "            \n",
    "            # Create AIMessage\n",
    "            message = AIMessage(content=generated_text)\n",
    "            \n",
    "            # Create ChatGeneration\n",
    "            generation = ChatGeneration(message=message)\n",
    "            \n",
    "            return ChatResult(generations=[generation])\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error calling AWS Bedrock: {str(e)}\")\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGeneration]:\n",
    "        \"\"\"\n",
    "        Stream chat response using AWS Bedrock.\n",
    "        \"\"\"\n",
    "        # Convert messages to Bedrock format\n",
    "        bedrock_messages = self._convert_messages_to_bedrock_format(messages)\n",
    "        \n",
    "        # Prepare request body\n",
    "        body = {\n",
    "            \"messages\": bedrock_messages,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }\n",
    "        \n",
    "        # Add stop sequences if provided\n",
    "        if stop:\n",
    "            body[\"stop_sequences\"] = stop\n",
    "            \n",
    "        try:\n",
    "            # Use streaming API\n",
    "            response = self.client.invoke_model_with_response_stream(\n",
    "                modelId=self.model_id,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "                body=json.dumps(body).encode(\"utf-8\")\n",
    "            )\n",
    "            \n",
    "            # Process streaming response\n",
    "            for event in response[\"body\"]:\n",
    "                chunk = json.loads(event[\"chunk\"][\"bytes\"].decode(\"utf-8\"))\n",
    "                \n",
    "                if chunk.get(\"type\") == \"content_block_delta\":\n",
    "                    if \"delta\" in chunk and \"text\" in chunk[\"delta\"]:\n",
    "                        text = chunk[\"delta\"][\"text\"]\n",
    "                        if run_manager:\n",
    "                            # Check if run_manager is async or sync\n",
    "                            if hasattr(run_manager.on_llm_new_token, '__call__'):\n",
    "                                try:\n",
    "                                    import asyncio\n",
    "                                    if asyncio.iscoroutinefunction(run_manager.on_llm_new_token):\n",
    "                                        # Skip async callback in sync context for now\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        run_manager.on_llm_new_token(text)\n",
    "                                except:\n",
    "                                    run_manager.on_llm_new_token(text)\n",
    "                            else:\n",
    "                                run_manager.on_llm_new_token(text)\n",
    "                        yield ChatGeneration(message=AIMessage(content=text))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            # Fallback to non-streaming if streaming fails\n",
    "            result = self._generate(messages, stop=stop, run_manager=run_manager, **kwargs)\n",
    "            yield result.generations[0]\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"\n",
    "        Async generate chat response using AWS Bedrock.\n",
    "        \"\"\"\n",
    "        # For now, run the sync version in a thread pool\n",
    "        import asyncio\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(\n",
    "            None, \n",
    "            lambda: self._generate(messages, stop, run_manager, **kwargs)\n",
    "        )\n",
    "\n",
    "    async def _astream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGeneration]:\n",
    "        \"\"\"\n",
    "        Async stream chat response using AWS Bedrock.\n",
    "        \"\"\"\n",
    "        # For now, run the sync version in a thread pool\n",
    "        import asyncio\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def sync_stream():\n",
    "            return list(self._stream(messages, stop, run_manager, **kwargs))\n",
    "        \n",
    "        chunks = await loop.run_in_executor(None, sync_stream)\n",
    "        for chunk in chunks:\n",
    "            yield chunk\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of chat model.\"\"\"\n",
    "        return \"govcloud-bedrock\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"region_name\": self.region_name,\n",
    "            \"endpoint_url\": self.endpoint_url,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "This example shows how to use **OnPrem.LLM** with cloud LLMs served from **AWS GovCloud**.\n",
    "\n",
    "The example below assumes you have set both `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` as environment variables.\n",
    "You can adjust the `inference_arn`, `endpoint_url`, and `region_name` based on your application scenario.\n",
    "\n",
    "```python\n",
    "from onprem import LLM\n",
    "\n",
    "inference_arn = \"YOUR INFERENCE ARN\"\n",
    "endpoint_url = \"YOUR ENDPOINT URL\"\n",
    "region_name = \"us-gov-east-1\" # replace as necessary\n",
    "\n",
    "# set up LLM connection to Bedrock on AWS GovCloud\n",
    "llm = LLM(\n",
    "  f\"govcloud-bedrock://{inference_arn}\",\n",
    "  region_name=region_name,\n",
    "  endpoint_url=endpoint_url,\n",
    ")\n",
    "\n",
    "# send prompt to LLM\n",
    "response = llm.prompt(\"Write a haiku about the moon.\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
