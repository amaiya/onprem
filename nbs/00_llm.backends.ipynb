{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm backends\n",
    "\n",
    "> Support classes for different LLM backends (e.g., AWS GovCloud LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp llm.backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "Custom LangChain Chat classes for various frameworks and cloud providers.\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "\n",
    "from pydantic import Field\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "\n",
    "\n",
    "class ChatGovCloudBedrock(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Custom LangChain Chat model for AWS GovCloud Bedrock.\n",
    "    \n",
    "    This class provides integration with Amazon Bedrock running in AWS GovCloud regions,\n",
    "    supporting custom VPC endpoints and GovCloud-specific configurations.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_id: str = Field(description=\"The Bedrock model ID or inference profile ARN\")\n",
    "    region_name: str = Field(default=\"us-gov-east-1\", description=\"AWS GovCloud region\")\n",
    "    endpoint_url: Optional[str] = Field(default=None, description=\"Custom endpoint URL for VPC endpoints\")\n",
    "    aws_access_key_id: Optional[str] = Field(default=None, description=\"AWS access key ID\")\n",
    "    aws_secret_access_key: Optional[str] = Field(default=None, description=\"AWS secret access key\")\n",
    "    max_tokens: int = Field(default=512, description=\"Maximum tokens to generate\")\n",
    "    temperature: float = Field(default=0.7, description=\"Sampling temperature\")\n",
    "    streaming: bool = Field(default=False, description=\"Enable streaming responses\")\n",
    "    client: Any = Field(default=None, exclude=True, description=\"Boto3 client instance\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        region_name: str = \"us-gov-east-1\",\n",
    "        endpoint_url: Optional[str] = None,\n",
    "        aws_access_key_id: Optional[str] = None,\n",
    "        aws_secret_access_key: Optional[str] = None,\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        streaming: bool = False,\n",
    "        callbacks: Optional[List] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ChatGovCloudBedrock.\n",
    "        \n",
    "        Args:\n",
    "            model_id: The Bedrock model ID or inference profile ARN\n",
    "            region_name: AWS GovCloud region (us-gov-east-1 or us-gov-west-1)\n",
    "            endpoint_url: Custom endpoint URL for VPC endpoints\n",
    "            aws_access_key_id: AWS access key ID (or use environment variables)\n",
    "            aws_secret_access_key: AWS secret access key (or use environment variables)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            streaming: Enable streaming responses\n",
    "            callbacks: LangChain callbacks\n",
    "            **kwargs: Additional parameters\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model_id=model_id,\n",
    "            region_name=region_name,\n",
    "            endpoint_url=endpoint_url,\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            streaming=streaming,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def model_post_init(self, __context: Any) -> None:\n",
    "        \"\"\"Initialize boto3 client after model validation.\"\"\"\n",
    "        # Initialize boto3 client\n",
    "        client_kwargs = {\n",
    "            \"service_name\": \"bedrock-runtime\",\n",
    "            \"region_name\": self.region_name,\n",
    "        }\n",
    "        \n",
    "        if self.endpoint_url:\n",
    "            client_kwargs[\"endpoint_url\"] = self.endpoint_url\n",
    "            \n",
    "        if self.aws_access_key_id:\n",
    "            client_kwargs[\"aws_access_key_id\"] = self.aws_access_key_id\n",
    "        else:\n",
    "            client_kwargs[\"aws_access_key_id\"] = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "            \n",
    "        if self.aws_secret_access_key:\n",
    "            client_kwargs[\"aws_secret_access_key\"] = self.aws_secret_access_key\n",
    "        else:\n",
    "            client_kwargs[\"aws_secret_access_key\"] = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "        \n",
    "        self.client = boto3.client(**client_kwargs)\n",
    "\n",
    "    def _convert_messages_to_bedrock_format(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Convert LangChain messages to Bedrock API format.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of LangChain BaseMessage objects\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries in Bedrock API format\n",
    "        \"\"\"\n",
    "        bedrock_messages = []\n",
    "        \n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": message.content\n",
    "                })\n",
    "            elif isinstance(message, AIMessage):\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": message.content\n",
    "                })\n",
    "            elif isinstance(message, SystemMessage):\n",
    "                # For Claude models, system messages can be handled as user messages\n",
    "                # or through the system parameter in the request body\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"System: {message.content}\"\n",
    "                })\n",
    "            else:\n",
    "                # Default to user role for unknown message types\n",
    "                bedrock_messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": str(message.content)\n",
    "                })\n",
    "        \n",
    "        return bedrock_messages\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"\n",
    "        Generate chat response using AWS Bedrock.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of chat messages\n",
    "            stop: List of stop sequences\n",
    "            run_manager: Callback manager\n",
    "            **kwargs: Additional parameters\n",
    "            \n",
    "        Returns:\n",
    "            ChatResult containing the response\n",
    "        \"\"\"\n",
    "        # If streaming is enabled, use streaming and collect all chunks\n",
    "        if self.streaming:\n",
    "            chunks = []\n",
    "            full_content = \"\"\n",
    "            for chunk in self._stream(messages, stop=stop, run_manager=run_manager, **kwargs):\n",
    "                chunks.append(chunk)\n",
    "                if chunk.message.content:\n",
    "                    full_content += chunk.message.content\n",
    "            \n",
    "            # Return final result with complete content\n",
    "            final_message = AIMessage(content=full_content)\n",
    "            return ChatResult(generations=[ChatGeneration(message=final_message)])\n",
    "        \n",
    "        # Non-streaming path\n",
    "        # Convert messages to Bedrock format\n",
    "        bedrock_messages = self._convert_messages_to_bedrock_format(messages)\n",
    "        \n",
    "        # Prepare request body\n",
    "        body = {\n",
    "            \"messages\": bedrock_messages,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }\n",
    "        \n",
    "        # Add stop sequences if provided\n",
    "        if stop:\n",
    "            body[\"stop_sequences\"] = stop\n",
    "            \n",
    "        try:\n",
    "            # Make the API call\n",
    "            response = self.client.invoke_model(\n",
    "                modelId=self.model_id,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "                body=json.dumps(body).encode(\"utf-8\")\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            response_body = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
    "            \n",
    "            # Extract the generated text\n",
    "            if \"content\" in response_body and len(response_body[\"content\"]) > 0:\n",
    "                generated_text = response_body[\"content\"][0][\"text\"]\n",
    "            else:\n",
    "                generated_text = \"\"\n",
    "            \n",
    "            # Create AIMessage\n",
    "            message = AIMessage(content=generated_text)\n",
    "            \n",
    "            # Create ChatGeneration\n",
    "            generation = ChatGeneration(message=message)\n",
    "            \n",
    "            return ChatResult(generations=[generation])\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error calling AWS Bedrock: {str(e)}\")\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGeneration]:\n",
    "        \"\"\"\n",
    "        Stream chat response using AWS Bedrock.\n",
    "        \"\"\"\n",
    "        # Convert messages to Bedrock format\n",
    "        bedrock_messages = self._convert_messages_to_bedrock_format(messages)\n",
    "        \n",
    "        # Prepare request body\n",
    "        body = {\n",
    "            \"messages\": bedrock_messages,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }\n",
    "        \n",
    "        # Add stop sequences if provided\n",
    "        if stop:\n",
    "            body[\"stop_sequences\"] = stop\n",
    "            \n",
    "        try:\n",
    "            # Use streaming API\n",
    "            response = self.client.invoke_model_with_response_stream(\n",
    "                modelId=self.model_id,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\",\n",
    "                body=json.dumps(body).encode(\"utf-8\")\n",
    "            )\n",
    "            \n",
    "            # Process streaming response\n",
    "            for event in response[\"body\"]:\n",
    "                chunk = json.loads(event[\"chunk\"][\"bytes\"].decode(\"utf-8\"))\n",
    "                \n",
    "                if chunk.get(\"type\") == \"content_block_delta\":\n",
    "                    if \"delta\" in chunk and \"text\" in chunk[\"delta\"]:\n",
    "                        text = chunk[\"delta\"][\"text\"]\n",
    "                        if run_manager:\n",
    "                            run_manager.on_llm_new_token(text)\n",
    "                        yield ChatGeneration(message=AIMessage(content=text))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            # Fallback to non-streaming if streaming fails\n",
    "            result = self._generate(messages, stop=stop, run_manager=run_manager, **kwargs)\n",
    "            yield result.generations[0]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of chat model.\"\"\"\n",
    "        return \"govcloud-bedrock\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"region_name\": self.region_name,\n",
    "            \"endpoint_url\": self.endpoint_url,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "The example below assumes you have set both `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` as environment variables.\n",
    "You can adjust the `inference_arn`, `endpoint_url`, and `region` based on your application scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a haiku about the moon:\n",
      "\n",
      "Pale orb in night sky\n",
      "Casting silver on still lakes\n",
      "Silent guardian"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "from onprem import LLM\n",
    "\n",
    "inference_arn = \"YOUR INFERENCE ARN\"\n",
    "endpoint_url = \"YOUR ENDPOINT URL\"\n",
    "region_name = \"us-gov-east-1\" # replace as necessary\n",
    "\n",
    "# set up LLM connection to Bedrock on AWS GovCloud\n",
    "llm = LLM(\n",
    "  f\"govcloud-bedrock://{inference_arn}\",\n",
    "  region_name=region_name,\n",
    "  endpoint_url=endpoint_url,\n",
    ")\n",
    "\n",
    "# send prompt to LLM\n",
    "response = llm.prompt(\"Write a haiku about the moon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
