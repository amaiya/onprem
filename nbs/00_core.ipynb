{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Core functionality for `onprem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from onprem import utils as U\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "import os\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# reference: https://github.com/langchain-ai/langchain/issues/5630#issuecomment-1574222564\n",
    "class AnswerConversationBufferMemory(ConversationBufferMemory):\n",
    "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n",
    "        return super(AnswerConversationBufferMemory, self).save_context(\n",
    "            inputs, {\"response\": outputs[\"answer\"]}\n",
    "        )\n",
    "\n",
    "MIN_MODEL_SIZE = 250000000\n",
    "MISTRAL_MODEL_URL = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "MISTRAL_MODEL_ID = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
    "MISTRAL_PROMPT_TEMPLATE = \"[INST] {prompt} [/INST]\"\n",
    "ZEPHYR_MODEL_URL = \"https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "ZEPHYR_MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "ZEPHYR_PROMPT_TEMPLATE = \"<|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\"\n",
    "LLAMA_MODEL_URL = \"https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n",
    "LLAMA_MODEL_ID = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
    "LLAMA_PROMPT_TEMPLATE = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a super-intelligent helpful assistant that executes instructions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "MODEL_URL_DICT = {'mistral' : MISTRAL_MODEL_URL, 'zephyr':ZEPHYR_MODEL_URL, 'llama' : LLAMA_MODEL_URL}\n",
    "MODEL_ID_DICT = {'mistral': MISTRAL_MODEL_ID, 'zephyr' : ZEPHYR_MODEL_ID, 'llama': LLAMA_MODEL_ID}\n",
    "LLAMA_CPP = 'llama.cpp'\n",
    "TRANSFORMERS = 'transformers'\n",
    "ENGINE_DICT = {LLAMA_CPP: MODEL_URL_DICT, TRANSFORMERS : MODEL_ID_DICT}\n",
    "PROMPT_DICT = {'mistral': MISTRAL_PROMPT_TEMPLATE, 'zephyr' : ZEPHYR_PROMPT_TEMPLATE, 'llama' : LLAMA_PROMPT_TEMPLATE} \n",
    "DEFAULT_MODEL = 'mistral'\n",
    "DEFAULT_ENGINE = LLAMA_CPP\n",
    "DEFAULT_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DEFAULT_QA_PROMPT = \"\"\"\"Use the following pieces of context delimited by three backticks to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "```{context}```\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url:Optional[str] = None,\n",
    "        model_id:Optional[str] = None,\n",
    "        default_model:str  = DEFAULT_MODEL,\n",
    "        default_engine:str = DEFAULT_ENGINE,\n",
    "        n_gpu_layers: Optional[int] = None,\n",
    "        prompt_template: Optional[str] = None,\n",
    "        model_download_path: Optional[str] = None,\n",
    "        vectordb_path: Optional[str] = None,\n",
    "        max_tokens: int = 512,\n",
    "        n_ctx: int = 3900,\n",
    "        n_batch: int = 1024,\n",
    "        stop:list=[],\n",
    "        mute_stream: bool = False,\n",
    "        callbacks=[],\n",
    "        embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        embedding_model_kwargs: dict = {\"device\": \"cpu\"},\n",
    "        embedding_encode_kwargs: dict = {\"normalize_embeddings\": False},\n",
    "        rag_num_source_docs: int = 4,\n",
    "        rag_score_threshold: float = 0.0,\n",
    "        check_model_download:bool=True,\n",
    "        confirm: bool = True,\n",
    "        verbose: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LLM Constructor.  Extra `kwargs` (e.g., temperature) are fed directly to `langchain.llms.LlamaCpp` \n",
    "        or `langchain.hugging_face.HuggingFacePipeline`.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *model_url*: URL to `.GGUF` model (or the filename if already been downloaded to `model_download_path`).\n",
    "                       To use an OpenAI-compatible REST API (e.g., vLLM, OpenLLM, Ollama), supply the URL (e.g., `http://localhost:8080/v1`).\n",
    "                       To use a cloud-based OpenAI model, replace URL with: `openai://<name_of_model>` (e.g., `openai://gpt-3.5-turbo`).\n",
    "                       To use Azure OpenAI, replace URL with: with: `azure://<deployment_name>`.\n",
    "                       If None, use the model indicated by `default_model`.\n",
    "        - *model_id*: Name of or path to Hugging Face model (e.g., in SafeTensor format). Hugging Face Transformers is used for LLM generation instead of **llama-cpp-python**. Mutually-exclusive with `model_url` and `default_model`. The `n_gpu_layers` and `model_download_path` parameters are ignored if `model_id` is supplied.\n",
    "        - *default_model*: One of {'mistral', 'zephyr', 'llama'}, where mistral is Mistral-Instruct-7B-v0.2, zephyr is Zephyr-7B-beta, and llama is Llama-3.1-8B.\n",
    "        - *default_engine*: The engine used to run the `default_model`. One of {'llama.cpp', 'transformers'}.\n",
    "        - *n_gpu_layers*: Number of layers to be loaded into gpu memory. Default is `None`.\n",
    "        - *prompt_template*: Optional prompt template (must have a variable named \"prompt\").\n",
    "        - *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
    "        - *vectordb_path*: Path to vector database (created if it doesn't exist).\n",
    "                           Default is `onprem_data/vectordb` in user's home directory.\n",
    "        - *max_tokens*: The maximum number of tokens to generate.\n",
    "        - *n_ctx*: Token context window. (Llama2 models have max of 4096.)\n",
    "        - *n_batch*: Number of tokens to process in parallel.\n",
    "        - *stop*: a list of strings to stop generation when encountered (applied to all calls to `LLM.prompt`)\n",
    "        - *mute_stream*: Mute ChatGPT-like token stream output during generation\n",
    "        - *callbacks*: Callbacks to supply model\n",
    "        - *embedding_model_name*: name of sentence-transformers model. Used for `LLM.ingest` and `LLM.ask`.\n",
    "        - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`).\n",
    "        - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                     embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        - *rag_num_source_docs*: The maximum number of documents retrieved and fed to `LLM.ask` and `LLM.chat` to generate answers\n",
    "        - *rag_score_threshold*: Minimum similarity score for source to be considered by `LLM.ask` and `LLM.chat`\n",
    "        - *confirm*: whether or not to confirm with user before downloading a model\n",
    "        - *verbose*: Verbosity\n",
    "        \"\"\"\n",
    "        self.model_id = None\n",
    "        self.model_url = None\n",
    "        if model_url and model_id:\n",
    "            raise ValueError('The parameters model_url and model_id are mutually-exclusive.')\n",
    "        elif model_id:\n",
    "            self.model_id = model_id\n",
    "            self.model_name = os.path.basename(model_id)\n",
    "        elif model_url:\n",
    "            self.model_url = model_url.split(\"?\")[0]\n",
    "            self.model_name = os.path.basename(model_url)\n",
    "            self.model_url = MODEL_DICT[default_model] if not model_url else model_url\n",
    "        else: # neither supplied so use defaults\n",
    "            url_or_id = ENGINE_DICT[default_engine][default_model]\n",
    "            self.model_name = os.path.basename(url_or_id)\n",
    "            if default_engine == LLAMA_CPP:\n",
    "                self.model_url = url_or_id\n",
    "                self.model_id = None\n",
    "                prompt_template = PROMPT_DICT[default_model] if not prompt_template else prompt_template\n",
    "            else:\n",
    "                self.model_url = None\n",
    "                self.model_id = url_or_id\n",
    "        self.model_download_path = model_download_path or U.get_datadir()\n",
    "\n",
    "        if self.is_llamacpp():\n",
    "            try:\n",
    "                from llama_cpp import Llama\n",
    "            except ImportError:\n",
    "                raise ValueError('To run local LLMs, the llama-cpp-python package is required. ' +\\\n",
    "                                 'You can visit https://python.langchain.com/docs/integrations/llms/llamacpp ' +\\\n",
    "                                 'and follow the instructions for your operating system.')\n",
    "        if self.is_llamacpp() and not os.path.isfile(os.path.join(self.model_download_path, self.model_name)):\n",
    "            self.download_model(\n",
    "                self.model_url,\n",
    "                model_download_path=self.model_download_path,\n",
    "                confirm=confirm,\n",
    "            )\n",
    "        self.prompt_template = prompt_template\n",
    "        self.vectordb_path = vectordb_path\n",
    "        self.llm = None\n",
    "        self.ingester = None\n",
    "        self.qa = None\n",
    "        self.chatqa = None\n",
    "        self.n_gpu_layers = n_gpu_layers\n",
    "        self.max_tokens = max_tokens\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_batch = n_batch\n",
    "        self.stop = stop\n",
    "        self.mute_stream = mute_stream\n",
    "        self.callbacks = [] if mute_stream else [StreamingStdOutCallbackHandler()]\n",
    "        if callbacks:\n",
    "            self.callbacks.extend(callbacks)\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_kwargs = embedding_model_kwargs\n",
    "        self.embedding_encode_kwargs = embedding_encode_kwargs\n",
    "        self.rag_num_source_docs = rag_num_source_docs\n",
    "        self.rag_score_threshold = rag_score_threshold\n",
    "        self.check_model_download = check_model_download\n",
    "        self.verbose = verbose\n",
    "        self.extra_kwargs = kwargs\n",
    "\n",
    "\n",
    "        # explicitly set offload_kqv\n",
    "        # reference: https://github.com/abetlen/llama-cpp-python/issues/999#issuecomment-1858041458\n",
    "        # commented out now: no longer needed as of llama-cpp-python==0.2.75\n",
    "        #self.offload_kqv = True if n_gpu_layers is not None and n_gpu_layers > 0 else False\n",
    "\n",
    "        # load LLM\n",
    "        self.load_llm()\n",
    "\n",
    "        # issue warning\n",
    "        if self.is_openai_model():\n",
    "            warnings.warn(f'The model you supplied is {self.model_name}, an external service (i.e., not on-premises). '+\\\n",
    "                          'Use with caution, as your data and prompts will be sent externally.')\n",
    "\n",
    "    def is_openai_model(self):\n",
    "        return self.model_url and self.model_url.lower().startswith('openai')\n",
    "\n",
    "    def is_azure(self):\n",
    "        return self.model_url and self.model_url.lower().startswith('azure')\n",
    "\n",
    "\n",
    "    def is_local_api(self):\n",
    "        basename = os.path.basename(self.model_url) if self.model_url else None\n",
    "        return self.model_url and self.model_url.lower().startswith('http') and not basename.lower().endswith('.gguf') and not basename.lower().endswith('.bin')\n",
    "\n",
    "    def is_local(self):\n",
    "        return not self.is_openai_model() and not self.is_local_api()\n",
    "\n",
    "    def is_llamacpp(self):\n",
    "        return self.is_local() and not self.is_hf()\n",
    "\n",
    "    def is_hf(self):\n",
    "        return self.model_id is not None\n",
    "\n",
    "    def update_max_tokens(self, value:int=512):\n",
    "        \"\"\"\n",
    "        Update `max_tokens` (maximum length of generation).\n",
    "        \"\"\"\n",
    "        llm = self.load_llm()\n",
    "        llm.max_tokens = value\n",
    "\n",
    "\n",
    "    def update_stop(self, value:list=[]):\n",
    "        \"\"\"\n",
    "        Update `max_tokens` (maximum length of generation).\n",
    "        \"\"\"\n",
    "        llm = self.load_llm()\n",
    "        llm.stop = value\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def download_model(\n",
    "        cls,\n",
    "        model_url:Optional[str] = None,\n",
    "        default_model:str=DEFAULT_MODEL,\n",
    "        model_download_path: Optional[str] = None,\n",
    "        confirm: bool = True,\n",
    "        ssl_verify: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *model_url*: URL of model. If None, then use default_model.\n",
    "        - *default_model*: One of {'mistral', 'zephyr', 'llama'}, where mistral is Mistral-Instruct-7B-v0.2, zephyr is Zephyr-7B-beta, and llama is Llama-3.1-8B.\n",
    "        - *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
    "        - *confirm*: whether or not to confirm with user before downloading\n",
    "        - *ssl_verify*: If True, SSL certificates are verified.\n",
    "                        You can set to False if corporate firewall gives you problems.\n",
    "        \"\"\"\n",
    "        model_url = MODEL_URL_DICT[default_model] if not model_url else model_url\n",
    "        if 'https://huggingface.co' in model_url and 'resolve' not in model_url:\n",
    "            warnings.warn('\\n\\nThe supplied URL may not be pointing to the actual GGUF model file.  Please check it.\\n\\n')\n",
    "        datadir = model_download_path or U.get_datadir()\n",
    "        model_name = os.path.basename(model_url)\n",
    "        filename = os.path.join(datadir, model_name)\n",
    "        confirm_msg = f\"\\nYou are about to download the LLM {model_name} to the {datadir} folder. Are you sure?\"\n",
    "        if os.path.isfile(filename):\n",
    "            confirm_msg = f\"There is already a file {model_name} in {datadir}.\\n Do you want to still download it?\"\n",
    "\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            shall = input(\"%s (y/N) \" % confirm_msg).lower() == \"y\"\n",
    "        if shall:\n",
    "            U.download(model_url, filename, verify=ssl_verify)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f'{model_name} was not downloaded because \"Y\" was not selected.'\n",
    "            )\n",
    "        return\n",
    "\n",
    "    def load_ingester(self):\n",
    "        \"\"\"\n",
    "        Get `Ingester` instance.\n",
    "        You can access the `langchain_chroma.Chroma` instance with `load_ingester().get_db()`.\n",
    "        \"\"\"\n",
    "        if not self.ingester:\n",
    "            from onprem.ingest import Ingester\n",
    "\n",
    "            self.ingester = Ingester(\n",
    "                embedding_model_name=self.embedding_model_name,\n",
    "                embedding_model_kwargs=self.embedding_model_kwargs,\n",
    "                embedding_encode_kwargs=self.embedding_encode_kwargs,\n",
    "                persist_directory=self.vectordb_path,\n",
    "            )\n",
    "        return self.ingester\n",
    "\n",
    "    def load_vectordb(self):\n",
    "        \"\"\"\n",
    "        Get Chroma db instance\n",
    "        \"\"\"\n",
    "        ingester = self.load_ingester()\n",
    "        db = ingester.get_db()\n",
    "        if not db:\n",
    "            raise ValueError(\n",
    "                \"A vector database has not yet been created. Please call the LLM.ingest method.\"\n",
    "            )\n",
    "        return db\n",
    "\n",
    "    def ingest(\n",
    "        self,\n",
    "        source_directory: str, # path to folder containing documents\n",
    "        chunk_size: int = 500, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        chunk_overlap: int = 50, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        ignore_fn:Optional[Callable] = None, # callable that accepts the file path and returns True for ignored files\n",
    "        pdf_use_unstructured:bool=False, # If True, use unstructured for PDF extraction\n",
    "        **kwargs, # Extra kwargs fed to `langchain_community.document_loaders.pdf.UnstructuredPDFLoader` when pdf_use_unstructured is True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` into vector database.\n",
    "        Previously-ingested documents are ignored.\n",
    "        Extra kwargs fed directly to `langchain_community.document_loaders.pdf.UnstructuredPDFLoader` when pdf_use_unstructured is True.\n",
    "        \"\"\"\n",
    "        ingester = self.load_ingester()\n",
    "        return ingester.ingest(\n",
    "            source_directory,\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, ignore_fn=ignore_fn,\n",
    "            pdf_use_unstructured=pdf_use_unstructured,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def check_model(self):\n",
    "        \"\"\"\n",
    "        Returns the path to the model\n",
    "        \"\"\"\n",
    "        if not self.is_llamacpp():\n",
    "            return None\n",
    "        datadir = self.model_download_path\n",
    "        model_path = os.path.join(datadir, self.model_name)\n",
    "        if not os.path.isfile(model_path):\n",
    "            raise ValueError(\n",
    "                f\"The LLM model {self.model_name} does not appear to have been downloaded. \"\n",
    "                + \"Execute the download_model() method to download it.\"\n",
    "            )\n",
    "        return model_path\n",
    "\n",
    "    def load_llm(self):\n",
    "        \"\"\"\n",
    "        Loads the LLM from the model path.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.llm and self.is_openai_model():\n",
    "            self.llm = ChatOpenAI(model_name=self.model_name, \n",
    "                                  callbacks=self.callbacks, \n",
    "                                  streaming=not self.mute_stream,\n",
    "                                  max_tokens=self.max_tokens,\n",
    "                                  **self.extra_kwargs)\n",
    "        elif not self.llm and self.is_azure():\n",
    "            self.llm = AzureChatOpenAI(azure_deployment=self.model_name, \n",
    "                                  callbacks=self.callbacks, \n",
    "                                  streaming=not self.mute_stream,\n",
    "                                  max_tokens=self.max_tokens,\n",
    "                                  **self.extra_kwargs)\n",
    "\n",
    "        elif not self.llm and self.is_local_api():\n",
    "            self.llm = ChatOpenAI(base_url=self.model_url,\n",
    "                                  #model_name=self.model_name, \n",
    "                                  callbacks=self.callbacks, \n",
    "                                  streaming=not self.mute_stream,\n",
    "                                  max_tokens=self.max_tokens,\n",
    "                                  **self.extra_kwargs)\n",
    "        elif not self.llm and self.is_hf():\n",
    "            # Hugging Face model\n",
    "            from transformers import BitsAndBytesConfig, TextStreamer, AutoTokenizer\n",
    "            from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "            import torch\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=\"float16\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')\n",
    "            streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "            hfpipe = HuggingFacePipeline.from_model_id(\n",
    "                model_id=self.model_id,\n",
    "                task=\"text-generation\",\n",
    "                pipeline_kwargs=dict(\n",
    "                     max_new_tokens=self.max_tokens,\n",
    "                     #max_length = self.n_ctx, # cannot supply both\n",
    "                     do_sample=True if self.extra_kwargs.get('temperature', 0.8)>0.0 else False ,\n",
    "                     repetition_penalty=1.03,\n",
    "                     return_full_text=False,\n",
    "                    streamer=streamer,\n",
    "                    **self.extra_kwargs,\n",
    "                     #max_memory={0: \"5GiB\", \"cpu\": \"30GiB\"},\n",
    "                ),\n",
    "                model_kwargs={\"quantization_config\": quantization_config,\n",
    "                              \"torch_dtype\": torch.bfloat16}\n",
    "                )\n",
    "            self.llm = ChatHuggingFace(llm=hfpipe)\n",
    "\n",
    "        elif not self.llm:\n",
    "            model_path = self.check_model()\n",
    "            if self.check_model_download and os.path.getsize(model_path) < MIN_MODEL_SIZE:\n",
    "                raise ValueError(f'The model file ({model_path} is less than {MIN_MODEL_SIZE} bytes. ' +\\\n",
    "                                 'It may not have been fully downloaded. Please delete the file and start again. ')\n",
    "            self.llm = LlamaCpp(\n",
    "                model_path=model_path,\n",
    "                max_tokens=self.max_tokens,\n",
    "                n_batch=self.n_batch,\n",
    "                callbacks=self.callbacks,\n",
    "                verbose=self.verbose,\n",
    "                n_gpu_layers=self.n_gpu_layers,\n",
    "                n_ctx=self.n_ctx,\n",
    "                #offload_kqv = self.offload_kqv,\n",
    "                **self.extra_kwargs,\n",
    "            )\n",
    "\n",
    "        return self.llm\n",
    "\n",
    "\n",
    "    def prompt(self, prompt, prompt_template: Optional[str] = None, stop:list=[], **kwargs):\n",
    "        \"\"\"\n",
    "        Send prompt to LLM to generate a response.\n",
    "        Extra keyword arguments are sent directly to the model invocation.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *prompt*: The prompt to supply to the model\n",
    "        - *prompt_template*: Optional prompt template (must have a variable named \"prompt\").\n",
    "                             This value will override any `prompt_template` value supplied \n",
    "                             to `LLM` constructor.\n",
    "        - *stop*: a list of strings to stop generation when encountered. \n",
    "                  This value will override the `stop` parameter supplied to `LLM` constructor.\n",
    "\n",
    "        \"\"\"\n",
    "        llm = self.load_llm()\n",
    "        prompt_template = self.prompt_template if prompt_template is None else prompt_template\n",
    "        if prompt_template:\n",
    "            prompt = prompt_template.format(**{\"prompt\": prompt})\n",
    "        stop = stop if stop else self.stop\n",
    "        res = llm.invoke(prompt, stop=stop, **kwargs)\n",
    "        return res.content if self.is_openai_model() else res\n",
    "\n",
    "\n",
    "\n",
    "    def load_qa(self, prompt_template: str = DEFAULT_QA_PROMPT):\n",
    "        \"\"\"\n",
    "        Prepares and loads the `langchain.chains.RetrievalQA` object\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *prompt_template*: A string representing the prompt with variables \"context\" and \"question\"\n",
    "        \"\"\"\n",
    "        if self.qa is None:\n",
    "            db = self.load_vectordb()\n",
    "            retriever = db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\",\n",
    "                search_kwargs={\n",
    "                    \"k\": self.rag_num_source_docs,\n",
    "                    \"score_threshold\": self.rag_score_threshold,\n",
    "                },\n",
    "            )\n",
    "            llm = self.load_llm()\n",
    "            PROMPT = PromptTemplate(\n",
    "                template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "            self.qa = RetrievalQA.from_chain_type(\n",
    "                llm=llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=retriever,\n",
    "                return_source_documents=True,\n",
    "                chain_type_kwargs={\"prompt\": PROMPT},\n",
    "            )\n",
    "        return self.qa\n",
    "\n",
    "    def load_chatqa(self):\n",
    "        \"\"\"\n",
    "        Prepares and loads a `langchain.chains.ConversationalRetrievalChain` instance\n",
    "        \"\"\"\n",
    "        if self.chatqa is None:\n",
    "            db = self.load_vectordb()\n",
    "            retriever = db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\",  # see note in constructor\n",
    "                search_kwargs={\n",
    "                    \"k\": self.rag_num_source_docs,\n",
    "                    \"score_threshold\": self.rag_score_threshold,\n",
    "                },\n",
    "            )\n",
    "            llm = self.load_llm()\n",
    "            memory = AnswerConversationBufferMemory(\n",
    "                memory_key=\"chat_history\", return_messages=True\n",
    "            )\n",
    "            self.chatqa = ConversationalRetrievalChain.from_llm(\n",
    "                llm, retriever, memory=memory, return_source_documents=True\n",
    "            )\n",
    "        return self.chatqa\n",
    "\n",
    "    def ask(self, question: str, qa_template=DEFAULT_QA_PROMPT, prompt_template=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Answer a question based on source documents fed to the `ingest` method.\n",
    "        Extra keyword arguments are sent directly to the model invocation.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *question*: a question you want to ask\n",
    "        - *qa_template*: A string representing the prompt with variables \"context\" and \"question\"\n",
    "        - *prompt_template*: the model-specific template in which everything (including QA template) should be wrapped.\n",
    "                            Should have a single variable \"{prompt}\". Overrides the `prompt_template` parameter supplied to \n",
    "                            `LLM` constructor.\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        - A dictionary with keys: `answer`, `source_documents`, `question`\n",
    "        \"\"\"\n",
    "        prompt_template = self.prompt_template if prompt_template is None else prompt_template\n",
    "        prompt_template = qa_template if prompt_template is None else prompt_template.format(**{'prompt': qa_template})\n",
    "        qa = self.load_qa(prompt_template=prompt_template)\n",
    "        res = qa.invoke(question, **kwargs)\n",
    "        res[\"question\"] = res[\"query\"]\n",
    "        del res[\"query\"]\n",
    "        res[\"answer\"] = res[\"result\"]\n",
    "        del res[\"result\"]\n",
    "        return res\n",
    "\n",
    "    def chat(self, question: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Chat with documents fed to the `ingest` method.\n",
    "        Unlike `LLM.ask`, `LLM.chat` includes conversational memory.\n",
    "        Extra keyword arguments are sent directly to the model invocation.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *question*: a question you want to ask\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        - A dictionary with keys: `answer`, `source_documents`, `question`, `chat_history`\n",
    "        \"\"\"\n",
    "        chatqa = self.load_chatqa()\n",
    "        res = chatqa.invoke(question, **kwargs)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.download_model\n",
       "\n",
       ">      LLM.download_model\n",
       ">                          (model_url:str='https://huggingface.co/TheBloke/Wizar\n",
       ">                          d-Vicuna-7B-Uncensored-GGUF/resolve/main/Wizard-\n",
       ">                          Vicuna-7B-Uncensored.Q4_K_M.gguf',\n",
       ">                          model_download_path:Optional[str]=None,\n",
       ">                          confirm:bool=True, ssl_verify:bool=True)\n",
       "\n",
       "Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *model_url*: URL of model\n",
       "- *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
       "- *confirm*: whether or not to confirm with user before downloading\n",
       "- *ssl_verify*: If True, SSL certificates are verified. \n",
       "                You can set to False if corporate firewall gives you problems."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.download_model\n",
       "\n",
       ">      LLM.download_model\n",
       ">                          (model_url:str='https://huggingface.co/TheBloke/Wizar\n",
       ">                          d-Vicuna-7B-Uncensored-GGUF/resolve/main/Wizard-\n",
       ">                          Vicuna-7B-Uncensored.Q4_K_M.gguf',\n",
       ">                          model_download_path:Optional[str]=None,\n",
       ">                          confirm:bool=True, ssl_verify:bool=True)\n",
       "\n",
       "Download an LLM in GGML format supported by [lLama.cpp](https://github.com/ggerganov/llama.cpp).\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *model_url*: URL of model\n",
       "- *model_download_path*: Path to download model. Default is `onprem_data` in user's home directory.\n",
       "- *confirm*: whether or not to confirm with user before downloading\n",
       "- *ssl_verify*: If True, SSL certificates are verified. \n",
       "                You can set to False if corporate firewall gives you problems."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.download_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L202){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_llm\n",
       "\n",
       ">      LLM.load_llm ()\n",
       "\n",
       "Loads the LLM from the model path."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L202){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_llm\n",
       "\n",
       ">      LLM.load_llm ()\n",
       "\n",
       "Loads the LLM from the model path."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L143){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_ingester\n",
       "\n",
       ">      LLM.load_ingester ()\n",
       "\n",
       "Get `Ingester` instance. \n",
       "You can access the `langchain.vectorstores.Chroma` instance with `load_ingester().get_db()`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L143){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_ingester\n",
       "\n",
       ">      LLM.load_ingester ()\n",
       "\n",
       "Get `Ingester` instance. \n",
       "You can access the `langchain.vectorstores.Chroma` instance with `load_ingester().get_db()`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_ingester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_qa\n",
       "\n",
       ">      LLM.load_qa (prompt_template:str='\"Use the following pieces of context\n",
       ">                   delimited by three backticks to answer the question at the\n",
       ">                   end. If you don\\'t know the answer, just say that you don\\'t\n",
       ">                   know, don\\'t try to make up an\n",
       ">                   answer.\\n\\n```{context}```\\n\\nQuestion: {question}\\nHelpful\n",
       ">                   Answer:')\n",
       "\n",
       "Prepares and loads the `langchain.chains.RetrievalQA` object\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *prompt_template*: A string representing the prompt with variables \"context\" and \"question\""
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_qa\n",
       "\n",
       ">      LLM.load_qa (prompt_template:str='\"Use the following pieces of context\n",
       ">                   delimited by three backticks to answer the question at the\n",
       ">                   end. If you don\\'t know the answer, just say that you don\\'t\n",
       ">                   know, don\\'t try to make up an\n",
       ">                   answer.\\n\\n```{context}```\\n\\nQuestion: {question}\\nHelpful\n",
       ">                   Answer:')\n",
       "\n",
       "Prepares and loads the `langchain.chains.RetrievalQA` object\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *prompt_template*: A string representing the prompt with variables \"context\" and \"question\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L258){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_chatqa\n",
       "\n",
       ">      LLM.load_chatqa ()\n",
       "\n",
       "Prepares and loads a `langchain.chains.ConversationalRetrievalChain` instance"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L258){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.load_chatqa\n",
       "\n",
       ">      LLM.load_chatqa ()\n",
       "\n",
       "Prepares and loads a `langchain.chains.ConversationalRetrievalChain` instance"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.load_chatqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L220){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.prompt\n",
       "\n",
       ">      LLM.prompt (prompt, prompt_template:Optional[str]=None)\n",
       "\n",
       "Send prompt to LLM to generate a response\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *prompt*: The prompt to supply to the model\n",
       "- *prompt_template*: Optional prompt template (must have a variable named \"prompt\")"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L220){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.prompt\n",
       "\n",
       ">      LLM.prompt (prompt, prompt_template:Optional[str]=None)\n",
       "\n",
       "Send prompt to LLM to generate a response\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *prompt*: The prompt to supply to the model\n",
       "- *prompt_template*: Optional prompt template (must have a variable named \"prompt\")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ingest\n",
       "\n",
       ">      LLM.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                  chunk_overlap:int=50)\n",
       "\n",
       "Ingests all documents in `source_folder` into vector database.\n",
       "Previously-ingested documents are ignored.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *source_directory*: path to folder containing document store\n",
       "- *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "- *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "\n",
       "**Returns:** `None`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ingest\n",
       "\n",
       ">      LLM.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                  chunk_overlap:int=50)\n",
       "\n",
       "Ingests all documents in `source_folder` into vector database.\n",
       "Previously-ingested documents are ignored.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *source_directory*: path to folder containing document store\n",
       "- *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "- *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "\n",
       "**Returns:** `None`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L277){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ask\n",
       "\n",
       ">      LLM.ask (question:str, prompt_template='\"Use the following pieces of\n",
       ">               context delimited by three backticks to answer the question at\n",
       ">               the end. If you don\\'t know the answer, just say that you don\\'t\n",
       ">               know, don\\'t try to make up an\n",
       ">               answer.\\n\\n```{context}```\\n\\nQuestion: {question}\\nHelpful\n",
       ">               Answer:')\n",
       "\n",
       "Answer a question based on source documents fed to the `ingest` method.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *question*: a question you want to ask\n",
       "- *prompt_template*: A string representing the prompt with variables \"context\" and \"question\"\n",
       "\n",
       "**Returns:**\n",
       "\n",
       "- A dictionary with keys: `answer`, `source_documents`, `question`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L277){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.ask\n",
       "\n",
       ">      LLM.ask (question:str, prompt_template='\"Use the following pieces of\n",
       ">               context delimited by three backticks to answer the question at\n",
       ">               the end. If you don\\'t know the answer, just say that you don\\'t\n",
       ">               know, don\\'t try to make up an\n",
       ">               answer.\\n\\n```{context}```\\n\\nQuestion: {question}\\nHelpful\n",
       ">               Answer:')\n",
       "\n",
       "Answer a question based on source documents fed to the `ingest` method.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *question*: a question you want to ask\n",
       "- *prompt_template*: A string representing the prompt with variables \"context\" and \"question\"\n",
       "\n",
       "**Returns:**\n",
       "\n",
       "- A dictionary with keys: `answer`, `source_documents`, `question`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L297){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.chat\n",
       "\n",
       ">      LLM.chat (question:str)\n",
       "\n",
       "Chat with documents fed to the `ingest` method.\n",
       "Unlike `LLM.ask`, `LLM.chat` includes conversational memory.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *question*: a question you want to ask\n",
       "\n",
       "**Returns:**\n",
       "\n",
       "- A dictionary with keys: `answer`, `source_documents`, `question`, `chat_history`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/core.py#L297){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LLM.chat\n",
       "\n",
       ">      LLM.chat (question:str)\n",
       "\n",
       "Chat with documents fed to the `ingest` method.\n",
       "Unlike `LLM.ask`, `LLM.chat` includes conversational memory.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *question*: a question you want to ask\n",
       "\n",
       "**Returns:**\n",
       "\n",
       "- A dictionary with keys: `answer`, `source_documents`, `question`, `chat_history`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LLM.chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "We'll use a small 3B-parameter model here for testing purposes. The vector database is stored under `~/onprem_data` by default. In this example, we will store the vector store in temporary folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "vectordb_path = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "url = \"https://huggingface.co/TheBloke/orca_mini_3B-GGML/resolve/main/orca-mini-3b.ggmlv3.q4_1.bin\"\n",
    "llm = LLM(model_url=url, vectordb_path=vectordb_path, confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "assert os.path.isfile(\n",
    "    os.path.join(U.get_datadir(), os.path.basename(url))\n",
    "), \"missing model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence: James Gandolfini and Paul Newman were great actors.\n",
    "People:\n",
    "James Gandolfini, Paul Newman\n",
    "Sentence:\n",
    "I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "People:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA TITAN V, compute capability 7.0\n",
      "  Device 1: NVIDIA TITAN V, compute capability 7.0\n",
      "llama.cpp: loading model from /home/amaiya/onprem_data/orca-mini-3b.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.06 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA TITAN V) as main device\n",
      "llama_model_load_internal: mem required  = 3066.94 MB (+  682.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/29 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 384 MB\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "assert saved_output.strip() == \"Cillian Murphy, Florence Pugh\", \"bad response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /tmp/tmpl6ww9w5p\n",
      "Loading documents from ./sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|| 3/3 [00:00<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 new documents from ./sample_data\n",
      "Split into 153 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the LLM.ask method\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "llm.ingest(\"./sample_data\", chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ktrain is a low-code library for augmented machine learning that enables beginners and domain experts with minimal programming or data science expertise to further democratize machine learning by facilitating the full machine learning workow from curating and preprocessing inputs (i.e., ground-truth-labeled training data) to training, tuning, troubleshooting, and applying models.\n",
      "\n",
      "References:\n",
      "\n",
      "\n",
      "\n",
      "1.> ./sample_data/1/ktrain_paper.pdf:\n",
      "lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\n",
      "tomation and instead focuses on either partially or fully automating other aspects of the\n",
      "machine learning (ML) workow. For these reasons, ktrain is less of a traditional Au-\n",
      "2\n",
      "\n",
      "2.> ./sample_data/1/ktrain_paper.pdf:\n",
      "possible, ktrain automates (either algorithmically or through setting well-performing de-\n",
      "faults), but also allows users to make choices that best t their unique application require-\n",
      "ments. In this way, ktrain uses automation to augment and complement human engineers\n",
      "rather than attempting to entirely replace them. In doing so, the strengths of both are\n",
      "better exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai\n",
      "\n",
      "3.> ./sample_data/1/ktrain_paper.pdf:\n",
      "with custom models and data formats, as well.\n",
      "Inspired by other low-code (and no-\n",
      "code) open-source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig\n",
      "(Molino et al., 2019), ktrain is intended to help further democratize machine learning by\n",
      "enabling beginners and domain experts with minimal programming or data science experi-\n",
      "4. http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
      "6\n",
      "\n",
      "4.> ./sample_data/1/ktrain_paper.pdf:\n",
      "ktrain: A Low-Code Library for Augmented Machine Learning\n",
      "toML platform and more of what might be called a low-code ML platform. Through\n",
      "automation or semi-automation, ktrain facilitates the full machine learning workow from\n",
      "curating and preprocessing inputs (i.e., ground-truth-labeled training data) to training,\n",
      "tuning, troubleshooting, and applying models. In this way, ktrain is well-suited for domain\n",
      "experts who may have less experience with machine learning and software coding. Where\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "question = \"\"\"What is ktrain?\"\"\"\n",
    "result = llm.ask(question)\n",
    "print(\"\\n\\nReferences:\\n\\n\")\n",
    "for i, document in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\n{i+1}.> \" + document.metadata[\"source\"] + \":\")\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro-Tip**: Smaller models like this tend to hallucinate more easily than larger ones. If you see the model hallucinating answers, you can supply `use_larger=True` to `LLM` and use the slightly larger default model better-suited to this use case (or supply the URL to a different model of your choosing to `LLM`), which can provide better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LLM.chat` method answers questions with consideration of conversational memory. Note that `LLM.chat` is better suited to larger/better models than the one used below, as the models are required to do more work (e.g., condensing the question and chat history into a standalone question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ktrain is a low-code library for augmented machine learning that allows users to automate or semi-automate various aspects of the machine learning workow, such as curating and preprocessing inputs, training, tuning, troubleshooting, and applying models. It is designed to improve the strengths of both humans and machines by enabling beginners and domain experts with minimal programming or data science expertise to use machine learning in their applications."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "question = \"What is ktrain?\"\n",
    "result = llm.chat(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is ktrain and how can it be used for image classification? ktrain is a low-code library for augmented machine learning that enables the full machine learning workow, including automation or semi-automation of tasks such as data curation, preprocessing, model training, tuning, troubleshooting, and application. It can be used with any machine learning model implemented in TensorFlow Keras (tf.keras). ktrain includes out-of-the-box support for various data types and tasks, including image classification using custom models and data formats, as well."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "question = \"Can it be used for image classification?\"\n",
    "result = llm.chat(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
