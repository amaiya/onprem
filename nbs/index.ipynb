{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OnPrem.LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A tool for running large language models on-premises using non-public data\n",
    "\n",
    "**OnPrem.LLM** is a simple Python package that makes it easier to run large language models (LLMs) on your own machines using non-public data (possibly behind corporate firewalls). Inspired by the [privateGPT](https://github.com/imartinez/privateGPT) GitHub repo and Simon Willison's [LLM](https://pypi.org/project/llm/) command-line utility, **OnPrem.LLM** is intended to help integrate local LLMs into practical applications.\n",
    "\n",
    "The full documentation is [here](https://amaiya.github.io/onprem/). \n",
    "\n",
    "A Google Colab demo of installing and using **OnPrem.LLM** is [here](https://colab.research.google.com/drive/1LVeacsQ9dmE1BVzwR3eTLukpeRIMmUqi?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have [installed PyTorch](https://pytorch.org/get-started/locally/), you can install **OnPrem.LLM** with:\n",
    "\n",
    "```sh\n",
    "pip install onprem\n",
    "```\n",
    "\n",
    "For fast GPU-accelerated inference, see [additional instructions below](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "from onprem import LLM\n",
    "\n",
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a 7B-parameter model is downloaded and used.  If `use_larger=True`, a 13B-parameter is used.  You can also supply the URL to an LLM of your choosing to `LLM` (see the [code generation section below](https://amaiya.github.io/onprem/#text-to-code-generation) for an example). Currently, only models in GGML format are supported.  Future versions of **OnPrem.LLM** will transition to the newer GGUF format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prompts to the LLM to Solve Problems\n",
    "This is an example of few-shot prompting, where we provide an example of what we want the LLM to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence: James Gandolfini and Paul Newman were great actors.\n",
    "People:\n",
    "James Gandolfini, Paul Newman\n",
    "Sentence:\n",
    "I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "People:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional prompt examples are [shown here](https://amaiya.github.io/onprem/examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk to Your Documents\n",
    "\n",
    "Answers are generated from the content of your documents (i.e., [retrieval augmented generation](https://arxiv.org/abs/2005.11401) or RAG). Here, we will supply `use_larger=True` to use the larger default model better suited to this use case in addition to using [GPU offloading](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu) to speed up answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "from onprem import LLM\n",
    "\n",
    "llm = LLM(use_larger=True, n_gpu_layers=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Ingest the  Documents into a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from ./sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 3/3 [00:00<00:00, 25.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 new documents from ./sample_data\n",
      "Split into 153 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the LLM.ask method\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "llm.ingest('./sample_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Answer Questions About the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ktrain is a low-code platform designed to facilitate the full machine learning workflow, from preprocessing inputs to training, tuning, troubleshooting, and applying models. It focuses on automating other aspects of the ML workflow in order to augment and complement human engineers rather than replacing them. Inspired by fastai and ludwig, ktrain is intended to democratize machine learning for beginners and domain experts with minimal programming or data science experience."
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "question = \"\"\"What is  ktrain?\"\"\" \n",
    "answer, docs = llm.ask(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sources used by the model to generate the answer are stored in `docs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sources:\n",
      "\n",
      "\n",
      "1.> ./sample_data/ktrain_paper.pdf:\n",
      "lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\n",
      "tomation and instead focuses on either partially or fully automating other aspects of the\n",
      "machine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-\n",
      "2\n",
      "\n",
      "2.> ./sample_data/ktrain_paper.pdf:\n",
      "possible, ktrain automates (either algorithmically or through setting well-performing de-\n",
      "faults), but also allows users to make choices that best ﬁt their unique application require-\n",
      "ments. In this way, ktrain uses automation to augment and complement human engineers\n",
      "rather than attempting to entirely replace them. In doing so, the strengths of both are\n",
      "better exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai\n",
      "\n",
      "3.> ./sample_data/ktrain_paper.pdf:\n",
      "with custom models and data formats, as well.\n",
      "Inspired by other low-code (and no-\n",
      "code) open-source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig\n",
      "(Molino et al., 2019), ktrain is intended to help further democratize machine learning by\n",
      "enabling beginners and domain experts with minimal programming or data science experi-\n",
      "4. http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
      "6\n",
      "\n",
      "4.> ./sample_data/ktrain_paper.pdf:\n",
      "ktrain: A Low-Code Library for Augmented Machine Learning\n",
      "toML platform and more of what might be called a “low-code” ML platform. Through\n",
      "automation or semi-automation, ktrain facilitates the full machine learning workﬂow from\n",
      "curating and preprocessing inputs (i.e., ground-truth-labeled training data) to training,\n",
      "tuning, troubleshooting, and applying models. In this way, ktrain is well-suited for domain\n",
      "experts who may have less experience with machine learning and software coding. Where\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "print('\\nSources:\\n')\n",
    "for i, document in enumerate(docs):\n",
    "    print(f\"\\n{i+1}.> \" + document.metadata[\"source\"] + \":\")\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Code Generation\n",
    "We'll use the CodeUp LLM by supplying the URL and employing the particular prompt format this model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "from onprem import LLM\n",
    "url = 'https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML/resolve/main/codeup-llama-2-13b-chat-hf.ggmlv3.q4_1.bin'\n",
    "llm = LLM(url, n_gpu_layers=43) # see below for GPU information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the prompt based on what [this model expects](https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML#prompt-template-alpaca) (this is important):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "template = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{prompt}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is an example of Python code that can be used to validate an email address:\n",
      "```\n",
      "import re\n",
      "\n",
      "def validate_email(email):\n",
      "    # Use a regular expression to check if the email address is in the correct format\n",
      "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
      "    if re.match(pattern, email):\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "# Test the validate_email function with different inputs\n",
      "print(\"Email address is valid:\", validate_email(\"example@example.com\"))  # Should print \"True\"\n",
      "print(\"Email address is invalid:\", validate_email(\"example@\"))  # Should print \"False\"\n",
      "print(\"Email address is invalid:\", validate_email(\"example.com\"))  # Should print \"False\"\n",
      "```\n",
      "The code defines a function `validate_email` that takes an email address as input and uses a regular expression to check if the email address is in the correct format. The regular expression checks for an email address that consists of one or more letters, numbers, periods, hyphens, or underscores followed by the `@` symbol, followed by one or more letters, periods, hyphens, or underscores followed by a `.` and two to three letters.\n",
      "The function returns `True` if the email address is valid, and `False` otherwise. The code also includes some test examples to demonstrate how to use the function."
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "answer = llm.prompt('Write Python code to validate an email address.', prompt_template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the code generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "import re\n",
    "def validate_email(email):\n",
    "    # Use a regular expression to check if the email address is in the correct format\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    if re.match(pattern, email):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "print(validate_email('sam@@openai.com')) # bad email address\n",
    "print(validate_email('sam@openai'))      # bad email address\n",
    "print(validate_email('sam@openai.com'))  # good email address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated code may sometimes need editing, but this one worked out-of-the-box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-In Web App\n",
    "\n",
    "**OnPrem.LLM** includes a built-in Web app to access the LLM. To start it, run the following command after installation:\n",
    "\n",
    "```shell\n",
    "onprem --port 8000\n",
    "```\n",
    "Then, enter `localhost:8000` (or `<domain_name>:8000` if running on remote server) in a Web browser to access the application:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/onprem_screenshot.png\" border=\"1\" alt=\"screenshot\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, [see the corresponding documentation](https://amaiya.github.io/onprem/webapp.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding Up Inference Using a GPU\n",
    "\n",
    "The above example employed the use of a CPU.  \n",
    "If you have a GPU (even an older one with less VRAM), you can speed up responses.\n",
    "\n",
    "#### Step 1: Install `llama-cpp-python` with CUBLAS support\n",
    "```shell\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.1.69 --no-cache-dir\n",
    "```\n",
    "It is important to use the specific version shown above due to library incompatibilities.\n",
    "\n",
    "#### Step 2: Use the `n_gpu_layers` argument with `LLM`\n",
    "\n",
    "```python\n",
    "llm = LLM(n_gpu_layers=35)\n",
    "```\n",
    "\n",
    "The value for `n_gpu_layers` depends on your GPU memory and the model you're using (e.g., max of 35 for default 7B model).  You can reduce the value if you get an error (e.g., `CUDA OOM`) or you observe the model stalling in the middle of a response.\n",
    "\n",
    "\n",
    "With the steps above, calls to methods like `llm.prompt` will offload computation to your GPU and speed up responses from the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "1. **How do I use other models with OnPrem.LLM?**\n",
    "\n",
    "    >You can supply the URL to other models to the `LLM` constructor, as we did above in the code generation example.\n",
    "\n",
    "    >We currently support models in GGML format (e.g., `ggmlv3` in the model file name on [huggingface.co](https://huggingface.co/models)). However, the GGML format has now been superseded by GGUF. As of August 21st 2023, llama.cpp no longer supports GGML models, which is why we are pinning to an older version of all dependencies.\n",
    "    >\n",
    "    >Future versions of **OnPrem.LLM** will use the newer GGUF format.\n",
    "\n",
    "2. **I'm behind a corporate firewall and am receiving an SSL error when trying to download the model?**\n",
    "\n",
    "    >Try this:\n",
    "    >```python\n",
    "    >from onprem import LLM\n",
    "    >LLM.download_model(url, ssl_verify=False)\n",
    "    >```\n",
    "\n",
    "3. **How do I use this on a machine with no internet access?**\n",
    "\n",
    "    >Use the `LLM.download_model` method to download the model files to `<your_home_directory>/onprem_data` and transfer them to the same location on the air-gapped  machine.\n",
    "\n",
    "    >For the `ingest` and `ask` methods, you will need to also download and transfer the embedding model files:\n",
    "    >```python\n",
    "    >from sentence_transformers import SentenceTransformer\n",
    "    >model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    >model.save('/some/folder')\n",
    "    >```\n",
    "    \n",
    "    >Copy the `some/folder` folder to the air-gapped machine and supply the path to `LLM` via the `embedding_model_name` parameter.\n",
    " \n",
    " \n",
    "\n",
    "4. **When installing `onprem`, I'm getting errors related to `llama-cpp-python` on Windows/Mac/Linux?** \n",
    "\n",
    "    > For **Linux** systems like Ubuntu, try this: `sudo apt-get install build-essential g++ clang`.  Other tips are [here](https://github.com/oobabooga/text-generation-webui/issues/1534).\n",
    "    \n",
    "    > For **Windows** systems, either use [Windows Subsystem for Linux (WSL)](https://learn.microsoft.com/en-us/windows/wsl/install) or install [Microsoft Visual Studio build tools](https://visualstudio.microsoft.com/vs/older-downloads/) and ensure the selections shown in [this post](https://github.com/imartinez/privateGPT/issues/445#issuecomment-1561343405) are installed. WSL is recommended.\n",
    "    \n",
    "    > For **Macs**, try following [these tips](https://github.com/imartinez/privateGPT/issues/445#issuecomment-1563333950).\n",
    "    \n",
    "    > If you still have problems, there are various other tips for each of the above OSes in [this privateGPT repo thread](https://github.com/imartinez/privateGPT/issues/445).  Of course, you can also [easily use](https://colab.research.google.com/drive/1LVeacsQ9dmE1BVzwR3eTLukpeRIMmUqi?usp=sharing) **OnPrem.LLM** on Google Colab.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
