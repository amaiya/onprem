{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OnPrem.LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A tool for running large language models on-premises using non-public data\n",
    "\n",
    "**[OnPrem.LLM](https://github.com/amaiya/onprem)** is a simple Python package that makes it easier to run large language models (LLMs) on your own machines using non-public data (possibly behind corporate firewalls). Inspired largely by the [privateGPT](https://github.com/imartinez/privateGPT) GitHub repo, **OnPrem.LLM** is intended to help integrate local LLMs into practical applications.\n",
    "\n",
    "The full documentation is [here](https://amaiya.github.io/onprem/). \n",
    "\n",
    "A Google Colab demo of installing and using **OnPrem.LLM** is [here](https://colab.research.google.com/drive/1LVeacsQ9dmE1BVzwR3eTLukpeRIMmUqi?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Latest News* ðŸ”¥\n",
    "\n",
    "\n",
    "- [2024/10] v0.3.0 released and now includes support for [concept-focused summarization](https://amaiya.github.io/onprem/examples_summarization.html#concept-focused-summarization)\n",
    "- [2024/09] v0.2.0 released and now includes PDF OCR support and better PDF table handling.\n",
    "- [2024/06] v0.1.0 of **OnPrem.LLM** has been released. Lots of new updates!\n",
    "  \n",
    "  - [Ability to use with any OpenAI-compatible API](https://amaiya.github.io/onprem/#connecting-to-llms-served-through-rest-apis) (e.g., vLLM, Ollama, OpenLLM, etc.).\n",
    "  - Pipeline for [information extraction](https://amaiya.github.io/onprem/examples_information_extraction.html) from raw documents.\n",
    "  - Pipeline for [few-shot text classification](https://amaiya.github.io/onprem/examples_classification.html) (i.e., training a classifier on a tiny number of labeled examples) along with the ability to explain few-shot predictions.\n",
    "  - Default model changed to [Mistral-7B-Instruct-v0.2](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)\n",
    "  - [API augmentations and bug fixes](https://github.com/amaiya/onprem/blob/master/CHANGELOG.md)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have [installed PyTorch](https://pytorch.org/get-started/locally/), you can install **OnPrem.LLM** with the following steps:\n",
    "\n",
    "1. Install **llama-cpp-python** by [visiting this site](https://python.langchain.com/docs/integrations/llms/llamacpp#installation) and following instructions for your operating system and machine. For CPU-based installations (i.e., no GPU acceleration), you can simply do: `pip install llama-cpp-python`.\n",
    "\n",
    "2. Install **OnPrem.LLM**: `pip install onprem`\n",
    "\n",
    "For fast GPU-accelerated inference, see [additional instructions below](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu). See [the FAQ](https://amaiya.github.io/onprem/#faq), if you experience issues with [llama-cpp-python](https://pypi.org/project/llama-cpp-python/) installation.\n",
    "\n",
    "**Note:** If using **OnPrem.LLM** with an LLM being served through an [external REST API](https://amaiya.github.io/onprem/#Connecting-to-LLMs-Served-Through-REST-APIs) (e.g., vLLM, OpenLLM, Ollama), installation of `llama-cpp-python` is optional. You will only be asked to install it if attempting to use a locally installed model directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "\n",
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a 7B-parameter model (**Mistral-7B-Instruct-v0.2**) is downloaded and used. If `default_model='llama'` is supplied, then a **Llama-3.1-8B-Instsruct** model is  automatically downloaded and used (which is useful if the default Mistral model struggles with a particular task):\n",
    "```python\n",
    "# Llama 3.1 is downloaded here and the correct prompt template for Llama-3.1 is automatically configured and used\n",
    "llm = LLM(default_model='llama')\n",
    "```\n",
    "Similarly, suppyling `default_model='zephyr`, will use **Zephyr-7B-beta**. Of course, you can also easily supply the URL to an LLM of your choosing to `LLM` (see the [code generation section below](https://amaiya.github.io/onprem/#text-to-code-generation) for an example or the [FAQ](https://amaiya.github.io/onprem/#faq)). Any extra parameters supplied to `LLM` are forwarded directly to `llama-cpp-python`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prompts to the LLM to Solve Problems\n",
    "This is an example of few-shot prompting, where we provide an example of what we want the LLM to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cillian Murphy, Florence Pugh."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence: James Gandolfini and Paul Newman were great actors.\n",
    "People:\n",
    "James Gandolfini, Paul Newman\n",
    "Sentence:\n",
    "I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "People:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional prompt examples are [shown here](https://amaiya.github.io/onprem/examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk to Your Documents\n",
    "\n",
    "Answers are generated from the content of your documents (i.e., [retrieval augmented generation](https://arxiv.org/abs/2005.11401) or RAG). Here, we will use [GPU offloading](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu) to speed up answer generation using the default model. However, the Zephyr-7B model may perform even better, responds faster, and is used in our [example notebook](https://amaiya.github.io/onprem/examples_rag.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "from onprem import LLM\n",
    "\n",
    "llm = LLM(n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Ingest the  Documents into a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from ./sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 new documents from ./sample_data\n",
      "Split into 153 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "llm.ingest(\"./sample_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Answer Questions About the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ktrain is a low-code machine learning library designed to facilitate the full machine learning workflow from curating and preprocessing inputs to training, tuning, troubleshooting, and applying models. Ktrain is well-suited for domain experts who may have less experience with machine learning and software coding."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "question = \"\"\"What is  ktrain?\"\"\"\n",
    "result = llm.ask(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sources used by the model to generate the answer are stored in `result['source_documents']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sources:\n",
      "\n",
      "\n",
      "1.> /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\n",
      "lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-\n",
      "tomation and instead focuses on either partially or fully automating other aspects of the\n",
      "machine learning (ML) workï¬‚ow. For these reasons, ktrain is less of a traditional Au-\n",
      "2\n",
      "\n",
      "2.> /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\n",
      "possible, ktrain automates (either algorithmically or through setting well-performing de-\n",
      "faults), but also allows users to make choices that best ï¬t their unique application require-\n",
      "ments. In this way, ktrain uses automation to augment and complement human engineers\n",
      "rather than attempting to entirely replace them. In doing so, the strengths of both are\n",
      "better exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai\n",
      "\n",
      "3.> /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\n",
      "with custom models and data formats, as well.\n",
      "Inspired by other low-code (and no-\n",
      "code) open-source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig\n",
      "(Molino et al., 2019), ktrain is intended to help further democratize machine learning by\n",
      "enabling beginners and domain experts with minimal programming or data science experi-\n",
      "4. http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups\n",
      "6\n",
      "\n",
      "4.> /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\n",
      "ktrain: A Low-Code Library for Augmented Machine Learning\n",
      "toML platform and more of what might be called a â€œlow-codeâ€ ML platform. Through\n",
      "automation or semi-automation, ktrain facilitates the full machine learning workï¬‚ow from\n",
      "curating and preprocessing inputs (i.e., ground-truth-labeled training data) to training,\n",
      "tuning, troubleshooting, and applying models. In this way, ktrain is well-suited for domain\n",
      "experts who may have less experience with machine learning and software coding. Where\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "print(\"\\nSources:\\n\")\n",
    "for i, document in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\n{i+1}.> \" + document.metadata[\"source\"] + \":\")\n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization Pipeline\n",
    "\n",
    "Summarize your raw documents (e.g., PDFs, MS Word) with an LLM.\n",
    "\n",
    "#### Map-Reduce Summarization\n",
    "Summarize each chunk in a document and then generate a single summary from the individual summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "llm = LLM(n_gpu_layers=-1, verbose=False, mute_stream=True) # disabling viewing of intermediate summarization prompts/inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ktrain is an open-source machine learning library that offers a unified interface for various machine learning tasks. The library supports both supervised and non-supervised machine learning, and includes methods for training models, evaluating models, making predictions on new data, and providing explanations for model decisions. Additionally, the library integrates with various explainable AI libraries such as shap, eli5 with lime, and others to provide more interpretable models.\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem.pipelines import Summarizer\n",
    "summ = Summarizer(llm)\n",
    "\n",
    "resp = summ.summarize('sample_data/1/ktrain_paper.pdf', max_chunks_to_use=5) # omit max_chunks_to_use parameter to consider entire document\n",
    "print(resp['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept-Focused Summarization\n",
    "\n",
    "Summarize a large document with respect to a particular concept of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "from onprem.pipelines import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The context provided describes the implementation of an open-domain question-answering system using ktrain, a low-code library for augmented machine learning. The system follows three main steps: indexing documents into a search engine, locating documents containing words in the question, and extracting candidate answers from those documents using a BERT model pretrained on the SQuAD dataset. Confidence scores are used to sort and prune candidate answers before returning results. The entire workflow can be implemented with only three lines of code using ktrain's SimpleQA module. This system allows for the submission of natural language questions and receives exact answers, as demonstrated in the provided example. Overall, the context highlights the ease and accessibility of building sophisticated machine learning models, including open-domain question-answering systems, through ktrain's low-code interface."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "llm = LLM(default_model='zephyr', n_gpu_layers=-1, verbose=False, temperature=0)\n",
    "summ = Summarizer(llm)\n",
    "summary, sources = summ.summarize_by_concept('sample_data/1/ktrain_paper.pdf', concept_description=\"question answering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction Pipeline\n",
    "\n",
    "Extract information from raw documents (e.g., PDFs, MS Word documents) with an LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaiya/projects/ghub/onprem/onprem/core.py:159: UserWarning: The model you supplied is gpt-3.5-turbo, an external service (i.e., not on-premises). Use with caution, as your data and prompts will be sent externally.\n",
      "  warnings.warn(f'The model you supplied is {self.model_name}, an external service (i.e., not on-premises). '+\\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Institute for Defense Analyses'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "from onprem import LLM\n",
    "from onprem.pipelines import Extractor\n",
    "# Notice that we're using a cloud-based, off-premises model here! See \"OpenAI\" section below.\n",
    "llm = LLM(model_url='openai://gpt-3.5-turbo', verbose=False, mute_stream=True, temperature=0) \n",
    "extractor = Extractor(llm)\n",
    "prompt = \"\"\"Extract the names of research institutions (e.g., universities, research labs, corporations, etc.) \n",
    "from the following sentence delimited by three backticks. If there are no organizations, return NA.  \n",
    "If there are multiple organizations, separate them with commas.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "df = extractor.apply(prompt, fpath='sample_data/1/ktrain_paper.pdf', pdf_pages=[1], stop=['\\n'])\n",
    "df.loc[df['Extractions'] != 'NA'].Extractions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Classification\n",
    "\n",
    "Make accurate text classification predictions using only a tiny number of labeled examples.\n",
    "\n",
    "```python\n",
    "# create classifier\n",
    "from onprem.pipelines import FewShotClassifier\n",
    "clf = FewShotClassifier(use_smaller=True)\n",
    "\n",
    "# Fetching data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "classes = [\"soc.religion.christian\", \"sci.space\"]\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\", categories=classes)\n",
    "corpus, group_labels = np.array(newsgroups.data), np.array(newsgroups.target_names)[newsgroups.target]\n",
    "\n",
    "# Wrangling data into a dataframe and selecting training examples\n",
    "data = pd.DataFrame({\"text\": corpus, \"label\": group_labels})\n",
    "train_df = data.groupby(\"label\").sample(5)\n",
    "test_df = data.drop(index=train_df.index)\n",
    "\n",
    "# X_sample only contains 5 examples of each class!\n",
    "X_sample, y_sample = train_df['text'].values, train_df['label'].values\n",
    "\n",
    "# test set\n",
    "X_test, y_test = test_df['text'].values, test_df['label'].values\n",
    "\n",
    "# train\n",
    "clf.train(X_sample,  y_sample, max_steps=20)\n",
    "\n",
    "# evaluate\n",
    "print(clf.evaluate(X_test, y_test)['accuracy'])\n",
    "#output: 0.98\n",
    "\n",
    "# make predictions\n",
    "clf.predict(['Elon Musk likes launching satellites.']).tolist()[0]\n",
    "#output: sci.space\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Code Generation\n",
    "We'll use the CodeUp LLM by supplying the URL and employing the particular prompt format this model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "from onprem import LLM\n",
    "\n",
    "url = \"https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGUF/resolve/main/codeup-llama-2-13b-chat-hf.Q4_K_M.gguf\"\n",
    "llm = LLM(url, n_gpu_layers=-1)  # see below for GPU information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the prompt based on what [this model expects](https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGUF#prompt-template-alpaca) (this is important):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "template = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{prompt}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can supply the `prompte_template` to either the `LLM` constructor (above) or the `LLM.prompt` method. We will do the latter here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is an example of Python code that can be used to validate an email address:\n",
      "```\n",
      "import re\n",
      "\n",
      "def validate_email(email):\n",
      "    # Use a regular expression to check if the email address is in the correct format\n",
      "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
      "    if re.match(pattern, email):\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "# Test the validate_email function with different inputs\n",
      "print(\"Email address is valid:\", validate_email(\"example@example.com\"))  # Should print \"True\"\n",
      "print(\"Email address is invalid:\", validate_email(\"example@\"))  # Should print \"False\"\n",
      "print(\"Email address is invalid:\", validate_email(\"example.com\"))  # Should print \"False\"\n",
      "```\n",
      "The code defines a function `validate_email` that takes an email address as input and uses a regular expression to check if the email address is in the correct format. The regular expression checks for an email address that consists of one or more letters, numbers, periods, hyphens, or underscores followed by the `@` symbol, followed by one or more letters, periods, hyphens, or underscores followed by a `.` and two to three letters.\n",
      "The function returns `True` if the email address is valid, and `False` otherwise. The code also includes some test examples to demonstrate how to use the function."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "answer = llm.prompt(\n",
    "    \"Write Python code to validate an email address.\", prompt_template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the code generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "import re\n",
    "\n",
    "\n",
    "def validate_email(email):\n",
    "    # Use a regular expression to check if the email address is in the correct format\n",
    "    pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "    if re.match(pattern, email):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "print(validate_email(\"sam@@openai.com\"))  # bad email address\n",
    "print(validate_email(\"sam@openai\"))  # bad email address\n",
    "print(validate_email(\"sam@openai.com\"))  # good email address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated code may sometimes need editing, but this one worked out-of-the-box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to LLMs Served Through REST APIs\n",
    "\n",
    "**OnPrem.LLM** can be used with LLMs being served through any OpenAI-compatible REST API.  This means you can easily use **OnPrem.LLM** with tools like [vLLM](https://github.com/vllm-project/vllm), [OpenLLM](https://github.com/bentoml/OpenLLM), [Ollama](https://ollama.com/blog/openai-compatibility), and the [llama.cpp server](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md).\n",
    "\n",
    "For instance, using [vLLM](https://github.com/vllm-project/vllm), you can serve a LLaMA 3 model as follows:\n",
    "```sh\n",
    "python -m vllm.entrypoints.openai.api_server --model NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --api-key token-abc123\n",
    "```\n",
    "You can then connect OnPrem.LLM to the LLM by supplying the URL of the server you just started:\n",
    "```python\n",
    "from onprem import LLM\n",
    "llm = LLM(model_url='http://localhost:8000/v1', api_key='token-abc123') \n",
    "# Note: The API key can either be supplied directly or stored in the OPENAI_API_KEY environment variable.\n",
    "#       If the server does not require an API key, `api_key` should still be supplied with a dummy value like 'na'.\n",
    "```\n",
    "\n",
    "That's it! Solve problems with **OnPrem.LLM** as you normally would (e.g., RAG question-answering, summarization, few-shot prompting, code generation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenAI Models with OnPrem.LLM\n",
    "\n",
    "Even when using on-premises language models, it can sometimes be useful to have easy access to non-local, cloud-based models (e.g., OpenAI) for testing, producing baselines for comparison, and generating synthetic examples for fine-tuning. For these reasons, in spite of the name, **OnPrem.LLM** now includes support for OpenAI chat models:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaiya/projects/ghub/onprem/onprem/core.py:138: UserWarning: The model you supplied is gpt-3.5-turbo, an external service (i.e., not on-premises). Use with caution, as your data and prompts will be sent externally.\n",
      "  warnings.warn(f'The model you supplied is {self.model_name}, an external service (i.e., not on-premises). '+\\\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "llm = LLM(model_url='openai://gpt-3.5-turbo', temperature=0) # ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Whiskers: Whiskers is a cute name for a cat because it perfectly describes one of the most adorable features of a feline - their long, delicate whiskers. It's a playful and endearing name that captures the essence of a cat's charm.\n",
      "\n",
      "2. Pudding: Pudding is an incredibly cute name for a cat because it evokes a sense of softness and sweetness. Just like a bowl of creamy pudding, this name brings to mind a cat's cuddly and lovable nature. It's a name that instantly makes you want to snuggle up with your furry friend.\n",
      "\n",
      "3. Muffin: Muffin is an adorable name for a cat because it conjures up images of something small, round, and irresistibly cute - just like a cat! This name is playful and charming, and it perfectly captures the delightful and lovable nature of our feline companions."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "saved_result = llm.prompt('List three cute  names for a cat and explain why each is cute.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azure OpenAI**\n",
    "\n",
    "For Azure OpenAI models, use the following URL format:\n",
    "```python\n",
    "llm = LLM(model_url='azure://<deployment_name>', ...) \n",
    "# <deployment_name> is the Azure deployment name and additional Azure-specific parameters \n",
    "# can be supplied as extra arguments to LLM (or set as environment variables)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Prompts\n",
    "\n",
    "You can use **OnPrem.LLM** with the [Guidance](https://github.com/guidance-ai/guidance) package to guide the LLM to generate outputs based on your conditions and constraints. We'll show a couple of examples here, but see [our documentation on guided prompts](https://amaiya.github.io/onprem/examples_guided_prompts.html) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "from onprem import LLM\n",
    "\n",
    "llm = LLM(n_gpu_layers=-1, verbose=False)\n",
    "from onprem.guider import Guider\n",
    "guider = Guider(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Guider, you can use use Regular Expressions to control LLM generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '7'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = f\"\"\"Question: Luke has ten balls. He gives three to his brother. How many balls does he have left?\n",
    "Answer: \"\"\" + gen(name='answer', regex='\\d+')\n",
    "\n",
    "guider.prompt(prompt, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>19, 18<span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>7</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>6</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>5</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>4</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>3</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>2</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 1</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>0</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 9</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> 8</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>,</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'output': ' 17, 16, 15, 14, 13, 12, 11, 10, 9, 8,'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = '19, 18,' + gen(name='output', max_tokens=50, stop_regex='[^\\d]7[^\\d]')\n",
    "guider.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the documentation](https://amaiya.github.io/onprem/examples_guided_prompts.html) for more examples of how to use [Guidance](https://github.com/guidance-ai/guidance) with **OnPrem.LLM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-In Web App\n",
    "\n",
    "**OnPrem.LLM** includes a built-in Web app to access the LLM. To start it, run the following command after installation:\n",
    "\n",
    "```shell\n",
    "onprem --port 8000\n",
    "```\n",
    "Then, enter `localhost:8000` (or `<domain_name>:8000` if running on remote server) in a Web browser to access the application:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_screenshot.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, [see the corresponding documentation](https://amaiya.github.io/onprem/webapp.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding Up Inference Using a GPU\n",
    "\n",
    "The above example employed the use of a CPU. If you have a GPU (even an older one with less VRAM), you can speed up responses. See [the llama-cpp-python documentation](https://llama-cpp-python.readthedocs.io/en/latest/#installation) for installing `llama-cpp-python` with GPU support for your system.  \n",
    "\n",
    "The steps below describe installing and using `llama-cpp-python` with `CUDA` support and can be employed for GPU acceleration on systems with NVIDIA GPUs (e.g., Linux, WSL2, Google Colab).\n",
    "\n",
    "#### Step 1: Install `llama-cpp-python` with GPU support\n",
    "```shell\n",
    "CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
    "\n",
    "# For Mac users replace above with:\n",
    "# CMAKE_ARGS=\"-DGGML_METAL=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
    "```\n",
    "\n",
    "#### Step 2: Use the `n_gpu_layers` argument with `LLM`\n",
    "\n",
    "```python\n",
    "llm = LLM(n_gpu_layers=35)\n",
    "```\n",
    "\n",
    "The value for `n_gpu_layers` depends on your GPU memory and the model you're using (e.g., max of 33 for default 7B model).  Set `n_gpu_layers=-1` to offload all layers to the GPU (this will offload all 33 layers to the default model). You can reduce the value if you get an error (e.g., `CUDA error: out-of-memory`).  For instance, using two old NVDIDIA TITAN V GPUs each with 12GB of VRAM, 59 out 83 layers in a [quantized Llama-2 70B model](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q3_K_S.gguf) can be offloaded to the GPUs (i.e., 60 layers or more results in a \"CUDA out of memory\" error).\n",
    "\n",
    "\n",
    "With the steps above, calls to methods like `llm.prompt` will offload computation to your GPU and speed up responses from the LLM.\n",
    "\n",
    "The above assumes that NVIDIA drivers and the CUDA toolkit are already installed. On Ubuntu Linux systems, this can be accomplished [with a single command](https://lambdalabs.com/lambda-stack-deep-learning-software)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "1. **How do I use other models with OnPrem.LLM?**\n",
    "\n",
    "    >You can supply the URL to other models to the `LLM` constructor, as we did above in the code generation example.\n",
    "\n",
    "    >As of v0.0.20, we support models in GGUF format, which supersedes the older GGML format. You can find llama.cpp-supported models with `GGUF` in the file name on [huggingface.co](https://huggingface.co/models?sort=trending&search=gguf).\n",
    "    \n",
    "    >Make sure you are pointing to the URL of the actual GGUF model file, which is the \"download\" link on the model's page. An example for **Mistral-7B** is shown below:\n",
    "\n",
    "    ><img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/model_download_link.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>\n",
    "\n",
    "    > Note that some models have specific prompt formats. For instance, the prompt template required for **Zephyr-7B**, as described on the [model's page](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF), is:\n",
    "    >\n",
    "    > ```<|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>```\n",
    "    >\n",
    "    > So, to use the **Zephyr-7B** model, you must supply the `prompt_template` argument to the `LLM` constructor (or specify it in the `webapp.yml` configuration for the Web app).\n",
    "    >\n",
    "    > ```python\n",
    "    > # how to use Zephyr-7B with OnPrem.LLM\n",
    "    > llm = LLM(model_url='https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf',\n",
    "    >           prompt_template = \"<|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\",\n",
    "    >           n_gpu_layers=33)\n",
    "    > llm.prompt(\"List three cute names for a cat.\")\n",
    "    >```\n",
    "\n",
    "\n",
    "2. **I'm behind a corporate firewall and am receiving an SSL error when trying to download the model?**\n",
    "\n",
    "    >Try this:\n",
    "    >```python\n",
    "    >from onprem import LLM\n",
    "    >LLM.download_model(url, ssl_verify=False)\n",
    "    >```\n",
    "\n",
    "    >You can download the embedding model (used by `LLM.ingest` and `LLM.ask`) as follows:\n",
    "    >```sh\n",
    "    >wget --no-check-certificate https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/all-MiniLM-L6-v2.zip\n",
    "    >```\n",
    "    \n",
    "    >Supply the unzipped folder name as the `embedding_model_name` argument to `LLM`.\n",
    "    \n",
    "\n",
    "3. **How do I use this on a machine with no internet access?**\n",
    "\n",
    "    >Use the `LLM.download_model` method to download the model files to `<your_home_directory>/onprem_data` and transfer them to the same location on the air-gapped  machine.\n",
    "\n",
    "    >For the `ingest` and `ask` methods, you will need to also download and transfer the embedding model files:\n",
    "    >```python\n",
    "    >from sentence_transformers import SentenceTransformer\n",
    "    >model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    >model.save('/some/folder')\n",
    "    >```\n",
    "    \n",
    "    >Copy the `some/folder` folder to the air-gapped machine and supply the path to `LLM` via the `embedding_model_name` parameter.\n",
    " \n",
    " \n",
    "\n",
    "4. **When installing `onprem`, I'm getting \"build\" errors related to `llama-cpp-python` (or `chroma-hnswlib`) on Windows/Mac/Linux?** \n",
    "\n",
    "    > See [this LangChain documentation on LLama.cpp](https://python.langchain.com/docs/integrations/llms/llamacpp) for help on installing the `llama-cpp-python` package for your system. Additional tips for different operating systems are shown below:\n",
    "   \n",
    "    > For **Linux** systems like Ubuntu, try this: `sudo apt-get install build-essential g++ clang`.  Other tips are [here](https://github.com/oobabooga/text-generation-webui/issues/1534).\n",
    "    \n",
    "    > For **Windows** systems, please try following [these instructions](https://github.com/amaiya/onprem/blob/master/MSWindows.md).  We recommend you use [Windows Subsystem for Linux (WSL)](https://learn.microsoft.com/en-us/windows/wsl/install) instead of using Microsoft Windows directly. If you do need to use Microsoft Window directly, be sure to install the [Microsoft C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/) and make sure the **Desktop development with C++** is selected. \n",
    "    \n",
    "    > For **Macs**, try following [these tips](https://github.com/imartinez/privateGPT/issues/445#issuecomment-1563333950).\n",
    "    \n",
    "    > There are also various other tips for each of the above OSes in [this privateGPT repo thread](https://github.com/imartinez/privateGPT/issues/445).  Of course, you can also [easily use](https://colab.research.google.com/drive/1LVeacsQ9dmE1BVzwR3eTLukpeRIMmUqi?usp=sharing) **OnPrem.LLM** on Google Colab.\n",
    "    \n",
    "    >  Finally, if you still can't overcome issues with building `llama-cpp-python`, you can try [installing the pre-built wheel file](https://abetlen.github.io/llama-cpp-python/whl/cpu/llama-cpp-python/) for your system:\n",
    "    \n",
    "    > **Example:** `pip install llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu`\n",
    "    >\n",
    "    > **Tip:** There are [pre-built wheel files for `chroma-hnswlib`](https://pypi.org/project/chroma-hnswlib/#files), as well. If running `pip install onprem` fails on building `chroma-hnswlib`, it may be because a pre-built wheel doesn't yet exist for the version of Python you're using (in which case you can try downgrading Python).   \n",
    "\n",
    "5. **`llama-cpp-python` is failing to load my model from the model path on Google Colab.** \n",
    "\n",
    "    > For reasons that are unclear, newer versions of `llama-cpp-python` fail to load models on Google Colab unless you supply `verbose=True` to the `LLM` constructor (which is passed directly to `llama-cpp-python`). If you experience this problem locally, try supplying `verbose=True` to `LLM`.\n",
    "\n",
    "6. **I'm getting an `â€œIllegal instruction (core dumped)` error when instantiating a `langchain.llms.Llamacpp` or `onprem.LLM` object?**\n",
    "\n",
    "   > Your CPU may not support instructions that `cmake` is using for one reason or another (e.g., [due to Hyper-V in VirtualBox settings](https://stackoverflow.com/questions/65780506/how-to-enable-avx-avx2-in-virtualbox-6-1-16-with-ubuntu-20-04-64bit)). You can try turning them off when building and installing `llama-cpp-python`:\n",
    "\n",
    "    >```sh\n",
    "    ># example\n",
    "    >CMAKE_ARGS=\"-DGGML_CUDA=ON -DGGML_AVX2=OFF -DGGML_AVX=OFF -DGGML_F16C=OFF -DGGML_FMA=OFF\" FORCE_CMAKE=1 pip install --force-reinstall llama-cpp-python --no-cache-dir\n",
    "    >```\n",
    "\n",
    "7. **How can I speed up `LLM.ingest` using my GPU?**\n",
    "\n",
    "    >\n",
    "    >Try using the `embedding_model_kwargs` argument:\n",
    "    >```python\n",
    "    >from onprem import LLM\n",
    "    >llm  = LLM(embedding_model_kwargs={'device':'cuda'})\n",
    "    >```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
