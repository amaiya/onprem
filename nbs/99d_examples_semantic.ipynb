{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying vector database in [OnPrem.LLM](https://github.com/amaiya/onprem) can be used for detecting semantic similarity among pieces of text.\n",
    "\n",
    "You can access the default vectorstore from an `LLM` object:\n",
    "```python\n",
    "from onprem import LLM\n",
    "\n",
    "vectordb_path = tempfile.mkdtemp()\n",
    "llm = LLM(\n",
    "    embedding_model_name=\"sentence-transformers/nli-mpnet-base-v2\",\n",
    "    embedding_encode_kwargs={\"normalize_embeddings\": True},\n",
    "    vectordb_path=vectordb_path,\n",
    "    store_type='dense',\n",
    "    verbose=False\n",
    ")\n",
    "```\n",
    "But, we will create `VectorStore` instances explicitly here to avoid loading an LLM, which is not needed in this example.\n",
    "\n",
    "The `VectorStoreFactory` is useful in instantiating different backend vectorstores (e.g., Chroma, Whoosh, Elasticsearch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "import os, tempfile\n",
    "\n",
    "from onprem.ingest.stores import VectorStoreFactory\n",
    "\n",
    "store = VectorStoreFactory.create(\n",
    "    kind='chroma',\n",
    "    persist_location='/tmp/my_vectordb',\n",
    "    embedding_model_name=\"sentence-transformers/nli-mpnet-base-v2\",\n",
    "    embedding_encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "data = [  # from txtai\n",
    "    \"US tops 5 million confirmed virus cases\",\n",
    "    \"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\",\n",
    "    \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n",
    "    \"The National Park Service warns against sacrificing slower friends in a bear attack\",\n",
    "    \"Maine man wins $1M from $25 lottery ticket\",\n",
    "    \"Make huge profits without work, earn up to $100,000 a day\",\n",
    "]\n",
    "source_folder = tempfile.mkdtemp()\n",
    "for i, d in enumerate(data):\n",
    "    filename = os.path.join(source_folder, f\"doc{i}.txt\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /tmp/my_vectordb\n",
      "Loading documents from /tmp/tmpeg2wt1z7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|████████████████████| 6/6 [00:00<00:00, 1540.98it/s]\n",
      "Processing and chunking 6 new documents: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 1940.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 6 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "store.ingest(source_folder, chunk_size=500, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get a reference to the underlying vector store and query it directly to find the best semantic match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feel good story : Maine man wins $1M from $25 lottery ticket\n",
      "climate change : Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\n",
      "public health story : US tops 5 million confirmed virus cases\n",
      "war : Beijing mobilises invasion craft along coast as Taiwan tensions escalate\n",
      "wildlife : The National Park Service warns against sacrificing slower friends in a bear attack\n",
      "asia : Beijing mobilises invasion craft along coast as Taiwan tensions escalate\n",
      "lucky : Maine man wins $1M from $25 lottery ticket\n",
      "dishonest junk : Make huge profits without work, earn up to $100,000 a day\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "for query in (\n",
    "    \"feel good story\",\n",
    "    \"climate change\",\n",
    "    \"public health story\",\n",
    "    \"war\",\n",
    "    \"wildlife\",\n",
    "    \"asia\",\n",
    "    \"lucky\",\n",
    "    \"dishonest junk\",\n",
    "):\n",
    "    docs = store.semantic_search(query)\n",
    "    print(f\"{query} : {docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
