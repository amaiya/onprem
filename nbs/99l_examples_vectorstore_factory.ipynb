{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Different Vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to use the **VectorStoreFactory** in [OnPrem.LLM](https://github.com/amaiya/onprem) to easily create and experiment with different types of vector stores for your RAG (Retrieval-Augmented Generation) and semantic search applications.\n",
    "\n",
    "The VectorStoreFactory provides a unified interface for creating three different types of vector stores, each optimized for different use cases:\n",
    "\n",
    "- **ChromaStore (default)**: Dense vector search using embeddings for semantic search\n",
    "- **WhooshStore**: Sparse keyword search using full-text indexing with on-the-fly dense vector encoding for semantic search.\n",
    "- **ElasticsearchStore**: Unified hybrid search combining both dense and sparse approaches, including support for hybrid search using [RRF](https://dl.acm.org/doi/10.1145/1571941.1572114).\n",
    "\n",
    "This makes it easy to experiment with different search strategies and find the best approach for your specific data and use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's create some sample documents that we'll use throughout our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 sample documents for testing\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "import tempfile\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from onprem.ingest.stores import VectorStoreFactory\n",
    "\n",
    "# Create some sample documents for our examples\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"Machine learning is a subset of artificial intelligence that enables computers to learn without explicit programming.\",\n",
    "        metadata={\"source\": \"ml_intro.txt\", \"topic\": \"AI\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "        metadata={\"source\": \"dl_guide.txt\", \"topic\": \"AI\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Natural language processing (NLP) enables computers to understand and process human language.\",\n",
    "        metadata={\"source\": \"nlp_basics.txt\", \"topic\": \"AI\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases store high-dimensional vectors and enable similarity search for AI applications.\",\n",
    "        metadata={\"source\": \"vector_db.txt\", \"topic\": \"databases\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Retrieval-augmented generation (RAG) combines information retrieval with language generation for better AI responses.\",\n",
    "        metadata={\"source\": \"rag_overview.txt\", \"topic\": \"AI\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    Document(\n",
    "    page_content=\"Cats have five toes on their front paws, four on their back paws, and zero interest in your personal space..\",\n",
    "    metadata={\"source\": \"cat_facts.txt\", \"topic\": \"cats\", \"difficulty\": \"advanced\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(sample_docs)} sample documents for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: ChromaStore (Dense Vector Search)\n",
    "\n",
    "ChromaStore is the default option and excels at semantic similarity search. It's perfect when you want to find documents that are conceptually similar to your query, even if they don't share exact keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ChromaStore at: /tmp/tmp9k2it641\n",
      "Store type: ChromaStore\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 documents to ChromaStore\n",
      "\n",
      "Semantic search results for 'artificial intelligence and machine learning':\n",
      "1. Machine learning is a subset of artificial intelligence that... (from ml_intro.txt)\n",
      "   Similarity score: 0.621\n",
      "2. Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n",
      "   Similarity score: 0.439\n",
      "3. Vector databases store high-dimensional vectors and enable s... (from vector_db.txt)\n",
      "   Similarity score: 0.357\n",
      "\n",
      "Semantic search results for 'feline feet':\n",
      "1. Cats have five toes on their front paws, four on their back ... (from cat_facts.txt)\n",
      "   Similarity score: 0.538\n",
      "2. Vector databases store high-dimensional vectors and enable s... (from vector_db.txt)\n",
      "   Similarity score: 0.059\n",
      "3. Natural language processing (NLP) enables computers to under... (from nlp_basics.txt)\n",
      "   Similarity score: 0.030\n",
      "\n",
      "Semantic search for 'computer intelligence' (no exact keyword matches):\n",
      "- Machine learning is a subset of artificial intelligence that... (score: 0.524, category: AI)\n",
      "- Natural language processing (NLP) enables computers to under... (score: 0.406, category: AI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Create ChromaStore using the factory (default)\n",
    "chroma_path = tempfile.mkdtemp()\n",
    "chroma_store = VectorStoreFactory.create(\n",
    "    kind='chroma',  # or just use default: VectorStoreFactory.create()\n",
    "    persist_location=chroma_path\n",
    ")\n",
    "\n",
    "print(f\"Created ChromaStore at: {chroma_path}\")\n",
    "print(f\"Store type: {type(chroma_store).__name__}\")\n",
    "\n",
    "# Add documents\n",
    "chroma_store.add_documents(sample_docs)\n",
    "print(f\"Added {len(sample_docs)} documents to ChromaStore\")\n",
    "\n",
    "# Test semantic search - look for documents about AI/ML\n",
    "results = chroma_store.semantic_search(\"artificial intelligence and machine learning\", limit=3)\n",
    "print(f\"\\nSemantic search results for 'artificial intelligence and machine learning':\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content[:60]}... (from {doc.metadata['source']})\")\n",
    "    print(f\"   Similarity score: {doc.metadata.get('score', 'N/A'):.3f}\")\n",
    "\n",
    "# Test semantic search - look for documents about felines\n",
    "results = chroma_store.semantic_search(\"feline feet\", limit=3)\n",
    "print(f\"\\nSemantic search results for 'feline feet':\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content[:60]}... (from {doc.metadata['source']})\")\n",
    "    print(f\"   Similarity score: {doc.metadata.get('score', 'N/A'):.3f}\")\n",
    "\n",
    "# Show that semantic search finds conceptually related content\n",
    "print(f\"\\nSemantic search for 'computer intelligence' (no exact keyword matches):\")\n",
    "results = chroma_store.semantic_search(\"computer intelligence\", limit=2)\n",
    "for doc in results:\n",
    "    print(f\"- {doc.page_content[:60]}... (score: {doc.metadata.get('score', 'N/A'):.3f}, category: {doc.metadata.get('topic', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: WhooshStore (Sparse Keyword Search)\n",
    "\n",
    "WhooshStore uses full-text search and is excellent for exact keyword matching and boolean queries. It's faster for ingestion and works well when you know specific terms you're looking for.  Unlike ChromaStore, WhooshStore converts text to dense vectors on-the-fly for semantic searches.  Since vectors are not computed at index time, ingestion is very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created WhooshStore at: /tmp/tmprihpjo79\n",
      "Store type: WhooshStore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 244.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 documents to WhooshStore\n",
      "\n",
      "Keyword search results for 'neural networks':\n",
      "Total hits: 1\n",
      "1. Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n",
      "\n",
      "Boolean search for 'machine AND learning':\n",
      "Total hits: 1\n",
      "- Machine learning is a subset of artificial intelligence that...\n",
      "\n",
      "Semantic search results for 'feline feet':\n",
      "- Cats have five toes on their front paws, four on their back ... (score: 0.538, category: cats)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Create WhooshStore using the factory\n",
    "whoosh_path = tempfile.mkdtemp()\n",
    "whoosh_store = VectorStoreFactory.create(\n",
    "    kind='whoosh',\n",
    "    persist_location=whoosh_path\n",
    ")\n",
    "\n",
    "print(f\"Created WhooshStore at: {whoosh_path}\")\n",
    "print(f\"Store type: {type(whoosh_store).__name__}\")\n",
    "\n",
    "# Add documents\n",
    "whoosh_store.add_documents(sample_docs)\n",
    "print(f\"Added {len(sample_docs)} documents to WhooshStore\")\n",
    "\n",
    "# Test keyword search - exact term matching\n",
    "results = whoosh_store.query(\"neural networks\", limit=3)\n",
    "print(f\"\\nKeyword search results for 'neural networks':\")\n",
    "print(f\"Total hits: {results['total_hits']}\")\n",
    "for i, hit in enumerate(results['hits'], 1):\n",
    "    print(f\"{i}. {hit['page_content'][:60]}... (from {hit['source']})\")\n",
    "\n",
    "# Show boolean search capabilities\n",
    "results = whoosh_store.query(\"machine AND learning\", limit=3)\n",
    "print(f\"\\nBoolean search for 'machine AND learning':\")\n",
    "print(f\"Total hits: {results['total_hits']}\")\n",
    "for hit in results['hits']:\n",
    "    print(f\"- {hit['page_content'][:60]}...\")\n",
    "\n",
    "# Test semantic search (uses embeddings on top of keyword results)\n",
    "semantic_results = whoosh_store.semantic_search(\"feline feet\", limit=2, filters={'topic' :'cats'})\n",
    "print(f\"\\nSemantic search results for 'feline feet':\")\n",
    "for doc in semantic_results:\n",
    "    print(f\"- {doc.page_content[:60]}... (score: {doc.metadata.get('score', 'N/A'):.3f}, category: {doc.metadata.get('topic', 'N/A')})\")\n",
    "whoosh_store.erase(confirm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: ElasticsearchStore (Hybrid Search)\n",
    "\n",
    "ElasticsearchStore combines both dense and sparse search capabilities in a single unified store. It can perform keyword search, semantic search, and hybrid search that combines both approaches. \n",
    "\n",
    "**Note**: This example requires Elasticsearch to be running.  These examples use Elasticsearch 8.15.5, but Elasticsearch 9.x is also supported.\n",
    "\n",
    "You can download Elasticsearch and start it from command-line:\n",
    "\n",
    "```bash\n",
    " ./elasticsearch-8.15.5/bin/elasticsearch\n",
    "```\n",
    "\n",
    "When starting Elasticsearch for the first time, make note of the password and set the following dictionary accordingly:\n",
    "\n",
    "If you don't have Elasticsearch installed, you can skip this section or also try setting it up using Docker:\n",
    "```bash\n",
    "\n",
    "# Elasticsearch 8.x with security disabled:\n",
    "docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" elasticsearch:8.15.5\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "elastic_params = {'persist_location': 'https://localhost:9200', \n",
    "                  'index_name': 'demo_index', \n",
    "                  'verify_certs': True, \n",
    "                  'ca_certs': '/PATH/TO/ELASTIC_FOLDER/elasticsearch-8.15.5/config/certs/http_ca.crt', \n",
    "                  'basic_auth': ('elastic', 'YOUR_PASSWORD')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ElasticsearchStore\n",
      "Store type: ElasticsearchStore\n",
      "Added 6 documents to ElasticsearchStore\n",
      "\n",
      "Keyword search results for 'neural networks':\n",
      "Total hits: 1\n",
      "- Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n",
      "\n",
      "Semantic search results for 'artificial intelligence and machine learning':\n",
      "Total hits: 6\n",
      "- Machine learning is a subset of artificial intelligence that... (score: 0.621063, category: AI)\n",
      "- Deep learning uses neural networks with multiple layers to m... (score: 0.439149, category: AI)\n",
      "- Vector databases store high-dimensional vectors and enable s... (score: 0.357402, category: databases)\n",
      "\n",
      "Semantic search results for 'feline feet':\n",
      "Total hits: 6\n",
      "- Cats have five toes on their front paws, four on their back ... (score: 0.537507, category: cats)\n",
      "- Vector databases store high-dimensional vectors and enable s... (score: 0.059024, category: databases)\n",
      "- Natural language processing (NLP) enables computers to under... (score: 0.029732, category: AI)\n",
      "\n",
      "Hybrid search results for 'machine learning algorithms':\n",
      "Total hits: 3\n",
      "- Vector databases store high-dimensional vectors and enable s... (combined score: 0.598861)\n",
      "- Retrieval-augmented generation (RAG) combines information re... (combined score: 0.355312)\n",
      "- Machine learning is a subset of artificial intelligence that... (combined score: 0.309971)\n",
      "\n",
      "Cleaned up ElasticsearchStore\n"
     ]
    }
   ],
   "source": [
    "  # | notest\n",
    "\n",
    "  # Create ElasticsearchStore using the factory\n",
    "  # Note: This requires Elasticsearch to be running on localhost:9200\n",
    "  try:\n",
    "      elasticsearch_store = VectorStoreFactory.create(\n",
    "          kind='elasticsearch', **elastic_params,\n",
    "      )\n",
    "\n",
    "      print(f\"Created ElasticsearchStore\")\n",
    "      print(f\"Store type: {type(elasticsearch_store).__name__}\")\n",
    "\n",
    "      # Add documents\n",
    "      elasticsearch_store.add_documents(sample_docs)\n",
    "      print(f\"Added {len(sample_docs)} documents to ElasticsearchStore\")\n",
    "\n",
    "      # Test keyword search (sparse)\n",
    "      search_results = elasticsearch_store.search(\"neural networks\", limit=3)\n",
    "      print(f\"\\nKeyword search results for 'neural networks':\")\n",
    "      print(f\"Total hits: {search_results['total_hits']}\")\n",
    "      for hit in search_results['hits']:\n",
    "          print(f\"- {hit['page_content'][:60]}... (from {hit['source']})\")\n",
    "\n",
    "      # Test semantic search (dense)\n",
    "      #semantic_results = elasticsearch_store.semantic_search(\"AI algorithms\", limit=3)\n",
    "      semantic_results = elasticsearch_store.semantic_search(\"artificial intelligence and machine learning\", limit=3)\n",
    "\n",
    "      print(f\"\\nSemantic search results for 'artificial intelligence and machine learning':\")\n",
    "      print(f\"Total hits: {semantic_results['total_hits']}\")\n",
    "      for hit in semantic_results['hits']:\n",
    "          # Show more precision in scores to see if they're actually different\n",
    "          score = hit.get('score', 'N/A')\n",
    "          score_str = f\"{score:.6f}\" if isinstance(score, (int, float)) else str(score)\n",
    "          print(f\"- {hit['page_content'][:60]}... (score: {score_str}, category: {hit.get('topic', 'N/A')})\")\n",
    "\n",
    "      # Test semantic search (dense)\n",
    "      semantic_results = elasticsearch_store.semantic_search(\"feline feet\", limit=3)\n",
    "      print(f\"\\nSemantic search results for 'feline feet':\")\n",
    "      print(f\"Total hits: {semantic_results['total_hits']}\")\n",
    "      for hit in semantic_results['hits']:\n",
    "          # Show more precision in scores to see if they're actually different\n",
    "          score = hit.get('score', 'N/A')\n",
    "          score_str = f\"{score:.6f}\" if isinstance(score, (int, float)) else str(score)\n",
    "          print(f\"- {hit['page_content'][:60]}... (score: {score_str}, category: {hit.get('topic', 'N/A')})\")\n",
    "      \n",
    "      # Test hybrid search (combines both dense and sparse)\n",
    "      hybrid_results = elasticsearch_store.hybrid_search(\n",
    "          \"AI algorithms\",\n",
    "          limit=3,\n",
    "          weights=[0.7, 0.3]  # 70% semantic, 30% keyword\n",
    "      )\n",
    "      print(f\"\\nHybrid search results for 'machine learning algorithms':\")\n",
    "      print(f\"Total hits: {hybrid_results['total_hits']}\")\n",
    "      for hit in hybrid_results['hits']:\n",
    "          score = hit.get('score', 'N/A')\n",
    "          score_str = f\"{score:.6f}\" if isinstance(score, (int, float)) else str(score)\n",
    "          print(f\"- {hit['page_content'][:60]}... (combined score: {score_str})\")\n",
    "\n",
    "      # Clean up\n",
    "      elasticsearch_store.erase(confirm=False)\n",
    "      print(f\"\\nCleaned up ElasticsearchStore\")\n",
    "\n",
    "  except Exception as e:\n",
    "      print(f\"ElasticsearchStore example skipped: {e}\")\n",
    "      print(\"Make sure Elasticsearch is running on localhost:9200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Use Cases with Elasticsearch\n",
    "\n",
    "Many applications have documents already stored in conventional Elasticsearch index with no vector embeddings.  The `ElasticsearchSparseStore` module in OnPrem.LLM allows you to point OnPrem.LLM to any Elasticsearch instance for RAG and semantic similarity appplications.\n",
    "\n",
    "You can do so by instantiating `ElasticsearchSparseStore` as follows:\n",
    "\n",
    "```python\n",
    "from onprem.ingest.stores.sparse import ElasticsearchSparseStore\n",
    "store = ElasticsearchSparseStore(\n",
    "    persist_location='https://localhost:9200',\n",
    "    index_name='NAME_OF_YOUR_INDEX',\n",
    "    # Map OnPrem.LLM field names to your existing field names\n",
    "    content_field='content',      # Your content field name\n",
    "    id_field='doc_id',            # Your ID field name\n",
    "    source_field='filepath',      # Your source field name (optional)\n",
    "    content_analyzer='english',   # Your analyzer (defaults to standard)\n",
    "    # Optional: Authentication if needed\n",
    "    basic_auth=('elastic', 'CHANGEME'),\n",
    "    verify_certs=False, # change to True if you provide path to ES certs as we did above\n",
    "    # Optional: Enable semantic search with dynamic chunking\n",
    "    chunk_for_semantic_search=True,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# traditional keyword search (supposes you have an extension field in your index)\n",
    "results = store.search('\"machine learning\"', filters={'extension' : 'pdf') # assumes you have an extension field in your index\n",
    "\n",
    "# semantic searches (no vectors need to be indexed in your Elasticsearch instance!)\n",
    "results = store.semantic_search('\"machine learning\"', return_chunks=False) # set return_chunks=True for RAG applications\n",
    "# best matching chunk from document\n",
    "best_chunk_id =  results[0].metadata['best_chunk_idx']\n",
    "print(results[0].metadata['chunks'][best_chunk_id]\n",
    "\n",
    "# OUTPUT: 'of the machine learning (ML) workflow such as data-preprocessing and human-in-the-loop\n",
    "#          model tuning and inspection. Following inspiration from a blog post by Rachel Thomas of\n",
    "#          fast.ai (Howard and Gugger, 2020), we refer to this as Augmented Machine Learning.'\n",
    "```\n",
    "\n",
    "The interesting thing in this example above is that:\n",
    "\n",
    "1. Embeddings do not have to be stored in the Elasticsearch index and are computed dynamically \n",
    "2. Documents do not need to be pre-chunked in your index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with LLM\n",
    "\n",
    "The VectorStoreFactory works seamlessly with OnPrem.LLM for complete RAG (Retrieval-Augmented Generation) workflows.\n",
    "\n",
    "By default, supplying `store_type=\"dense\"` to `LLM` will use `ChromaStore` and supplying `store_type=\"sparse\"` will use  WhooshStore.  To use `ElasticsearchStore`, you can supply it to `load_vectorstore` as a custom vectorstore:\n",
    "\n",
    "```python\n",
    "llm = LLM(...)\n",
    "llm.load_vectorstore(custom_vectorstore=elasticsearch_store)\n",
    "```\n",
    "\n",
    "You can also implement and use your own custom VectorStore instances (by subclassing `DenseStore`, `SparseStore`, or `DualStore`) using whatever vector database backend you like.\n",
    "\n",
    "For illustration purposes, in the example below, we explictly tell `LLM` to use `WhooshStore` as a custom vectorstore.  (This is equivalent to supplying `store_type=\"sparse\"` to `LLM`, but it shows how you would use `LLM` with Elasticsearch or your own custom vectorstore.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Integration with OnPrem.LLM:\n",
      "✓ Created 3 documents in /tmp/tmpjekc6pkt\n",
      "Creating new vectorstore at /tmp/my_search_index\n",
      "Loading documents from /tmp/tmpjekc6pkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|█████████████████████| 3/3 [00:00<00:00, 175.48it/s]\n",
      "Processing and chunking 3 new documents: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 248.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 3 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 983.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n",
      "\n",
      "\n",
      "----RAG EXAMPLE----\n",
      "QUESTION: What are the types of machine learning?\n",
      "\n",
      "The types of machine learning are:\n",
      "\n",
      "1. Supervised learning - uses labeled data.\n",
      "2. Unsupervised learning - finds patterns in unlabeled data.\n",
      "3. Reinforcement learning - learns through trial and error.\n",
      "\n",
      "SOURCES:\n",
      "source #1: /tmp/tmpjekc6pkt/ml_types.txt\n",
      "source #2: /tmp/tmpjekc6pkt/ai_overview.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Example: Using VectorStoreFactory with LLM for RAG\n",
    "print(\"🤖 Integration with OnPrem.LLM:\")\n",
    "\n",
    "# Create a simple document corpus\n",
    "documents_dir = tempfile.mkdtemp()\n",
    "doc_files = {\n",
    "    \"ai_overview.txt\": \"Artificial intelligence is transforming how we work and live. Machine learning enables computers to learn from data without explicit programming.\",\n",
    "    \"ml_types.txt\": \"There are three main types of machine learning: supervised learning uses labeled data, unsupervised learning finds patterns in unlabeled data, and reinforcement learning learns through trial and error.\",\n",
    "    \"applications.txt\": \"AI applications include natural language processing for text analysis, computer vision for image recognition, and recommendation systems for personalized content.\"\n",
    "}\n",
    "\n",
    "# Write documents to files\n",
    "for filename, content in doc_files.items():\n",
    "    with open(os.path.join(documents_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"✓ Created {len(doc_files)} documents in {documents_dir}\")\n",
    "\n",
    "# Show how to use custom vector store with LLM\n",
    "from onprem import LLM\n",
    "from onprem.ingest.stores import VectorStoreFactory\n",
    "\n",
    "# Create custom vector store\n",
    "store = VectorStoreFactory.create('whoosh', persist_location='/tmp/my_search_index')\n",
    "\n",
    "# Create LLM and use custom vector store\n",
    "llm = LLM('openai/gpt-4o-mini', vectordb_path=tempfile.mkdtemp())\n",
    "llm.load_vectorstore(custom_vectorstore=store)\n",
    "\n",
    "# Ingest documents\n",
    "llm.ingest(documents_dir)\n",
    "\n",
    "print('\\n\\n----RAG EXAMPLE----')\n",
    "# Ask questions\n",
    "question = 'What are the types of machine learning?'\n",
    "print(f'QUESTION: {question}')\n",
    "print()\n",
    "result = llm.ask(question)\n",
    "\n",
    "print('\\n\\nSOURCES:')\n",
    "for i, d in enumerate(result['source_documents']):\n",
    "    print(f\"source #{i+1}: {d.metadata['source']}\")\n",
    "store.erase(confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaned up temporary directories\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Clean up temporary directories\n",
    "import shutil\n",
    "\n",
    "temp_dirs = [chroma_path, whoosh_path, documents_dir]\n",
    "for temp_dir in temp_dirs:\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(\"🧹 Cleaned up temporary directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
