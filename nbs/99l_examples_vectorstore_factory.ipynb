{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All About Vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to use the **VectorStoreFactory** in [OnPrem.LLM](https://github.com/amaiya/onprem) to easily create and experiment with different types of vector stores for your RAG (Retrieval-Augmented Generation) applications.\n",
    "\n",
    "The VectorStoreFactory provides a unified interface for creating three different types of vector stores, each optimized for different use cases:\n",
    "\n",
    "- **ChromaStore (default)**: Dense vector search using embeddings for semantic search\n",
    "- **WhooshStore**: Sparse keyword search using full-text indexing with on-the-fly dense vector encoding for semantic search.\n",
    "- **ElasticsearchStore**: Unified hybrid search combining both dense and sparse approaches, including support for hybrid search using [RRF](https://dl.acm.org/doi/10.1145/1571941.1572114).\n",
    "\n",
    "This makes it easy to experiment with different search strategies and find the best approach for your specific data and use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's create some sample documents that we'll use throughout our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 sample documents for testing\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "import tempfile\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from onprem.ingest.stores import VectorStoreFactory\n",
    "\n",
    "# Create some sample documents for our examples\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"Machine learning is a subset of artificial intelligence that enables computers to learn without explicit programming.\",\n",
    "        metadata={\"source\": \"ml_intro.txt\", \"topic\": \"AI\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "        metadata={\"source\": \"dl_guide.txt\", \"topic\": \"AI\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Natural language processing (NLP) enables computers to understand and process human language.\",\n",
    "        metadata={\"source\": \"nlp_basics.txt\", \"topic\": \"AI\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases store high-dimensional vectors and enable similarity search for AI applications.\",\n",
    "        metadata={\"source\": \"vector_db.txt\", \"topic\": \"databases\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Retrieval-augmented generation (RAG) combines information retrieval with language generation for better AI responses.\",\n",
    "        metadata={\"source\": \"rag_overview.txt\", \"topic\": \"AI\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    Document(\n",
    "    page_content=\"Cats have five toes on their front paws, four on their back paws, and zero interest in your personal space..\",\n",
    "    metadata={\"source\": \"cat_facts.txt\", \"topic\": \"cats\", \"difficulty\": \"advanced\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(sample_docs)} sample documents for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: ChromaStore (Dense Vector Search)\n",
    "\n",
    "ChromaStore is the default option and excels at semantic similarity search. It's perfect when you want to find documents that are conceptually similar to your query, even if they don't share exact keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ChromaStore at: /tmp/tmpr40k2ow5\n",
      "Store type: ChromaStore\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 documents to ChromaStore\n",
      "\n",
      "Semantic search results for 'artificial intelligence and machine learning':\n",
      "1. Machine learning is a subset of artificial intelligence that... (from ml_intro.txt)\n",
      "   Similarity score: 0.621\n",
      "2. Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n",
      "   Similarity score: 0.439\n",
      "3. Vector databases store high-dimensional vectors and enable s... (from vector_db.txt)\n",
      "   Similarity score: 0.357\n",
      "\n",
      "Semantic search for 'computer intelligence' (no exact keyword matches):\n",
      "- Machine learning is a subset of artificial intelligence that... (score: 0.524, category: AI)\n",
      "- Natural language processing (NLP) enables computers to under... (score: 0.406, category: AI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Create ChromaStore using the factory (default)\n",
    "chroma_path = tempfile.mkdtemp()\n",
    "chroma_store = VectorStoreFactory.create(\n",
    "    kind='chroma',  # or just use default: VectorStoreFactory.create()\n",
    "    persist_location=chroma_path\n",
    ")\n",
    "\n",
    "print(f\"Created ChromaStore at: {chroma_path}\")\n",
    "print(f\"Store type: {type(chroma_store).__name__}\")\n",
    "\n",
    "# Add documents\n",
    "chroma_store.add_documents(sample_docs)\n",
    "print(f\"Added {len(sample_docs)} documents to ChromaStore\")\n",
    "\n",
    "# Test semantic search - look for documents about AI/ML\n",
    "results = chroma_store.semantic_search(\"artificial intelligence and machine learning\", limit=3)\n",
    "print(f\"\\nSemantic search results for 'artificial intelligence and machine learning':\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content[:60]}... (from {doc.metadata['source']})\")\n",
    "    print(f\"   Similarity score: {doc.metadata.get('score', 'N/A'):.3f}\")\n",
    "\n",
    "# Show that semantic search finds conceptually related content\n",
    "print(f\"\\nSemantic search for 'computer intelligence' (no exact keyword matches):\")\n",
    "results = chroma_store.semantic_search(\"computer intelligence\", limit=2)\n",
    "for doc in results:\n",
    "    print(f\"- {doc.page_content[:60]}... (score: {doc.metadata.get('score', 'N/A'):.3f}, category: {doc.metadata.get('topic', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: WhooshStore (Sparse Keyword Search)\n",
    "\n",
    "WhooshStore uses full-text search and is excellent for exact keyword matching and boolean queries. It's faster for ingestion and works well when you know specific terms you're looking for.  Unlike ChromaStore, WhooshStore converts text to dense vectors on-the-fly for semantic searches.  Since vectors are not computed at index time, ingestion is very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created WhooshStore at: /tmp/tmpj0itm4tn\n",
      "Store type: WhooshStore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 871.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 documents to WhooshStore\n",
      "\n",
      "Keyword search results for 'neural networks':\n",
      "Total hits: 1\n",
      "1. Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n",
      "\n",
      "Boolean search for 'machine AND learning':\n",
      "Total hits: 1\n",
      "- Machine learning is a subset of artificial intelligence that...\n",
      "\n",
      "Semantic search results for 'AI learning':\n",
      "- Machine learning is a subset of artificial intelligence that... (score: 0.514, category: AI)\n",
      "- Deep learning uses neural networks with multiple layers to m... (score: 0.411, category: AI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Create WhooshStore using the factory\n",
    "whoosh_path = tempfile.mkdtemp()\n",
    "whoosh_store = VectorStoreFactory.create(\n",
    "    kind='whoosh',\n",
    "    persist_location=whoosh_path\n",
    ")\n",
    "\n",
    "print(f\"Created WhooshStore at: {whoosh_path}\")\n",
    "print(f\"Store type: {type(whoosh_store).__name__}\")\n",
    "\n",
    "# Add documents\n",
    "whoosh_store.add_documents(sample_docs)\n",
    "print(f\"Added {len(sample_docs)} documents to WhooshStore\")\n",
    "\n",
    "# Test keyword search - exact term matching\n",
    "results = whoosh_store.query(\"neural networks\", limit=3)\n",
    "print(f\"\\nKeyword search results for 'neural networks':\")\n",
    "print(f\"Total hits: {results['total_hits']}\")\n",
    "for i, hit in enumerate(results['hits'], 1):\n",
    "    print(f\"{i}. {hit['page_content'][:60]}... (from {hit['source']})\")\n",
    "\n",
    "# Show boolean search capabilities\n",
    "results = whoosh_store.query(\"machine AND learning\", limit=3)\n",
    "print(f\"\\nBoolean search for 'machine AND learning':\")\n",
    "print(f\"Total hits: {results['total_hits']}\")\n",
    "for hit in results['hits']:\n",
    "    print(f\"- {hit['page_content'][:60]}...\")\n",
    "\n",
    "# Test semantic search (uses embeddings on top of keyword results)\n",
    "semantic_results = whoosh_store.semantic_search(\"AI learning\", limit=2)\n",
    "print(f\"\\nSemantic search results for 'AI learning':\")\n",
    "for doc in semantic_results:\n",
    "    print(f\"- {doc.page_content[:60]}... (score: {doc.metadata.get('score', 'N/A'):.3f}, category: {doc.metadata.get('topic', 'N/A')})\")\n",
    "whoosh_store.erase(confirm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: ElasticsearchStore (Hybrid Search)\n",
    "\n",
    "ElasticsearchStore combines both dense and sparse search capabilities in a single unified store. It can perform keyword search, semantic search, and hybrid search that combines both approaches.\n",
    "\n",
    "**Note**: This example requires Elasticsearch to be running. If you don't have Elasticsearch installed, you can skip this section or set it up using Docker:\n",
    "```bash\n",
    "\n",
    "# Or for Elasticsearch 8.x with security disabled:\n",
    "docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" elasticsearch:8.15.5\n",
    "```\n",
    "\n",
    "You can also download Elasticsearch and start it from command-line:\n",
    "\n",
    "```bash\n",
    " ./bin/elasticsearch \n",
    "```\n",
    "\n",
    "When starting Elasticsearch for the first time, make note of the password and set the following dictionary accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "elastic_params = {'persist_location': 'https://localhost:9200', \n",
    "                  'index_name': 'demo_index', \n",
    "                  'verify_certs': True, \n",
    "                  'ca_certs': '/PATH/TO/ELASTIC_FOLDER/elasticsearch-8.15.5/config/certs/http_ca.crt', \n",
    "                  'basic_auth': ('elastic', 'YOUR_PASSWORD')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ElasticsearchStore\n",
      "Store type: ElasticsearchStore\n",
      "Added 6 documents to ElasticsearchStore\n",
      "\n",
      "Keyword search results for 'neural networks':\n",
      "Total hits: 1\n",
      "- Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n",
      "\n",
      "Semantic search results for 'AI algorithms':\n",
      "Total hits: 3\n",
      "- Machine learning is a subset of artificial intelligence that... (score: 0.722216, category: AI)\n",
      "- Vector databases store high-dimensional vectors and enable s... (score: 0.714347, category: databases)\n",
      "- Retrieval-augmented generation (RAG) combines information re... (score: 0.647074, category: AI)\n",
      "\n",
      "Hybrid search results for 'machine learning algorithms':\n",
      "Total hits: 3\n",
      "- Machine learning is a subset of artificial intelligence that... (combined score: 0.841920)\n",
      "- Deep learning uses neural networks with multiple layers to m... (combined score: 0.632784)\n",
      "- Vector databases store high-dimensional vectors and enable s... (combined score: 0.488335)\n",
      "\n",
      "Cleaned up ElasticsearchStore\n"
     ]
    }
   ],
   "source": [
    "  # | notest\n",
    "\n",
    "  # Create ElasticsearchStore using the factory\n",
    "  # Note: This requires Elasticsearch to be running on localhost:9200\n",
    "  try:\n",
    "      elasticsearch_store = VectorStoreFactory.create(\n",
    "          kind='elasticsearch', **elastic_params,\n",
    "      )\n",
    "\n",
    "      print(f\"Created ElasticsearchStore\")\n",
    "      print(f\"Store type: {type(elasticsearch_store).__name__}\")\n",
    "\n",
    "      # Add documents\n",
    "      elasticsearch_store.add_documents(sample_docs)\n",
    "      print(f\"Added {len(sample_docs)} documents to ElasticsearchStore\")\n",
    "\n",
    "      # Test keyword search (sparse)\n",
    "      search_results = elasticsearch_store.search(\"neural networks\", limit=3)\n",
    "      print(f\"\\nKeyword search results for 'neural networks':\")\n",
    "      print(f\"Total hits: {search_results['total_hits']}\")\n",
    "      for hit in search_results['hits']:\n",
    "          print(f\"- {hit['page_content'][:60]}... (from {hit['source']})\")\n",
    "\n",
    "      # Test semantic search (dense) - FIX: use 'hit' not 'doc'\n",
    "      semantic_results = elasticsearch_store.semantic_search(\"AI algorithms\", limit=3)\n",
    "      print(f\"\\nSemantic search results for 'AI algorithms':\")\n",
    "      print(f\"Total hits: {semantic_results['total_hits']}\")\n",
    "      for hit in semantic_results['hits']:\n",
    "          # Show more precision in scores to see if they're actually different\n",
    "          score = hit.get('score', 'N/A')\n",
    "          score_str = f\"{score:.6f}\" if isinstance(score, (int, float)) else str(score)\n",
    "          print(f\"- {hit['page_content'][:60]}... (score: {score_str}, category: {hit.get('topic', 'N/A')})\")\n",
    "\n",
    "      # Test hybrid search (combines both dense and sparse)\n",
    "      hybrid_results = elasticsearch_store.hybrid_search(\n",
    "          \"machine learning algorithms\",\n",
    "          limit=3,\n",
    "          weights=[0.7, 0.3]  # 70% semantic, 30% keyword\n",
    "      )\n",
    "      print(f\"\\nHybrid search results for 'machine learning algorithms':\")\n",
    "      print(f\"Total hits: {hybrid_results['total_hits']}\")\n",
    "      for hit in hybrid_results['hits']:\n",
    "          score = hit.get('score', 'N/A')\n",
    "          score_str = f\"{score:.6f}\" if isinstance(score, (int, float)) else str(score)\n",
    "          print(f\"- {hit['page_content'][:60]}... (combined score: {score_str})\")\n",
    "\n",
    "      # Clean up\n",
    "      elasticsearch_store.erase(confirm=False)\n",
    "      print(f\"\\nCleaned up ElasticsearchStore\")\n",
    "\n",
    "  except Exception as e:\n",
    "      print(f\"ElasticsearchStore example skipped: {e}\")\n",
    "      print(\"Make sure Elasticsearch is running on localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ElasticsearchStore\n",
      "Store type: ElasticsearchStore\n",
      "Added 6 documents to ElasticsearchStore\n",
      "\n",
      "Keyword search results for 'neural networks':\n",
      "Total hits: 1\n",
      "- Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n",
      "\n",
      "Semantic search results for 'AI algorithms':\n",
      "Total hits: 3\n",
      "- Machine learning is a subset of artificial intelligence that... (score: 0.411, category: AI)\n",
      "- Vector databases store high-dimensional vectors and enable s... (score: 0.411, category: AI)\n",
      "- Retrieval-augmented generation (RAG) combines information re... (score: 0.411, category: AI)\n",
      "\n",
      "Hybrid search results for 'machine learning algorithms':\n",
      "Total hits: 3\n",
      "- Machine learning is a subset of artificial intelligence that... (combined score: 0.842)\n",
      "- Deep learning uses neural networks with multiple layers to m... (combined score: 0.633)\n",
      "- Vector databases store high-dimensional vectors and enable s... (combined score: 0.488)\n",
      "\n",
      "Cleaned up ElasticsearchStore\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Create ElasticsearchStore using the factory\n",
    "# Note: This requires Elasticsearch to be running on localhost:9200\n",
    "try:\n",
    "    elasticsearch_store = VectorStoreFactory.create(\n",
    "        kind='elasticsearch',\n",
    "        **elastic_params,\n",
    "    )\n",
    "    \n",
    "    print(f\"Created ElasticsearchStore\")\n",
    "    print(f\"Store type: {type(elasticsearch_store).__name__}\")\n",
    "    \n",
    "    # Add documents\n",
    "    elasticsearch_store.add_documents(sample_docs)\n",
    "    print(f\"Added {len(sample_docs)} documents to ElasticsearchStore\")\n",
    "    \n",
    "    # Test keyword search (sparse)\n",
    "    search_results = elasticsearch_store.search(\"neural networks\", limit=3)\n",
    "    print(f\"\\nKeyword search results for 'neural networks':\")\n",
    "    print(f\"Total hits: {search_results['total_hits']}\")\n",
    "    for hit in search_results['hits']:\n",
    "        print(f\"- {hit['page_content'][:60]}... (from {hit['source']})\")\n",
    "    \n",
    "    # Test semantic search (dense)\n",
    "    semantic_results = elasticsearch_store.semantic_search(\"AI algorithms\", limit=3)\n",
    "    print(f\"\\nSemantic search results for 'AI algorithms':\")\n",
    "    print(f\"Total hits: {semantic_results['total_hits']}\")\n",
    "    for hit in semantic_results['hits']:\n",
    "        print(f\"- {hit['page_content'][:60]}... (score: {doc.metadata.get('score', 'N/A'):.3f}, category: {doc.metadata.get('topic', 'N/A')})\")\n",
    "    \n",
    "    # Test hybrid search (combines both dense and sparse)\n",
    "    hybrid_results = elasticsearch_store.hybrid_search(\n",
    "        \"machine learning algorithms\", \n",
    "        limit=3, \n",
    "        weights=[0.7, 0.3]  # 70% semantic, 30% keyword\n",
    "    )\n",
    "    print(f\"\\nHybrid search results for 'machine learning algorithms':\")\n",
    "    print(f\"Total hits: {hybrid_results['total_hits']}\")\n",
    "    for hit in hybrid_results['hits']:\n",
    "        print(f\"- {hit['page_content'][:60]}... (combined score: {hit.get('score', 'N/A'):.3f})\")\n",
    "    \n",
    "    # Clean up\n",
    "    elasticsearch_store.erase(confirm=False)\n",
    "    print(f\"\\nCleaned up ElasticsearchStore\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ElasticsearchStore example skipped: {e}\")\n",
    "    print(\"Make sure Elasticsearch is running on localhost:9200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with OnPrem.LLM\n",
    "\n",
    "The VectorStoreFactory works seamlessly with OnPrem.LLM for complete RAG (Retrieval-Augmented Generation) workflows.\n",
    "\n",
    "By default, supplying `store_type=\"dense\"` to `LLM` will use `ChromaStore` and supplying `store_type=\"sparse\"` will use  WhooshStore.  To use `ElasticsearchStore`, you can supply it to `load_vectorstore` as a custom vectorstore:\n",
    "\n",
    "```python\n",
    "llm = LLM(...)\n",
    "llm.load_vectorstore(custom_vectorstore=elasticsearch_store)\n",
    "```\n",
    "\n",
    "You can also implement and use your own custom VectorStore instances (by subclassing `DenseStore`, `SparseStore`, or `DualStore`) using whatever vector database backend you like.\n",
    "\n",
    "For illustration purposes, in the example below, we explictly tell `LLM` to use `WhooshStore` as a custom vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Integration with OnPrem.LLM:\n",
      "✓ Created 3 documents in /tmp/tmp309fl1k7\n",
      "Creating new vectorstore at /tmp/my_search_index\n",
      "Loading documents from /tmp/tmp309fl1k7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|█████████████████████| 3/3 [00:00<00:00, 960.23it/s]\n",
      "Processing and chunking 3 new documents: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 3640.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 3 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1914.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n",
      "\n",
      "\n",
      "----RAG EXAMPLE----\n",
      "QUESTION: What are the types of machine learning?\n",
      "\n",
      "The types of machine learning are:\n",
      "\n",
      "1. Supervised learning: uses labeled data.\n",
      "2. Unsupervised learning: finds patterns in unlabeled data.\n",
      "3. Reinforcement learning: learns through trial and error.\n",
      "\n",
      "SOURCES:\n",
      "source #1: /tmp/tmp309fl1k7/ml_types.txt\n",
      "source #2: /tmp/tmp309fl1k7/ai_overview.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Example: Using VectorStoreFactory with LLM for RAG\n",
    "print(\"🤖 Integration with OnPrem.LLM:\")\n",
    "\n",
    "# Create a simple document corpus\n",
    "documents_dir = tempfile.mkdtemp()\n",
    "doc_files = {\n",
    "    \"ai_overview.txt\": \"Artificial intelligence is transforming how we work and live. Machine learning enables computers to learn from data without explicit programming.\",\n",
    "    \"ml_types.txt\": \"There are three main types of machine learning: supervised learning uses labeled data, unsupervised learning finds patterns in unlabeled data, and reinforcement learning learns through trial and error.\",\n",
    "    \"applications.txt\": \"AI applications include natural language processing for text analysis, computer vision for image recognition, and recommendation systems for personalized content.\"\n",
    "}\n",
    "\n",
    "# Write documents to files\n",
    "for filename, content in doc_files.items():\n",
    "    with open(os.path.join(documents_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"✓ Created {len(doc_files)} documents in {documents_dir}\")\n",
    "\n",
    "# Show how to use custom vector store with LLM\n",
    "from onprem import LLM\n",
    "from onprem.ingest.stores import VectorStoreFactory\n",
    "\n",
    "# Create custom vector store\n",
    "store = VectorStoreFactory.create('whoosh', persist_location='/tmp/my_search_index')\n",
    "\n",
    "# Create LLM and use custom vector store\n",
    "llm = LLM('openai/gpt-4o-mini', vectordb_path=tempfile.mkdtemp())\n",
    "llm.load_vectorstore(custom_vectorstore=store)\n",
    "\n",
    "# Ingest documents\n",
    "llm.ingest(documents_dir)\n",
    "\n",
    "print('\\n\\n----RAG EXAMPLE----')\n",
    "# Ask questions\n",
    "question = 'What are the types of machine learning?'\n",
    "print(f'QUESTION: {question}')\n",
    "print()\n",
    "result = llm.ask(question)\n",
    "\n",
    "print('\\n\\nSOURCES:')\n",
    "for i, d in enumerate(result['source_documents']):\n",
    "    print(f\"source #{i+1}: {d.metadata['source']}\")\n",
    "store.erase(confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaned up temporary directories\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# Clean up temporary directories\n",
    "import shutil\n",
    "\n",
    "temp_dirs = [chroma_path, whoosh_path, documents_dir]\n",
    "for temp_dir in temp_dirs:\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(\"🧹 Cleaned up temporary directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
