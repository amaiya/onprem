{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b898e7f",
   "metadata": {},
   "source": [
    "# sk.base\n",
    "\n",
    "> scikit-learn base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp sk.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\"\"\"\n",
    "sk base\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import charset_normalizer as chardet\n",
    "import langdetect\n",
    "\n",
    "DEFAULT_TOKEN_PATTERN = (\n",
    "    r\"\\b[a-zA-Z][a-zA-Z0-9]*(?:[_/&-][a-zA-Z0-9]+)+\\b|\"\n",
    "    r\"\\b\\d*[a-zA-Z][a-zA-Z0-9][a-zA-Z0-9]+\\b\"\n",
    ")\n",
    "\n",
    "NOSPACE_LANGS = [\"zh-cn\", \"zh-tw\", \"ja\"]\n",
    "\n",
    "def detect_encoding(texts, sample_size=32):\n",
    "    if not isinstance(texts, list):\n",
    "        # check for instance of list as bytes are supplied as input\n",
    "        texts = [texts]\n",
    "    lst = [chardet.detect(doc)[\"encoding\"] for doc in texts[:sample_size]]\n",
    "    encoding = max(set(lst), key=lst.count)\n",
    "    # standardize to utf-8 to prevent BERT problems\n",
    "    encoding = \"utf-8\" if encoding.lower() in [\"ascii\", \"utf8\", \"utf-8\"] else encoding\n",
    "    return encoding\n",
    "\n",
    "def decode_by_line(texts, encoding=\"utf-8\", verbose=1):\n",
    "    \"\"\"\n",
    "    ```\n",
    "    Decode text line by line and skip over errors.\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    new_texts = []\n",
    "    skips = 0\n",
    "    num_lines = 0\n",
    "    for doc in texts:\n",
    "        text = \"\"\n",
    "        for line in doc.splitlines():\n",
    "            num_lines += 1\n",
    "            try:\n",
    "                line = line.decode(encoding)\n",
    "            except:\n",
    "                skips += 1\n",
    "                continue\n",
    "            text += line\n",
    "        new_texts.append(text)\n",
    "    pct = round((skips * 1.0 / num_lines) * 100, 1)\n",
    "    if verbose:\n",
    "        print(\"skipped %s lines (%s%%) due to character decoding errors\" % (skips, pct))\n",
    "        if pct > 10:\n",
    "            print(\"If this is too many, try a different encoding\")\n",
    "    return new_texts\n",
    "\n",
    "\n",
    "def detect_lang(texts:list, sample_size:int=32):\n",
    "    \"\"\"\n",
    "    detect language of texts\n",
    "    \"\"\"\n",
    "\n",
    "    # convert sentence pairs\n",
    "    if isinstance(texts, (tuple, list, np.ndarray)) and len(texts) == 2:\n",
    "        texts = [texts[0], texts[1]]\n",
    "    elif (\n",
    "        isinstance(texts, (tuple, list, np.ndarray))\n",
    "        and isinstance(texts[0], (tuple, list, np.ndarray))\n",
    "        and len(texts[0]) == 2\n",
    "    ):\n",
    "        texts = [t[0] for t in texts]\n",
    "\n",
    "    if isinstance(texts, (pd.Series, pd.DataFrame)):\n",
    "        texts = texts.values\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if not isinstance(texts, (list, np.ndarray)):\n",
    "        raise ValueError(\"texts must be a list or NumPy array of strings\")\n",
    "    lst = []\n",
    "    for doc in texts[:sample_size]:\n",
    "        try:\n",
    "            lst.append(langdetect.detect(doc))\n",
    "        except:\n",
    "            continue\n",
    "    if len(lst) == 0:\n",
    "        warnings.warn(\n",
    "            \"Defaulting to English for language detection: could not detect language from documents. \"\n",
    "            + \"This may be due to empty or invalid texts being provided to detect_lang.\"\n",
    "        )\n",
    "        lang = \"en\"\n",
    "    else:\n",
    "        lang = max(set(lst), key=lst.count)\n",
    "    return lang\n",
    "\n",
    "\n",
    "def is_nospace_lang(lang):\n",
    "    return lang in NOSPACE_LANGS\n",
    "\n",
    "def is_chinese(lang, strict=True):\n",
    "    \"\"\"\n",
    "    ```\n",
    "    Args:\n",
    "      lang(str): language code (e.g., en)\n",
    "      strict(bool):  If False, include additional languages due to mistakes on short texts by langdetect\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if strict:\n",
    "        extra_clause = False\n",
    "    else:\n",
    "        extra_clause = lang in [\"ja\", \"ko\"]\n",
    "    return lang is not None and lang.startswith(\"zh-\") or extra_clause\n",
    "\n",
    "def split_chinese(texts):\n",
    "    try:\n",
    "        import jieba\n",
    "    except ImportError:\n",
    "        raise ImportError('Please install jieba: pip install jieba')\n",
    "        \n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    split_texts = []\n",
    "    for doc in texts:\n",
    "        seg_list = jieba.cut(doc, cut_all=False)\n",
    "        seg_list = list(seg_list)\n",
    "        split_texts.append(seg_list)\n",
    "    return [\" \".join(tokens) for tokens in split_texts]\n",
    "\n",
    "def get_random_colors(n, name=\"hsv\", hex_format=True):\n",
    "    \"\"\"Returns a function that maps each index in 0, 1, ..., n-1 to a distinct\n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.\"\"\"\n",
    "    from matplotlib import pyplot as plt\n",
    "    cmap = plt.cm.get_cmap(name, n)\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        color = cmap(i)\n",
    "        if hex_format:\n",
    "            color = rgb2hex(color)\n",
    "        result.append(color)\n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7563b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb628a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38598f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
