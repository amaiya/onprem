{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.sparse\n",
    "\n",
    "> full-text search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Sequence\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from whoosh import index\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.fields import *\n",
    "from whoosh.filedb.filestore import RamStorage\n",
    "from whoosh.qparser import MultifieldParser, OrGroup\n",
    "from whoosh.query import Term, And, Variations\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from onprem.ingest.base import VectorStore\n",
    "from onprem.ingest.helpers import doc_from_dict\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTANT: Metadata fields in langchain_core.documents.Document objects\n",
    "#            (i.e., the input to WSearch.index_documents) should\n",
    "#            ideally match schema fields below, but this is not strictly required.\n",
    "#\n",
    "#            The page_content field is the only truly required field in supplied\n",
    "#            Document objects. All other fields, including dynamic fields, are optional. \n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "DEFAULT_SCHEMA = Schema(\n",
    "    page_content=TEXT(stored=True, analyzer=StemmingAnalyzer()), # REQUIRED with stemming enabled\n",
    "    id=ID(stored=True, unique=True),\n",
    "    source=KEYWORD(stored=True, commas=True), \n",
    "    source_search=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "    filepath=KEYWORD(stored=True, commas=True),\n",
    "    filepath_search=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "    filename=KEYWORD(stored=True),\n",
    "    ocr=BOOLEAN(stored=True),\n",
    "    table=BOOLEAN(stored=True),\n",
    "    markdown=BOOLEAN(stored=True),\n",
    "    page=NUMERIC(stored=True),\n",
    "    document_title=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "    md5=KEYWORD(stored=True),\n",
    "    mimetype=KEYWORD(stored=True),\n",
    "    extension=KEYWORD(stored=True),\n",
    "    filesize=NUMERIC(stored=True),\n",
    "    createdate=DATETIME(stored=True),\n",
    "    modifydate=DATETIME(stored=True),\n",
    "    tags=KEYWORD(stored=True, commas=True),\n",
    "    notes=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "    msg=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "    )\n",
    "DEFAULT_SCHEMA.add(\"*_t\", TEXT(stored=True, analyzer=StemmingAnalyzer()), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_k\", KEYWORD(stored=True, commas=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_b\", BOOLEAN(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_n\", NUMERIC(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_d\", DATETIME(stored=True), glob=True)\n",
    "\n",
    "\n",
    "def default_schema():\n",
    "    schema = DEFAULT_SCHEMA\n",
    "    #if \"raw\" not in schema.stored_names():\n",
    "        #schema.add(\"raw\", TEXT(stored=True))\n",
    "    return schema\n",
    "\n",
    "\n",
    "class SparseStore(VectorStore):\n",
    "    def __init__(self,\n",
    "                persist_directory: Optional[str]=None, \n",
    "                index_name:str = 'myindex',\n",
    "                **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes full-text search engine.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *persist_directory*: path to folder where search index is stored\n",
    "        - *index_name*: name of index\n",
    "        - *embedding_model*: name of sentence-transformers model\n",
    "        - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "        - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                     embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        \"\"\"\n",
    "\n",
    "        self.persist_directory = persist_directory # alias for consistency with DenseStore\n",
    "        self.index_name = index_name\n",
    "        if self.persist_directory and not self.index_name:\n",
    "            raise ValueError('index_name is required if persist_directory is supplied')\n",
    "        if self.persist_directory:\n",
    "            if not index.exists_in(self.persist_directory, indexname=self.index_name):\n",
    "                self.ix = __class__.initialize_index(self.persist_directory, self.index_name)\n",
    "            else:\n",
    "                self.ix = index.open_dir(self.persist_directory, indexname=self.index_name)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"No persist_directory was supplied, so an in-memory only index\"\n",
    "                \"was created using DEFAULT_SCHEMA\"\n",
    "            )\n",
    "            self.ix = RamStorage().create_index(default_schema())\n",
    "        self.init_embedding_model(**kwargs) # stored as self.embeddings\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Get raw index\n",
    "        \"\"\"\n",
    "        return self.ix\n",
    "\n",
    "\n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if documents have been added to search index\n",
    "        \"\"\"\n",
    "        return self.get_size() > 0\n",
    "\n",
    "\n",
    "    def add_documents(self,\n",
    "                      docs: Sequence[Document], # list of LangChain Documents\n",
    "                      limitmb:int=1024, # maximum memory in  megabytes to use\n",
    "                      verbose:bool=True, # Set to False to disable progress bar\n",
    "                      **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Indexes documents. Extra kwargs supplied to `TextStore.ix.writer`.\n",
    "        \"\"\"\n",
    "        writer = self.ix.writer(limitmb=limitmb, **kwargs)\n",
    "        for doc in tqdm(docs, total=len(docs), disable=not verbose):\n",
    "            d = self.doc2dict(doc)\n",
    "            writer.update_document(**d)\n",
    "        writer.commit(optimize=True)\n",
    "\n",
    "\n",
    "    def remove_document(self, value:str, field:str='id'):\n",
    "        \"\"\"\n",
    "        Remove document with corresponding value and field.\n",
    "        Default field is the id field.\n",
    "        \"\"\"\n",
    "        writer = self.ix.writer()\n",
    "        writer.delete_by_term(field, value)\n",
    "        writer.commit(optimize=True)\n",
    "        return\n",
    "\n",
    "\n",
    "    def remove_source(self, source:str, optimize:bool=True):\n",
    "        \"\"\"\n",
    "        remove all documents associated with `source`.\n",
    "        The `source` argument can either be the full path to\n",
    "        document or a parent folder.  In the latter case,\n",
    "        ALL documents in parent folder will be removed.\n",
    "        \"\"\"\n",
    "        return self.delete_by_prefix(source, field='source', optimize=optimize)\n",
    "\n",
    "\n",
    "    def delete_by_prefix(self, prefix:str, field:str, optimize:bool=True):\n",
    "        \"\"\"\n",
    "        Deletes all documents from a Whoosh index where the `source_field` starts with the given prefix.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *prefix*: The prefix to match in the `field`.\n",
    "        - *field*: The name of the field to match against.\n",
    "        - *optimize*: If True, optimize when committing.\n",
    "\n",
    "        **Returns:**\n",
    "        - Number of records deleted\n",
    "\t\t\"\"\"\n",
    "\n",
    "        from whoosh.query import Prefix\n",
    "        with self.ix.searcher() as searcher:\n",
    "            results = searcher.search(Prefix(field, prefix), limit=None)\n",
    "\n",
    "            if results:\n",
    "                writer = self.ix.writer()\n",
    "                for hit in results:\n",
    "                    writer.delete_document(hit.docnum)\n",
    "                writer.commit(optimize=optimize)\n",
    "                return len(results)\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "\n",
    "    def update_documents(self,\n",
    "                         doc_dicts:dict, # dictionary with keys 'page_content', 'source', 'id', etc.\n",
    "                         **kwargs):\n",
    "        \"\"\"\n",
    "        Update a set of documents (doc in index with same ID will be over-written)\n",
    "        \"\"\"\n",
    "        docs = [doc_from_dict(d) for d in doc_dicts]\n",
    "        self.add_documents(docs)\n",
    "\n",
    "\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a generator to iterate through all indexed documents\n",
    "        \"\"\"\n",
    "        return self.ix.searcher().documents()\n",
    "       \n",
    "\n",
    "    def get_doc(self, id:str):\n",
    "        \"\"\"\n",
    "        Get an indexed record by ID\n",
    "        \"\"\"\n",
    "        r = self.query(f'id:{id}', return_dict=True)\n",
    "        return r['hits'][0] if len(r['hits']) > 0 else None\n",
    "\n",
    "\n",
    "    def get_size(self, include_deleted:bool=False) -> int:\n",
    "        \"\"\"\n",
    "        Gets size of index\n",
    "\n",
    "        If include_deleted is True, will include deletd detects (prior to optimization).\n",
    "        \"\"\"\n",
    "        return self.ix.doc_count_all() if include_deleted else self.ix.doc_count()\n",
    "\n",
    "        \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Clears index\n",
    "        \"\"\"\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            msg = (\n",
    "                f\"You are about to remove all documents from the search index.\"\n",
    "                + f\"(Original documents on file system will remain.) Are you sure?\"\n",
    "            )\n",
    "            shall = input(\"%s (Y/n) \" % msg) == \"Y\"\n",
    "        if shall and index.exists_in(\n",
    "            self.persist_directory, indexname=self.index_name\n",
    "        ):\n",
    "            ix = index.create_in(\n",
    "                self.persist_directory,\n",
    "                indexname=self.index_name,\n",
    "                schema=default_schema(),\n",
    "            )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def _preprocess_query(self, query):\n",
    "        \"\"\"\n",
    "        Removes question marks at the end of queries.\n",
    "        This essentially disables using the question mark\n",
    "        wildcard at end of search term so legitimate\n",
    "        questions are not treated differntly depending\n",
    "        on existence of question mark.\n",
    "        \"\"\"\n",
    "        # Replace question marks at the end of the query\n",
    "        if query.endswith('?'):\n",
    "            query = query[:-1]\n",
    "\n",
    "        # Handle quoted phrases with question marks at the end\n",
    "        import re\n",
    "        # Match question marks at the end of words or at the end of quoted phrases\n",
    "        query = re.sub(r'(\\w)\\?([\\s\\\"]|$)', r'\\1\\2', query)\n",
    "        return query\n",
    "\n",
    "\n",
    "    def query(\n",
    "            self,\n",
    "            q: str,\n",
    "            fields: Sequence = [\"page_content\"],\n",
    "            highlight: bool = True,\n",
    "            limit:int=10,\n",
    "            page:int=1,\n",
    "            return_dict:bool=False,\n",
    "            filters:Optional[Dict[str, str]] = None,\n",
    "            where_document:Optional[str]=None,\n",
    "\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Queries the index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *q*: the query string\n",
    "        - *fields*: a list of fields to search\n",
    "        - *highlight*: If True, highlight hits\n",
    "        - *limit*: results per page\n",
    "        - *page*: page of hits to return\n",
    "        - *return_dict*: If True, return list of dictionaries instead of LangChain Document objects\n",
    "        - *filters*: filter results by field values (e.g., {'extension':'pdf'})\n",
    "        - *where_document*: optional query to further filter results\n",
    "        \"\"\"\n",
    "\n",
    "        search_results = []\n",
    "\n",
    "\n",
    "        q = self._preprocess_query(q)\n",
    "\n",
    "        if where_document:\n",
    "            q = f'({q}) AND ({where_document})'\n",
    "\n",
    "        # process filters\n",
    "        combined_filter=None\n",
    "        if filters:\n",
    "            terms = []\n",
    "            for k in filters:\n",
    "                terms.append(Term(k, filters[k]))\n",
    "            combined_filter = And(terms)\n",
    "                   \n",
    "        # process search\n",
    "        with self.ix.searcher() as searcher:\n",
    "            if page == 1:\n",
    "                results = searcher.search(\n",
    "                    MultifieldParser(fields, schema=self.ix.schema, termclass=Variations, group=OrGroup.factory(0.9)).parse(q), limit=limit, filter=combined_filter)\n",
    "            else:\n",
    "                results = searcher.search_page(\n",
    "                    MultifieldParser(fields, schema=self.ix.schema, termclass=Variations, group=OrGroup.factory(0.9)).parse(q), page, limit, filter=combined_filter)\n",
    "            total_hits = results.scored_length()\n",
    "            if page > math.ceil(total_hits/limit):\n",
    "               results = []\n",
    "            for r in results:\n",
    "                #d = json.loads(r[\"raw\"])\n",
    "                d = dict(r)\n",
    "                if highlight:\n",
    "                    for f in fields:\n",
    "                        if r[f] and isinstance(r[f], str):\n",
    "                            d['hl_'+f] = r.highlights(f) or r[f]\n",
    "                d = d if return_dict else doc_from_dict(d)\n",
    "                search_results.append(d)\n",
    "   \n",
    "        return {'hits':search_results, 'total_hits':total_hits}\n",
    "\n",
    "    def semantic_search(self, \n",
    "                        query, \n",
    "                        k:int=4, # number of records to return based on highest semantic similarity scores.\n",
    "                        n_candidates=50, # Number of records to consider (for which we compute embeddings on-the-fly)\n",
    "                        filters:Optional[Dict[str, str]] = None, # filter sources by field values (e.g., {'table':True})\n",
    "                        where_document:Optional[str]=None, # a boolean query to filter results further (e.g., \"climate\" AND extension:pdf)\n",
    "                        **kwargs):\n",
    "        \"\"\"\n",
    "        Retrieves results based on semantic similarity to supplied `query`.\n",
    "        \"\"\"\n",
    "        from sentence_transformers import util\n",
    "        import torch\n",
    "\n",
    "        results = self.query(query, limit=n_candidates, return_dict=True, filters=filters, where_document=where_document)['hits']\n",
    "        if not results: return []\n",
    "        texts = [r['page_content'] for r in results]\n",
    "        embeddings = self.get_embedding_model()\n",
    "\n",
    "        # Compute embeddings\n",
    "        query_emb = torch.tensor(embeddings.embed_query(query)).unsqueeze(0)  # Shape (1, embedding_dim)\n",
    "        text_embs = torch.tensor(embeddings.embed_documents(texts))  # Shape (len(texts), embedding_dim)\n",
    "    \n",
    "        # Compute cosine similarity\n",
    "        cos_scores = util.pytorch_cos_sim(query_emb, text_embs).squeeze(0).tolist()  # Shape (len(texts),)\n",
    "\n",
    "        # Assign scores back to results\n",
    "        for i, score in enumerate(cos_scores):\n",
    "            results[i]['score'] = score\n",
    "\n",
    "        # Sort results by similarity in descending order\n",
    "        sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)[:k]\n",
    "        return [doc_from_dict(r) for r in sorted_results]\n",
    "              \n",
    "        \n",
    "    @classmethod\n",
    "    def index_exists_in(cls, index_path: str, index_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Returns True if index exists with name, *indexname*, and path, *index_path*.\n",
    "        \"\"\"\n",
    "        return index.exists_in(index_path, indexname=index_name)\n",
    "\n",
    "    @classmethod\n",
    "    def initialize_index(\n",
    "        cls, index_path: str, index_name: str, schema: Optional[Schema] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *index_path*: path to folder storing search index\n",
    "        - *index_name*: name of index\n",
    "        - *schema*: optional whoosh.fields.Schema object.\n",
    "                    If None, DEFAULT_SCHEMA is used\n",
    "        \"\"\"\n",
    "        schema = default_schema() if not schema else schema\n",
    "\n",
    "        if index.exists_in(index_path, indexname=index_name):\n",
    "            raise ValueError(\n",
    "                f\"There is already an existing index named {index_name}  with path {index_path} \\n\"\n",
    "                + f\"Delete {index_path} manually and try again.\"\n",
    "            )\n",
    "        if not os.path.exists(index_path):\n",
    "            os.makedirs(index_path)\n",
    "        ix = index.create_in(index_path, indexname=index_name, schema=schema)\n",
    "        return ix\n",
    "\n",
    "    def doc2dict(self, doc:Document):\n",
    "        \"\"\"\n",
    "        Convert LangChain Document to expected format\n",
    "        \"\"\"\n",
    "        stored_names = self.ix.schema.stored_names()\n",
    "        d = {}\n",
    "        for k,v in doc.metadata.items():\n",
    "            suffix = None\n",
    "            if k in stored_names:\n",
    "                suffix = ''\n",
    "            elif isinstance(v, bool):\n",
    "                suffix = '_b' if not k.endswith('_b') else ''\n",
    "            elif isinstance(v, str):\n",
    "                if k.endswith('_date'):\n",
    "                    suffix = '_d'\n",
    "                else:\n",
    "                    suffix = '_k'if not k.endswith('_k') else ''\n",
    "            elif isinstance(v, (int, float)):\n",
    "                suffix = '_n'if not k.endswith('_n') else ''\n",
    "            if suffix is not None:\n",
    "                d[k+suffix] = v\n",
    "        d['id'] = uuid.uuid4().hex if not doc.metadata.get('id', '') else doc.metadata['id']\n",
    "        d['page_content' ] = doc.page_content\n",
    "        #d['raw'] = json.dumps(d)\n",
    "        if 'source' in d:\n",
    "            d['source_search'] = d['source']\n",
    "        if 'filepath' in d:\n",
    "            d['filepath_search'] = d['filepath']\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.get_db\n",
       "\n",
       ">      SparseStore.get_db ()\n",
       "\n",
       "*Get raw index*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.get_db\n",
       "\n",
       ">      SparseStore.get_db ()\n",
       "\n",
       "*Get raw index*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.get_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.exists\n",
       "\n",
       ">      SparseStore.exists ()\n",
       "\n",
       "*Returns True if documents have been added to search index*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.exists\n",
       "\n",
       ">      SparseStore.exists ()\n",
       "\n",
       "*Returns True if documents have been added to search index*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L126){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.add_documents\n",
       "\n",
       ">      SparseStore.add_documents\n",
       ">                                 (docs:Sequence[langchain_core.documents.base.D\n",
       ">                                 ocument], limitmb:int=1024, verbose:bool=True,\n",
       ">                                 **kwargs)\n",
       "\n",
       "*Indexes documents. Extra kwargs supplied to `TextStore.ix.writer`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| docs | Sequence |  | list of LangChain Documents |\n",
       "| limitmb | int | 1024 | maximum memory in  megabytes to use |\n",
       "| verbose | bool | True | Set to False to disable progress bar |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L126){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.add_documents\n",
       "\n",
       ">      SparseStore.add_documents\n",
       ">                                 (docs:Sequence[langchain_core.documents.base.D\n",
       ">                                 ocument], limitmb:int=1024, verbose:bool=True,\n",
       ">                                 **kwargs)\n",
       "\n",
       "*Indexes documents. Extra kwargs supplied to `TextStore.ix.writer`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| docs | Sequence |  | list of LangChain Documents |\n",
       "| limitmb | int | 1024 | maximum memory in  megabytes to use |\n",
       "| verbose | bool | True | Set to False to disable progress bar |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L142){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.remove_document\n",
       "\n",
       ">      SparseStore.remove_document (value:str, field:str='id')\n",
       "\n",
       "*Remove document with corresponding value and field.\n",
       "Default field is the id field.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L142){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.remove_document\n",
       "\n",
       ">      SparseStore.remove_document (value:str, field:str='id')\n",
       "\n",
       "*Remove document with corresponding value and field.\n",
       "Default field is the id field.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.remove_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L152){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.remove_source\n",
       "\n",
       ">      SparseStore.remove_source (source:str)\n",
       "\n",
       "*remove all documents associated with `source1.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L152){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.remove_source\n",
       "\n",
       ">      SparseStore.remove_source (source:str)\n",
       "\n",
       "*remove all documents associated with `source1.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.remove_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L153){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.update_documents\n",
       "\n",
       ">      SparseStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update a set of documents (doc in index with same ID will be over-written)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| doc_dicts | dict | dictionary with keys 'page_content', 'source', 'id', etc. |\n",
       "| kwargs | VAR_KEYWORD |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L153){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.update_documents\n",
       "\n",
       ">      SparseStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update a set of documents (doc in index with same ID will be over-written)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| doc_dicts | dict | dictionary with keys 'page_content', 'source', 'id', etc. |\n",
       "| kwargs | VAR_KEYWORD |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.update_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L163){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.get_all_docs\n",
       "\n",
       ">      SparseStore.get_all_docs ()\n",
       "\n",
       "*Returns a generator to iterate through all indexed documents*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L163){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.get_all_docs\n",
       "\n",
       ">      SparseStore.get_all_docs ()\n",
       "\n",
       "*Returns a generator to iterate through all indexed documents*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.get_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L170){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.get_doc\n",
       "\n",
       ">      SparseStore.get_doc (id:str)\n",
       "\n",
       "*Get an indexed record by ID*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L170){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.get_doc\n",
       "\n",
       ">      SparseStore.get_doc (id:str)\n",
       "\n",
       "*Get an indexed record by ID*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.get_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L178){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.get_size\n",
       "\n",
       ">      SparseStore.get_size ()\n",
       "\n",
       "*Gets size of index*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L178){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.get_size\n",
       "\n",
       ">      SparseStore.get_size ()\n",
       "\n",
       "*Gets size of index*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.get_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L185){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.erase\n",
       "\n",
       ">      SparseStore.erase (confirm=True)\n",
       "\n",
       "*Clears index*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L185){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.erase\n",
       "\n",
       ">      SparseStore.erase (confirm=True)\n",
       "\n",
       "*Clears index*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L215){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.query\n",
       "\n",
       ">      SparseStore.query (q:str, fields:Sequence=['page_content'],\n",
       ">                         highlight:bool=True, limit:int=10, page:int=1,\n",
       ">                         return_dict:bool=False,\n",
       ">                         filters:Optional[Dict[str,str]]=None,\n",
       ">                         where_document:Optional[str]=None)\n",
       "\n",
       "*Queries the index\n",
       "\n",
       "**Args**\n",
       "\n",
       "- *q*: the query string\n",
       "- *fields*: a list of fields to search\n",
       "- *highlight*: If True, highlight hits\n",
       "- *limit*: results per page\n",
       "- *page*: page of hits to return\n",
       "- *return_dict*: If True, return list of dictionaries instead of LangChain Document objects\n",
       "- *filters*: filter results by field values (e.g., {'extension':'pdf'})\n",
       "- *where_document*: optional query to further filter results*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L215){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.query\n",
       "\n",
       ">      SparseStore.query (q:str, fields:Sequence=['page_content'],\n",
       ">                         highlight:bool=True, limit:int=10, page:int=1,\n",
       ">                         return_dict:bool=False,\n",
       ">                         filters:Optional[Dict[str,str]]=None,\n",
       ">                         where_document:Optional[str]=None)\n",
       "\n",
       "*Queries the index\n",
       "\n",
       "**Args**\n",
       "\n",
       "- *q*: the query string\n",
       "- *fields*: a list of fields to search\n",
       "- *highlight*: If True, highlight hits\n",
       "- *limit*: results per page\n",
       "- *page*: page of hits to return\n",
       "- *return_dict*: If True, return list of dictionaries instead of LangChain Document objects\n",
       "- *filters*: filter results by field values (e.g., {'extension':'pdf'})\n",
       "- *where_document*: optional query to further filter results*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L281){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.semantic_search\n",
       "\n",
       ">      SparseStore.semantic_search (query, k:int=4, n_candidates=50,\n",
       ">                                   filters:Optional[Dict[str,str]]=None,\n",
       ">                                   where_document:Optional[str]=None, **kwargs)\n",
       "\n",
       "*Retrieves results based on semantic similarity to supplied `query`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query |  |  |  |\n",
       "| k | int | 4 | number of records to return based on highest semantic similarity scores. |\n",
       "| n_candidates | int | 50 | Number of records to consider (for which we compute embeddings on-the-fly) |\n",
       "| filters | Optional | None | filter sources by field values (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | a boolean query to filter results further (e.g., \"climate\" AND extension:pdf) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L281){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.semantic_search\n",
       "\n",
       ">      SparseStore.semantic_search (query, k:int=4, n_candidates=50,\n",
       ">                                   filters:Optional[Dict[str,str]]=None,\n",
       ">                                   where_document:Optional[str]=None, **kwargs)\n",
       "\n",
       "*Retrieves results based on semantic similarity to supplied `query`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query |  |  |  |\n",
       "| k | int | 4 | number of records to return based on highest semantic similarity scores. |\n",
       "| n_candidates | int | 50 | Number of records to consider (for which we compute embeddings on-the-fly) |\n",
       "| filters | Optional | None | filter sources by field values (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | a boolean query to filter results further (e.g., \"climate\" AND extension:pdf) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
