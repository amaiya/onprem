{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.sparse\n",
    "\n",
    "> full-text search engine as a backend vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from onprem.ingest.stores.base import VectorStore\n",
    "from onprem.ingest.helpers import doc_from_dict\n",
    "N_CANDIDATES = 12\n",
    "\n",
    "class SparseStore(VectorStore):\n",
    "    \"\"\"\n",
    "    A factory for built-in SparseStore instances.   \n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        if type(self) is SparseStore:\n",
    "            raise TypeError(\"Use the SparseStore.create() method instead of instantiating SparseStore directly.\")\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, persist_location=None, kind=None, **kwargs) -> 'SparseStore':\n",
    "        \"\"\"\n",
    "        Factory method to construct a `SparseStore` instance.       \n",
    "        Extra kwargs passed to object instantiation.\n",
    "        \n",
    "        Args:\n",
    "            persist_location: where the index is stored (for whoosh) or Elasticsearch URL (for elasticsearch)\n",
    "            kind: one of {whoosh, elasticsearch}\n",
    "            \n",
    "        Elasticsearch-specific kwargs:\n",
    "            basic_auth: tuple of (username, password) for basic authentication\n",
    "            verify_certs: whether to verify SSL certificates (default: True)\n",
    "            ca_certs: path to CA certificate file\n",
    "            timeout: connection timeout in seconds (default: 30, becomes request_timeout for v9+ compatibility)\n",
    "            max_retries: maximum number of retries (default: 3)\n",
    "            retry_on_timeout: whether to retry on timeout (default: True)\n",
    "            maxsize: maximum number of connections in the pool (default: 25, removed for Elasticsearch v9+ compatibility)\n",
    "\n",
    "        Returns:\n",
    "            SparseStore instance\n",
    "        \"\"\"\n",
    "        \n",
    "        kind = 'whoosh' if not kind else kind\n",
    "        \n",
    "        if kind == 'whoosh':\n",
    "            return WhooshStore(persist_location=persist_location, **kwargs)\n",
    "        elif kind == 'elasticsearch':\n",
    "            if not ELASTICSEARCH_INSTALLED:\n",
    "                raise ImportError(\"Please install the elasticsearch package version for your \"\n",
    "                                  \"Elasticsearch instance: e.g., pip install elasticsearch==9\")\n",
    "            return ElasticsearchSparseStore(persist_location=persist_location, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SparseStore type: {kind}\")\n",
    "\n",
    "\n",
    "\n",
    "    def _is_boolean_or_phrase_query(self, query: str) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if the query looks like a Boolean or phrase query.\n",
    "        \"\"\"\n",
    "        # Check for quoted phrases\n",
    "        if re.search(r'\"[^\"]+\"', query):\n",
    "            return True\n",
    "\n",
    "        # Check for Boolean operators (case-insensitive)\n",
    "        if re.search(r'\\b(AND|OR|NOT)\\b', query, re.IGNORECASE):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def augment_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Augments a natural language query with extracted noun phrases,\n",
    "        unless the query already looks like a Boolean or phrase query.\n",
    "        \"\"\"\n",
    "        from onprem.utils import extract_noun_phrases\n",
    "\n",
    "        if self._is_boolean_or_phrase_query(query):\n",
    "            return query  # Don't modify Boolean/phrase queries\n",
    "\n",
    "        noun_phrases = extract_noun_phrases(query)\n",
    "        if not noun_phrases:\n",
    "            return query\n",
    "\n",
    "        quoted_nps = [f'\"{np}\"^2.0' for np in noun_phrases]\n",
    "        or_clause = \" OR \".join(quoted_nps)\n",
    "\n",
    "        return f\"({query}) OR ({or_clause})\"\n",
    "\n",
    "    def _preprocess_query(self, query):\n",
    "        \"\"\"\n",
    "        Removes question marks at the end of queries.\n",
    "        This essentially disables using the question mark\n",
    "        wildcard at end of search term so legitimate\n",
    "        questions are not treated differntly depending\n",
    "        on existence of question mark.\n",
    "        \"\"\"\n",
    "        # Replace question marks at the end of the query\n",
    "        if query.endswith('?'):\n",
    "            query = query[:-1]\n",
    "\n",
    "        # Handle quoted phrases with question marks at the end\n",
    "        import re\n",
    "        # Match question marks at the end of words or at the end of quoted phrases\n",
    "        query = re.sub(r'(\\w)\\?([\\s\\\"]|$)', r'\\1\\2', query)\n",
    "        return query\n",
    "\n",
    "    \n",
    "    def get_content_field(self):\n",
    "        \"\"\"\n",
    "        Get content field name (default to 'page_content')\n",
    "        \"\"\"\n",
    "        return getattr(self, 'content_field', 'page_content')\n",
    "\n",
    "        \n",
    "    def semantic_search(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Any subclass of SparseStore can inherit this method for on-the-fly semantic searches.\n",
    "        Retrieves results based on semantic similarity to supplied `query`.\n",
    "        All arguments are fowrwarded on to the store's search method.\n",
    "        The search method is expected to include \"query\" as first positional argument and a \"limit\" keyword argument.\n",
    "        Additional kwargs can be supplied to focus the search (e.g., see `where_document` and `filters` arguments of search method).\n",
    "        Results of invoked search method are expected to be in the form: {'hits': list_of_dicts, 'total_hits' : int}.\n",
    "        Result of this method is a list of  LangChain Document objects sorted by semantic similarity.\n",
    "        \n",
    "        If subclass supports dynamic chunking (has chunk_for_semantic_search=True), \n",
    "        it will chunk large documents and find the best matching chunks per document.\n",
    "        \n",
    "        Args:\n",
    "            return_chunks (bool): If True (default), return individual chunks as Document objects for RAG.\n",
    "                                 If False, return original documents with full content and all chunk scores.\n",
    "            load_web_documents (bool): If True, attempt to load content from web URLs when content field is empty (default: False).\n",
    "            verbose (bool): If True (default), show progress bar during semantic search processing.\n",
    "        \"\"\"\n",
    "        args = list(args)\n",
    "        query = args[0] # for semantic search\n",
    "        args[0] = self.augment_query(args[0]) # for keyword search\n",
    "        args = tuple(args)\n",
    "\n",
    "        limit = kwargs.get('limit', 4)\n",
    "        n_candidates = kwargs.pop('n_candidates', getattr(self, 'n_candidates', limit * 10)) # check instance attribute first\n",
    "        n_candidates = n_candidates if n_candidates and n_candidates>0 else limit*10 # if n_candidates is 0 or None\n",
    "        return_chunks = kwargs.pop('return_chunks', True)  # Default True for RAG optimization\n",
    "        load_web_documents = kwargs.pop('load_web_documents', getattr(self, 'load_web_documents', False))  # Check instance attribute first\n",
    "        verbose = kwargs.pop('verbose', True)  # Progress bar enabled by default\n",
    "        \n",
    "        # Create a copy of kwargs to avoid modifying the original\n",
    "        query_kwargs = kwargs.copy()\n",
    "        query_kwargs['limit'] = n_candidates\n",
    "        \n",
    "        results = self.search(*args, **query_kwargs)['hits']\n",
    "        \n",
    "        # If no results but we have filters/where_document, try a broader search\n",
    "        if not results and (kwargs.get('filters') or kwargs.get('where_document')):\n",
    "            # Try with a wildcard query to get all documents, then filter\n",
    "            wildcard_args = ('*',) + args[1:]  # Replace query with wildcard\n",
    "            results = self.search(*wildcard_args, **query_kwargs)['hits']\n",
    "        \n",
    "        if not results: return []\n",
    "        \n",
    "        # Load web documents if enabled and content is empty\n",
    "        if load_web_documents:\n",
    "            results = self._load_web_documents_if_needed(results, verbose=verbose)\n",
    "        \n",
    "        # Check if subclass supports dynamic chunking\n",
    "        if hasattr(self, 'chunk_for_semantic_search') and self.chunk_for_semantic_search:\n",
    "            return self._semantic_search_with_chunking(query, results, limit, return_chunks, verbose=verbose)\n",
    "        else:\n",
    "            return self._semantic_search_original(query, results, limit, return_chunks, verbose=verbose)\n",
    "    \n",
    "    def _semantic_search_original(self, query, results, limit, return_chunks=True, verbose=True):\n",
    "        \"\"\"Original semantic search implementation without chunking.\n",
    "        \n",
    "        Args:\n",
    "            return_chunks (bool): Included for API consistency. Since this method doesn't chunk,\n",
    "                                 this parameter doesn't affect behavior.\n",
    "        \"\"\"\n",
    "        texts = [r[self.get_content_field()] or '' for r in results]\n",
    "\n",
    "        cos_scores = self.compute_similarity(query, texts)\n",
    "\n",
    "        # Assign scores back to results\n",
    "        for i, score in enumerate(cos_scores):\n",
    "            results[i]['score'] = score\n",
    "\n",
    "        # Sort results by similarity in descending order\n",
    "        sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)[:limit]\n",
    "        return [doc_from_dict(r) for r in sorted_results]\n",
    "    \n",
    "    def _semantic_search_with_chunking(self, query, results, limit, return_chunks=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Enhanced semantic search that dynamically chunks large documents.\n",
    "        Subclasses can override this method to customize chunking behavior.\n",
    "        \n",
    "        Args:\n",
    "            return_chunks (bool): If True, return individual chunks as Document objects for RAG.\n",
    "                                 If False, return original documents with full content and all chunk scores.\n",
    "        \"\"\"\n",
    "        from ..base import chunk_documents\n",
    "        \n",
    "        content_field = self.get_content_field()\n",
    "        chunk_size = getattr(self, 'chunk_size', 500)\n",
    "        chunk_overlap = getattr(self, 'chunk_overlap', 50)\n",
    "        \n",
    "        # Process each document and its chunks\n",
    "        doc_chunk_data = []\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc_idx, doc in enumerate(results):\n",
    "            # Extract text and use chunk_documents to split it\n",
    "            doc_text = doc[content_field] or ''\n",
    "            chunk_docs = chunk_documents(\n",
    "                [doc_text],  # Pass text string directly\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            \n",
    "            # Store mapping of chunks to original document\n",
    "            for chunk_idx, chunk_doc in enumerate(chunk_docs):\n",
    "                doc_chunk_data.append({\n",
    "                    'doc_idx': doc_idx,\n",
    "                    'chunk_idx': chunk_idx,\n",
    "                    'chunk_text': chunk_doc.page_content,\n",
    "                    'original_doc': doc\n",
    "                })\n",
    "                all_chunks.append(chunk_doc.page_content)\n",
    "        \n",
    "        if not all_chunks:\n",
    "            return []\n",
    "        \n",
    "        # Compute similarity for all chunks\n",
    "        cos_scores = self.compute_similarity(query, all_chunks)\n",
    "        \n",
    "        # Store chunk scores and find best chunk for each document\n",
    "        doc_best_scores = {}\n",
    "        doc_best_chunks = {}\n",
    "        chunk_score_data = []  # Store all chunks with their scores\n",
    "        \n",
    "        for i, chunk_data in enumerate(doc_chunk_data):\n",
    "            doc_idx = chunk_data['doc_idx']\n",
    "            score = cos_scores[i]\n",
    "            \n",
    "            # Store chunk with score for potential individual return\n",
    "            chunk_score_data.append({\n",
    "                'doc_idx': doc_idx,\n",
    "                'chunk_idx': chunk_data['chunk_idx'],\n",
    "                'chunk_text': chunk_data['chunk_text'],\n",
    "                'score': score,\n",
    "                'original_doc': chunk_data['original_doc']\n",
    "            })\n",
    "            \n",
    "            # Keep track of best chunk per document\n",
    "            if doc_idx not in doc_best_scores or score > doc_best_scores[doc_idx]:\n",
    "                doc_best_scores[doc_idx] = score\n",
    "                doc_best_chunks[doc_idx] = {\n",
    "                    'chunk_text': chunk_data['chunk_text'],\n",
    "                    'chunk_idx': chunk_data['chunk_idx'],\n",
    "                    'score': score\n",
    "                }\n",
    "        \n",
    "        if return_chunks:\n",
    "            # Return individual chunks as Document objects, sorted by score\n",
    "            chunk_score_data.sort(key=lambda x: x['score'], reverse=True)\n",
    "            chunk_score_data = chunk_score_data[:limit]\n",
    "            \n",
    "            documents = []\n",
    "            for chunk_data in chunk_score_data:\n",
    "                # Create metadata from original document and add chunk-specific info\n",
    "                metadata = chunk_data['original_doc'].copy()\n",
    "                metadata['score'] = chunk_data['score']\n",
    "                metadata['chunk_idx'] = chunk_data['chunk_idx']\n",
    "                metadata['is_chunk'] = True\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=chunk_data['chunk_text'],\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            \n",
    "            return documents\n",
    "        else:\n",
    "            # Return original documents with scores from best chunks (backward compatibility)\n",
    "            final_results = []\n",
    "            for doc_idx, best_chunk in doc_best_chunks.items():\n",
    "                doc = results[doc_idx].copy()\n",
    "                doc['score'] = best_chunk['score']\n",
    "                \n",
    "                # Get all chunks for this document with their scores\n",
    "                doc_chunks_with_scores = []\n",
    "                for chunk_data in chunk_score_data:\n",
    "                    if chunk_data['doc_idx'] == doc_idx:\n",
    "                        doc_chunks_with_scores.append({\n",
    "                            'text': chunk_data['chunk_text'],\n",
    "                            'idx': chunk_data['chunk_idx'],\n",
    "                            'score': chunk_data['score']\n",
    "                        })\n",
    "                \n",
    "                # Sort chunks by their original order (by chunk_idx)\n",
    "                doc_chunks_with_scores.sort(key=lambda x: x['idx'])\n",
    "                \n",
    "                # Extract parallel arrays\n",
    "                doc_chunks = [item['text'] for item in doc_chunks_with_scores]\n",
    "                doc_chunk_scores = [item['score'] for item in doc_chunks_with_scores]\n",
    "                \n",
    "                # Add metadata about all chunks and the best matching chunk\n",
    "                doc['best_chunk_text'] = best_chunk['chunk_text']\n",
    "                doc['best_chunk_idx'] = best_chunk['chunk_idx']\n",
    "                doc['total_chunks'] = len(doc_chunks)\n",
    "                doc['chunk_scores'] = doc_chunk_scores  # Parallel list of scores\n",
    "                \n",
    "                final_results.append((doc, doc_chunks))\n",
    "            \n",
    "            # Sort by best chunk score and limit\n",
    "            final_results.sort(key=lambda x: x[0]['score'], reverse=True)\n",
    "            final_results = final_results[:limit]\n",
    "            \n",
    "            # Create Document objects with chunks as content\n",
    "            documents = []\n",
    "            for doc_data, chunks in final_results:\n",
    "                # Store original chunks list in metadata\n",
    "                doc_data['chunks'] = chunks\n",
    "                doc = Document(\n",
    "                    page_content='\\n\\n'.join(chunks),  # Join chunks for LangChain compatibility\n",
    "                    metadata=doc_data\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            \n",
    "            return documents\n",
    "    \n",
    "    def _load_web_documents_if_needed(self, results, verbose=True):\n",
    "        \"\"\"\n",
    "        Load content from web URLs when content field is empty.\n",
    "        Only processes URLs that start with 'http://' or 'https://'.\n",
    "        \"\"\"\n",
    "        from ..base import load_single_document\n",
    "        import tempfile\n",
    "        import os\n",
    "        import requests\n",
    "        from urllib.parse import urlparse\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        content_field = self.get_content_field()\n",
    "        updated_results = []\n",
    "        \n",
    "        # Count documents that need web loading for progress bar\n",
    "        web_docs_needed = sum(1 for result in results \n",
    "                             if not result.get(content_field, '').strip() \n",
    "                             and result.get('source', '').strip() \n",
    "                             and self._is_web_url(result.get('source', '')))\n",
    "        \n",
    "        if web_docs_needed > 0 and verbose:\n",
    "            progress_bar = tqdm(results, desc=f\"Loading {web_docs_needed} web documents\", total=len(results))\n",
    "        else:\n",
    "            progress_bar = results\n",
    "        \n",
    "        for result in progress_bar:\n",
    "            # Check if content is empty or missing\n",
    "            content = result.get(content_field, '').strip()\n",
    "            source = result.get('source', '').strip()\n",
    "            \n",
    "            if not content and source and self._is_web_url(source):\n",
    "                try:\n",
    "                    # Download the document from the web URL\n",
    "                    loaded_docs = self._load_web_document(source)\n",
    "                    if loaded_docs:\n",
    "                        # Concatenate content from all pages/documents\n",
    "                        result[content_field] = '\\n\\n'.join([doc.page_content for doc in loaded_docs])\n",
    "                        # Optionally merge metadata\n",
    "                        if hasattr(loaded_docs[0], 'metadata'):\n",
    "                            for key, value in loaded_docs[0].metadata.items():\n",
    "                                if key not in result:\n",
    "                                    result[key] = value\n",
    "                except Exception as e:\n",
    "                    # Log warning but continue with empty content\n",
    "                    import warnings\n",
    "                    warnings.warn(f\"Failed to load web document from {source}: {str(e)}\")\n",
    "            \n",
    "            updated_results.append(result)\n",
    "        \n",
    "        return updated_results\n",
    "    \n",
    "    def _is_web_url(self, url):\n",
    "        \"\"\"\n",
    "        Check if the given URL is a web URL (starts with http:// or https://).\n",
    "        \"\"\"\n",
    "        return url.startswith(('http://', 'https://'))\n",
    "    \n",
    "    def _load_web_document(self, url):\n",
    "        \"\"\"\n",
    "        Download and extract text from a web document using load_single_document.\n",
    "        \"\"\"\n",
    "        from ..base import load_web_document\n",
    "        \n",
    "        # Get credentials from SharePointStore instance if available\n",
    "        username = getattr(self, 'username', None)\n",
    "        password = getattr(self, 'password', None)\n",
    "        \n",
    "        return load_web_document(url, username=username, password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L22){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.create\n",
       "\n",
       ">      SparseStore.create (persist_location=None, kind=None, **kwargs)\n",
       "\n",
       "*Factory method to construct a `SparseStore` instance.       \n",
       "Extra kwargs passed to object instantiation.\n",
       "\n",
       "Args:\n",
       "    persist_location: where the index is stored (for whoosh) or Elasticsearch URL (for elasticsearch)\n",
       "    kind: one of {whoosh, elasticsearch}\n",
       "\n",
       "Elasticsearch-specific kwargs:\n",
       "    basic_auth: tuple of (username, password) for basic authentication\n",
       "    verify_certs: whether to verify SSL certificates (default: True)\n",
       "    ca_certs: path to CA certificate file\n",
       "    timeout: connection timeout in seconds (default: 30, becomes request_timeout for v9+ compatibility)\n",
       "    max_retries: maximum number of retries (default: 3)\n",
       "    retry_on_timeout: whether to retry on timeout (default: True)\n",
       "    maxsize: maximum number of connections in the pool (default: 25, removed for Elasticsearch v9+ compatibility)\n",
       "\n",
       "Returns:\n",
       "    SparseStore instance*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L22){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.create\n",
       "\n",
       ">      SparseStore.create (persist_location=None, kind=None, **kwargs)\n",
       "\n",
       "*Factory method to construct a `SparseStore` instance.       \n",
       "Extra kwargs passed to object instantiation.\n",
       "\n",
       "Args:\n",
       "    persist_location: where the index is stored (for whoosh) or Elasticsearch URL (for elasticsearch)\n",
       "    kind: one of {whoosh, elasticsearch}\n",
       "\n",
       "Elasticsearch-specific kwargs:\n",
       "    basic_auth: tuple of (username, password) for basic authentication\n",
       "    verify_certs: whether to verify SSL certificates (default: True)\n",
       "    ca_certs: path to CA certificate file\n",
       "    timeout: connection timeout in seconds (default: 30, becomes request_timeout for v9+ compatibility)\n",
       "    max_retries: maximum number of retries (default: 3)\n",
       "    retry_on_timeout: whether to retry on timeout (default: True)\n",
       "    maxsize: maximum number of connections in the pool (default: 25, removed for Elasticsearch v9+ compatibility)\n",
       "\n",
       "Returns:\n",
       "    SparseStore instance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L116){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.semantic_search\n",
       "\n",
       ">      SparseStore.semantic_search (*args, **kwargs)\n",
       "\n",
       "*Any subclass of SparseStore can inherit this method for on-the-fly semantic searches.\n",
       "Retrieves results based on semantic similarity to supplied `query`.\n",
       "All arguments are fowrwarded on to the store's query method.\n",
       "The query method is expected to include \"query\" as first positional argument and a \"limit\" keyword argument.\n",
       "Results of query method are expected to be in the form: {'hits': list_of_dicts, 'total_hits' : int}.\n",
       "Result of this method is a list of  LangChain Document objects sorted by semantic similarity.\n",
       "\n",
       "If subclass supports dynamic chunking (has chunk_for_semantic_search=True), \n",
       "it will chunk large documents and find the best matching chunks per document.\n",
       "\n",
       "Args:\n",
       "    return_chunks (bool): If True (default), return individual chunks as Document objects for RAG.\n",
       "                         If False, return original documents with full content and all chunk scores.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L116){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparseStore.semantic_search\n",
       "\n",
       ">      SparseStore.semantic_search (*args, **kwargs)\n",
       "\n",
       "*Any subclass of SparseStore can inherit this method for on-the-fly semantic searches.\n",
       "Retrieves results based on semantic similarity to supplied `query`.\n",
       "All arguments are fowrwarded on to the store's query method.\n",
       "The query method is expected to include \"query\" as first positional argument and a \"limit\" keyword argument.\n",
       "Results of query method are expected to be in the form: {'hits': list_of_dicts, 'total_hits' : int}.\n",
       "Result of this method is a list of  LangChain Document objects sorted by semantic similarity.\n",
       "\n",
       "If subclass supports dynamic chunking (has chunk_for_semantic_search=True), \n",
       "it will chunk large documents and find the best matching chunks per document.\n",
       "\n",
       "Args:\n",
       "    return_chunks (bool): If True (default), return individual chunks as Document objects for RAG.\n",
       "                         If False, return original documents with full content and all chunk scores.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparseStore.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class ReadOnlySparseStore(SparseStore):\n",
    "    \"\"\"\n",
    "    A sparse vector store based on a read-only full-text search engine\n",
    "    \"\"\"\n",
    "\n",
    "    def exists(self):\n",
    "        return True\n",
    "\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Adds documents.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Not supported for ReadOnlySparseStore instances.')\n",
    "        \n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a single document.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Not supported for ReadOnlySparseStore instances.')\n",
    "\n",
    "    \n",
    "    def remove_source(self, source):\n",
    "        \"\"\"\n",
    "        Remove documents by source\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Not supported for ReadOnlySparseStore instances.')\n",
    "\n",
    "    def update_documents(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Update a set of documents.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Not supported for ReadOnlySparseStore instances.')\n",
    "\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a list of files previously added to vector store.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Not supported for ReadOnlySparseStore instances.')        \n",
    "\n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get total number of records added to vector store\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Not supported for ReadOnlySparseStore instances.')\n",
    "\n",
    "    def erase(self):\n",
    "        \"\"\"\n",
    "        Removes all documents in vector store\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Not supported for ReadOnlySparseStore instances.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Optional, Dict, Any\n",
    "import requests\n",
    "try:\n",
    "    from requests_ntlm import HttpNtlmAuth\n",
    "    REQUESTS_NTLM_INSTALLED = True\n",
    "except ImportError:\n",
    "    REQUESTS_NTLM_INSTALLED = False\n",
    "    \n",
    "    \n",
    "class SharePointStore(ReadOnlySparseStore):\n",
    "    \"\"\"\n",
    "    A sparse vector store based on Microsoft Sharepoint using a \"vectors-on-demand\" approach.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                persist_location:str,  # SharePoint URL\n",
    "                username:str, # user name (e.g., CORP\\my_username)\n",
    "                password:str, # your SharePoint password\n",
    "                chunk_for_semantic_search: bool = True, # whether to do dynamic chunking\n",
    "                chunk_size: int = 500, # chunk size in characters (1 word ~ 4 characters)\n",
    "                chunk_overlap: int = 50, # chunk overlap between chunks\n",
    "                n_candidates:Optional[int]=None, # number of candidate documents to consider for answer. Defaults to limit*10.\n",
    "                load_web_documents: bool = True, # whether to load content from web URLs when content field is empty\n",
    "                **kwargs, \n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes full-text search engine based on Microsoft Sharepoint\n",
    "        \"\"\"\n",
    "\n",
    "        if not REQUESTS_NTLM_INSTALLED:\n",
    "            raise ImportError('Please install requests_ntlm: pip install requests_ntlm')\n",
    "\n",
    "        self.persist_location = persist_location # SharePoint URL\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.chunk_for_semantic_search = chunk_for_semantic_search\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        if n_candidates: self.n_candidates = n_candidates\n",
    "        self.load_web_documents = load_web_documents\n",
    "        self.init_embedding_model(**kwargs)\n",
    "        \n",
    "\n",
    "    def search(self, \n",
    "               query:str,  # query string\n",
    "               is_document:int=1,   # search documents by default\n",
    "               filters:Optional[Dict[str, Any]]=None,  # filters\n",
    "               where_document:Optional[str]=None,     # boolean clause appended to query\n",
    "               limit=10,  # rowlimit in SharePoint\n",
    "               page=0,    # startrow in SharePoint\n",
    "               trimduplicates=False, # Prevents SharePoint from collapsing results it thinks are duplicates. This often changes total count.\n",
    "               enablequeryrules=False, # Turns off \"smart\" query rewriting that SharePoint applies for predictability\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Perform SharePoint keyword search for documents\n",
    "        \n",
    "        Args:\n",
    "            query: search query string\n",
    "            is_document: filter for document type (default: 1)\n",
    "            filters: dictionary of field filters (e.g., {'FileExtension': 'pdf'})\n",
    "            where_document: additional query to filter documents\n",
    "        \"\"\"\n",
    "        # Start with base query\n",
    "        search_query = f\"({query}) AND (IsDocument={str(is_document)})\"\n",
    "        \n",
    "        # Add filters\n",
    "        if filters:\n",
    "            for field, value in filters.items():\n",
    "                search_query += f\" AND ({field}:{value})\"\n",
    "        \n",
    "        # Add where_document filter\n",
    "        if where_document:\n",
    "            search_query += f\" AND ({where_document})\"\n",
    "        \n",
    "        from urllib.parse import quote\n",
    "        encoded_query = quote(search_query)\n",
    "        search_url = (\n",
    "            f\"{self.persist_location}/_api/search/query\"\n",
    "            f\"?querytext='{encoded_query}'\"\n",
    "            f\"&selectproperties='Title,Path,FileExtension,Contents'\"\n",
    "            f\"&rowlimit={limit}&startrow={page}\"\n",
    "            f\"&trimduplicates={'true' if trimduplicates else 'false'}\"\n",
    "            f\"&enablequeryrules={'true' if enablequeryrules else 'false'}\"\n",
    "        )\n",
    "\n",
    "        # Make request with NTLM (Windows auth)\n",
    "        response = requests.get(\n",
    "            search_url,\n",
    "            auth=HttpNtlmAuth(self.username, self.password),\n",
    "            headers={\"Accept\": \"application/json;odata=verbose\"}\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Parse and print results\n",
    "        total_hits = -1\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            rows = results['d']['query']['PrimaryQueryResult']['RelevantResults']['Table']['Rows']['results']\n",
    "            total_hits = results['d']['query']['PrimaryQueryResult']['RelevantResults']['TotalRows']\n",
    "\n",
    "            results = []\n",
    "            for row in rows:\n",
    "                props = {item['Key']: item['Value'] for item in row['Cells']['results']}\n",
    "                title = props.get('Title', '')\n",
    "                path = props.get('Path', '')\n",
    "                contents = props.get('Contents', '') or ''\n",
    "                id = props.get('DocId', '')\n",
    "                results.append({'id':id,\n",
    "                                'source': path,\n",
    "                                'title': title,\n",
    "                                'page_content': contents})\n",
    "            return {'hits':results, 'total_hits':total_hits}\n",
    "        else:\n",
    "            response.raise_for_status()\n",
    "\n",
    "    def get_doc(self, id: str):\n",
    "        \"\"\"\n",
    "        Get a specific document by ID from SharePoint\n",
    "        \"\"\"\n",
    "        query = f\"DocId:{id}\"\n",
    "        results = self.search(query)\n",
    "        return results['hits'][0] if results['hits'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Sequence, Any\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from whoosh import index, sorting\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.fields import *\n",
    "from whoosh.filedb.filestore import RamStorage\n",
    "from whoosh.qparser import MultifieldParser, OrGroup\n",
    "from whoosh.query import Term, And, Variations\n",
    "from whoosh.analysis import RegexTokenizer, LowercaseFilter\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from elasticsearch import Elasticsearch\n",
    "    ELASTICSEARCH_INSTALLED = True\n",
    "except ImportError:\n",
    "    ELASTICSEARCH_INSTALLED = False\n",
    "\n",
    "from onprem.ingest.helpers import doc_from_dict\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTANT: The schema below contains only essential fields required for core\n",
    "#            functionality. All other fields (filepath, filename, tags, etc.) are\n",
    "#            automatically added as dynamic fields when documents are indexed.\n",
    "#\n",
    "#            The page_content field is the only truly required field in supplied\n",
    "#            Document objects. All other fields, including dynamic fields, are optional. \n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "DEFAULT_SCHEMA = Schema(\n",
    "    page_content=TEXT(stored=True, analyzer=StemmingAnalyzer()), # REQUIRED with stemming enabled\n",
    "    id=ID(stored=True, unique=True),\n",
    "    source=KEYWORD(stored=True, commas=True), \n",
    "    source_search=TEXT(stored=True, analyzer=StemmingAnalyzer()),\n",
    "    )\n",
    "DEFAULT_SCHEMA.add(\"*_t\", TEXT(stored=True, analyzer=StemmingAnalyzer()), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_k\", KEYWORD(stored=True, commas=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_b\", BOOLEAN(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_n\", NUMERIC(stored=True), glob=True)\n",
    "DEFAULT_SCHEMA.add(\"*_d\", DATETIME(stored=True), glob=True)\n",
    "\n",
    "\n",
    "def default_schema():\n",
    "    schema = DEFAULT_SCHEMA\n",
    "    #if \"raw\" not in schema.stored_names():\n",
    "        #schema.add(\"raw\", TEXT(stored=True))\n",
    "    return schema\n",
    "\n",
    "KEYWORD_ANALYZER = RegexTokenizer() | LowercaseFilter()\n",
    "\n",
    "def create_field_for_value(field_name, value):\n",
    "    \"\"\"Create field definition based on value type - same logic as add_documents.\"\"\"\n",
    "    if isinstance(value, bool):\n",
    "        return BOOLEAN(stored=True)\n",
    "    elif isinstance(value, list):\n",
    "        return KEYWORD(stored=True, commas=True, analyzer=KEYWORD_ANALYZER)\n",
    "    elif isinstance(value, str):\n",
    "        if field_name.endswith('_date'):\n",
    "            return DATETIME(stored=True)\n",
    "        elif len(value) > 100:\n",
    "            return TEXT(stored=True, analyzer=StemmingAnalyzer())\n",
    "        else:\n",
    "            return KEYWORD(stored=True, commas=True, analyzer=KEYWORD_ANALYZER)\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return NUMERIC(stored=True)\n",
    "    return None\n",
    "\n",
    "def get_field_analyzer(field_name, value, schema=None):\n",
    "    \"\"\"Get the analyzer that would be used for a field.\"\"\"\n",
    "    if schema and field_name in schema:\n",
    "        field = schema[field_name]\n",
    "        if hasattr(field, 'analyzer') and field.analyzer:\n",
    "            return field.analyzer\n",
    "    \n",
    "    # Use same logic as add_documents\n",
    "    field_def = create_field_for_value(field_name, value)\n",
    "    if field_def and hasattr(field_def, 'analyzer'):\n",
    "        return field_def.analyzer\n",
    "    return None\n",
    "\n",
    "class WhooshStore(SparseStore):\n",
    "    \"\"\"\n",
    "    A sparse vector store based on the Whoosh full-text search engine.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                persist_location: Optional[str]=None, \n",
    "                index_name:str = 'myindex',\n",
    "                # Dynamic chunking for semantic search\n",
    "                chunk_for_semantic_search: bool = False,\n",
    "                chunk_size: int = 500,\n",
    "                chunk_overlap: int = 50,\n",
    "                **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes full-text search engine.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *persist_location*: path to folder where search index is stored\n",
    "        - *index_name*: name of index\n",
    "        - *chunk_for_semantic_search*: if True, dynamically chunk large documents for semantic search (default: False)\n",
    "        - *chunk_size*: size of chunks when chunk_for_semantic_search=True (default: 500)\n",
    "        - *chunk_overlap*: overlap between chunks when chunk_for_semantic_search=True (default: 50)\n",
    "        - *embedding_model*: name of sentence-transformers model\n",
    "        - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "        - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                     embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        \"\"\"\n",
    "\n",
    "        self.persist_location = persist_location # alias for consistency with DenseStore\n",
    "        self.index_name = index_name\n",
    "        \n",
    "        # Store dynamic chunking settings\n",
    "        self.chunk_for_semantic_search = chunk_for_semantic_search\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.content_field = 'page_content'  # WhooshStore uses standard field name\n",
    "        if self.persist_location and not self.index_name:\n",
    "            raise ValueError('index_name is required if persist_location is supplied')\n",
    "        if self.persist_location:\n",
    "            if not index.exists_in(self.persist_location, indexname=self.index_name):\n",
    "                self.ix = __class__.initialize_index(self.persist_location, self.index_name)\n",
    "            else:\n",
    "                self.ix = index.open_dir(self.persist_location, indexname=self.index_name)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"No persist_location was supplied, so an in-memory only index \"\n",
    "                \"was created using DEFAULT_SCHEMA\"\n",
    "            )\n",
    "            self.ix = RamStorage().create_index(default_schema())\n",
    "        self.init_embedding_model(**kwargs) # stored as self.embeddings\n",
    "\n",
    "    @classmethod\n",
    "    def index_exists_in(cls, index_path: str, index_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Returns True if index exists with name, *indexname*, and path, *index_path*.\n",
    "        \"\"\"\n",
    "        return index.exists_in(index_path, indexname=index_name)\n",
    "\n",
    "    @classmethod\n",
    "    def initialize_index(\n",
    "        cls, index_path: str, index_name: str, schema: Optional[Schema] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *index_path*: path to folder storing search index\n",
    "        - *index_name*: name of index\n",
    "        - *schema*: optional whoosh.fields.Schema object.\n",
    "                    If None, DEFAULT_SCHEMA is used\n",
    "        \"\"\"\n",
    "        schema = default_schema() if not schema else schema\n",
    "\n",
    "        if index.exists_in(index_path, indexname=index_name):\n",
    "            raise ValueError(\n",
    "                f\"There is already an existing index named {index_name}  with path {index_path} \\n\"\n",
    "                + f\"Delete {index_path} manually and try again.\"\n",
    "            )\n",
    "        if not os.path.exists(index_path):\n",
    "            os.makedirs(index_path)\n",
    "        ix = index.create_in(index_path, indexname=index_name, schema=schema)\n",
    "        return ix\n",
    "\n",
    "\n",
    "    def doc2dict(self, doc:Document):\n",
    "        \"\"\"\n",
    "        Convert LangChain Document to expected format\n",
    "        \"\"\"\n",
    "        d = {}\n",
    "        for k,v in doc.metadata.items():\n",
    "            d[k] = v\n",
    "        d['id'] = uuid.uuid4().hex if not doc.metadata.get('id', '') else doc.metadata['id']\n",
    "        d['page_content' ] = self.normalize_text(doc.page_content)\n",
    "        #d['raw'] = json.dumps(d)\n",
    "        if 'source' in d:\n",
    "            d['source_search'] = d['source']\n",
    "        if 'filepath' in d:\n",
    "            d['filepath_search'] = d['filepath']\n",
    "        return d\n",
    "\n",
    "    def delete_by_prefix(self, prefix:str, field:str, **kwargs):\n",
    "        \"\"\"\n",
    "        Deletes all documents from a Whoosh index where the `source_field` starts with the given prefix.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *prefix*: The prefix to match in the `field`.\n",
    "        - *field*: The name of the field to match against.\n",
    "\n",
    "        **Returns:**\n",
    "        - Number of records deleted\n",
    "\t\t\"\"\"\n",
    "\n",
    "        from whoosh.query import Prefix\n",
    "        with self.ix.searcher() as searcher:\n",
    "            results = searcher.search(Prefix(field, prefix), limit=None)\n",
    "\n",
    "            if results:\n",
    "                with self.ix.writer(**kwargs) as writer:\n",
    "                    for hit in results:\n",
    "                        writer.delete_document(hit.docnum)\n",
    "                return len(results)\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def close_index(self):\n",
    "        \"\"\"\n",
    "        Run this if there are lock errors when trying to write to index\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'ix') and self.ix:\n",
    "            self.ix.close()\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimize the search index for better performance.\n",
    "        Should be called after bulk operations like adding many documents.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'ix') and self.ix:\n",
    "            self.ix.optimize()\n",
    "    \n",
    "    #------------------------------\n",
    "    # overrides of abstract methods\n",
    "    # -----------------------------\n",
    "\n",
    "\n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if documents have been added to search index\n",
    "        \"\"\"\n",
    "        return self.get_size() > 0\n",
    "\n",
    "\n",
    "    def add_documents(self,\n",
    "                      docs: Sequence[Document], # list of LangChain Documents\n",
    "                      limitmb:int=1024, # maximum memory in  megabytes to use\n",
    "                      verbose:bool=True, # Set to False to disable progress bar\n",
    "                      **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Indexes documents. Extra kwargs supplied to `TextStore.ix.writer`.\n",
    "        \"\"\"\n",
    "        with self.ix.writer(limitmb=limitmb, **kwargs) as writer:\n",
    "            # Add any new fields first\n",
    "            stored_names = self.ix.schema.stored_names()\n",
    "            new_fields = set()\n",
    "            \n",
    "            for doc in docs:\n",
    "                for k, v in doc.metadata.items():\n",
    "                    if k not in stored_names and k not in new_fields:\n",
    "                        # Add field using same logic as filter processing\n",
    "                        field_def = create_field_for_value(k, v)\n",
    "                        if field_def:\n",
    "                            writer.add_field(k, field_def)\n",
    "                        new_fields.add(k)\n",
    "            \n",
    "            for doc in tqdm(docs, total=len(docs), disable=not verbose):\n",
    "                d = self.doc2dict(doc)\n",
    "                writer.update_document(**d)\n",
    "\n",
    "       \n",
    "    def remove_document(self, value:str, field:str='id', **kwargs):\n",
    "        \"\"\"\n",
    "        Remove document with corresponding value and field.\n",
    "        Default field is the id field.\n",
    "        \"\"\"\n",
    "        with self.ix.writer(**kwargs) as writer:\n",
    "            writer.delete_by_term(field, value)\n",
    "        return\n",
    "\n",
    "\n",
    "    def remove_source(self, source:str):\n",
    "        \"\"\"\n",
    "        remove all documents associated with `source`.\n",
    "        The `source` argument can either be the full path to\n",
    "        document or a parent folder.  In the latter case,\n",
    "        ALL documents in parent folder will be removed.\n",
    "        \"\"\"\n",
    "        return self.delete_by_prefix(source, field='source')\n",
    "        \n",
    "\n",
    "    def update_documents(self,\n",
    "                         doc_dicts: List[dict], # list of dictionaries with keys 'page_content', 'source', 'id', etc.\n",
    "                         **kwargs):\n",
    "        \"\"\"\n",
    "        Update a set of documents (doc in index with same ID will be over-written)\n",
    "        \"\"\"\n",
    "        docs = [doc_from_dict(d) for d in doc_dicts]\n",
    "        self.add_documents(docs)\n",
    "\n",
    "\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a generator to iterate through all indexed documents\n",
    "        \"\"\"\n",
    "        return self.ix.searcher().documents()\n",
    "       \n",
    "\n",
    "    def get_doc(self, id:str):\n",
    "        \"\"\"\n",
    "        Get an indexed record by ID\n",
    "        \"\"\"\n",
    "        r = self.search(f'id:{id}', return_dict=True)\n",
    "        return r['hits'][0] if len(r['hits']) > 0 else None\n",
    "\n",
    "\n",
    "    def get_size(self, include_deleted:bool=False) -> int:\n",
    "        \"\"\"\n",
    "        Gets size of index\n",
    "\n",
    "        If include_deleted is True, will include deletd detects (prior to optimization).\n",
    "        \"\"\"\n",
    "        return self.ix.doc_count_all() if include_deleted else self.ix.doc_count()\n",
    "\n",
    "        \n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Clears index\n",
    "        \"\"\"\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            msg = (\n",
    "                f\"You are about to remove all documents from the search index.\"\n",
    "                + f\"(Original documents on file system will remain.) Are you sure?\"\n",
    "            )\n",
    "            shall = input(\"%s (Y/n) \" % msg) == \"Y\"\n",
    "        if shall and index.exists_in(\n",
    "            self.persist_location, indexname=self.index_name\n",
    "        ):\n",
    "            ix = index.create_in(\n",
    "                self.persist_location,\n",
    "                indexname=self.index_name,\n",
    "                schema=default_schema(),\n",
    "            )\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def search(\n",
    "            self,\n",
    "            query: str,\n",
    "            fields: Sequence = [\"page_content\"],\n",
    "            highlight: bool = True,\n",
    "            limit:int=10,\n",
    "            page:int=1,\n",
    "            return_dict:bool=True,\n",
    "            filters:Optional[Dict[str, Any]] = None,\n",
    "            where_document:Optional[str]=None,\n",
    "            return_generator=False,\n",
    "            **kwargs\n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Queries the index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *query*: the query string\n",
    "        - *fields*: a list of fields to search\n",
    "        - *highlight*: If True, highlight hits\n",
    "        - *limit*: results per page\n",
    "        - *page*: page of hits to return\n",
    "        - *return_dict*: If True, return list of dictionaries instead of LangChain Document objects\n",
    "        - *filters*: filter results by field values (e.g., {'extension':'pdf'})\n",
    "        - *where_document*: optional query to further filter results\n",
    "        - *return_generator*: If True, returns a generator, not a list\n",
    "        \"\"\"\n",
    "\n",
    "        q = query\n",
    "        q = self._preprocess_query(q)\n",
    "\n",
    "        if where_document:\n",
    "            q = f'({q}) AND ({where_document})'\n",
    "\n",
    "        # process filters\n",
    "        combined_filter=None\n",
    "        if filters:\n",
    "            terms = []\n",
    "            for k, v in filters.items():\n",
    "                # Only apply analyzers to string values\n",
    "                if isinstance(v, str):\n",
    "                    analyzer = get_field_analyzer(k, v, self.ix.schema)\n",
    "                    if analyzer:\n",
    "                        analyzed_tokens = list(analyzer(v, removestops=False))\n",
    "                        if analyzed_tokens:\n",
    "                            v = analyzed_tokens[0].text\n",
    "                terms.append(Term(k, v))\n",
    "            combined_filter = And(terms)\n",
    "        \n",
    "        def get_search_results(searcher):\n",
    "            \"\"\"Helper to get search results from searcher\"\"\"\n",
    "            if page == 1:\n",
    "                return searcher.search(\n",
    "                    MultifieldParser(fields, schema=self.ix.schema, termclass=Variations, group=OrGroup.factory(0.9)).parse(q), limit=limit, filter=combined_filter)\n",
    "            else:\n",
    "                return searcher.search_page(\n",
    "                    MultifieldParser(fields, schema=self.ix.schema, termclass=Variations, group=OrGroup.factory(0.9)).parse(q), page, limit, filter=combined_filter)\n",
    "        \n",
    "        def process_result(r):\n",
    "            \"\"\"Helper to process individual search result\"\"\"\n",
    "            d = dict(r)\n",
    "            if highlight:\n",
    "                for f in fields:\n",
    "                    if r[f] and isinstance(r[f], str):\n",
    "                        d['hl_'+f] = r.highlights(f) or r[f]\n",
    "            return d if return_dict else doc_from_dict(d)\n",
    "        \n",
    "        if return_generator:\n",
    "            searcher = self.ix.searcher()\n",
    "            try:\n",
    "                results = get_search_results(searcher)\n",
    "                total_hits = results.scored_length()\n",
    "                if page > math.ceil(total_hits/limit):\n",
    "                   results = []\n",
    "                \n",
    "                def result_generator():\n",
    "                    try:\n",
    "                        for r in results:\n",
    "                            yield process_result(r)\n",
    "                    finally:\n",
    "                        searcher.close()\n",
    "                \n",
    "                return {'hits': result_generator(),\n",
    "                        'total_hits': total_hits}\n",
    "            except:\n",
    "                searcher.close()\n",
    "                raise\n",
    "        else:\n",
    "            with self.ix.searcher() as searcher:\n",
    "                results = get_search_results(searcher)\n",
    "                total_hits = results.scored_length()\n",
    "                if page > math.ceil(total_hits/limit):\n",
    "                   results = []\n",
    "                \n",
    "                return {'hits': [process_result(r) for r in results],\n",
    "                        'total_hits': total_hits}\n",
    "\n",
    "    def get_aggregations(self, query=\"*\", facets=None, filters=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Get aggregate statistics on field values using Whoosh faceted search.\n",
    "        \n",
    "        **Args:**\n",
    "        - *query*: search query (default: \"*\" for all documents)\n",
    "        - *facets*: dictionary defining facets to compute. Format:\n",
    "          ```\n",
    "          {\n",
    "              'facet_name': {\n",
    "                  'type': 'terms',  # or 'range'\n",
    "                  'field': 'field_name',\n",
    "                  # For range facets:\n",
    "                  'min': 0, 'max': 1000, 'gap': 100\n",
    "              }\n",
    "          }\n",
    "          ```\n",
    "        - *filters*: filter results by field values before aggregating\n",
    "        \n",
    "        **Returns:**\n",
    "        Dictionary with facet results: `{'facet_name': {'value': count, ...}}`\n",
    "        \n",
    "        **Example:**\n",
    "        ```python\n",
    "        facets = {\n",
    "            'doc_types': {'type': 'terms', 'field': 'doc_type'},\n",
    "            'file_sizes': {'type': 'range', 'field': 'file_size', 'min': 0, 'max': 1000000, 'gap': 100000}\n",
    "        }\n",
    "        aggs = store.get_aggregations(\"*\", facets=facets)\n",
    "        # Returns: {'doc_types': {'pdf': 45, 'txt': 23}, 'file_sizes': {...}}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if not facets:\n",
    "            return {}\n",
    "        \n",
    "        # Build Whoosh facet objects\n",
    "        whoosh_facets = {}\n",
    "        for name, config in facets.items():\n",
    "            if config['type'] == 'terms':\n",
    "                whoosh_facets[name] = sorting.FieldFacet(config['field'])\n",
    "            elif config['type'] == 'range':\n",
    "                whoosh_facets[name] = sorting.RangeFacet(\n",
    "                    config['field'], \n",
    "                    config.get('min', 0), \n",
    "                    config.get('max', 100), \n",
    "                    config.get('gap', 10)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported facet type: {config['type']}. Use 'terms' or 'range'.\")\n",
    "        \n",
    "        with self.ix.searcher() as searcher:\n",
    "            # Process query\n",
    "            q = self._preprocess_query(query)\n",
    "            q_obj = MultifieldParser(['page_content'], self.ix.schema, \n",
    "                                   termclass=Variations, group=OrGroup.factory(0.9)).parse(q)\n",
    "            \n",
    "            # Process filters (reuse existing filter logic)\n",
    "            combined_filter = None\n",
    "            if filters:\n",
    "                terms = []\n",
    "                for k, v in filters.items():\n",
    "                    # Only apply analyzers to string values\n",
    "                    if isinstance(v, str):\n",
    "                        analyzer = get_field_analyzer(k, v, self.ix.schema)\n",
    "                        if analyzer:\n",
    "                            analyzed_tokens = list(analyzer(v, removestops=False))\n",
    "                            if analyzed_tokens:\n",
    "                                v = analyzed_tokens[0].text\n",
    "                    terms.append(Term(k, v))\n",
    "                combined_filter = And(terms)\n",
    "            \n",
    "            # Execute faceted search\n",
    "            results = searcher.search(q_obj, filter=combined_filter, groupedby=whoosh_facets)\n",
    "            \n",
    "            # Extract aggregation results\n",
    "            aggregations = {}\n",
    "            for facet_name in whoosh_facets.keys():\n",
    "                facet_groups = results.groups(facet_name)\n",
    "                \n",
    "                # Convert Whoosh facet results to count dictionary\n",
    "                result_dict = {}\n",
    "                if isinstance(facet_groups, dict):\n",
    "                    # Direct dictionary format {value: [doc_ids]} or {value: count}\n",
    "                    for key, value in facet_groups.items():\n",
    "                        if isinstance(value, (list, tuple)):\n",
    "                            result_dict[str(key)] = len(value)\n",
    "                        else:\n",
    "                            result_dict[str(key)] = value\n",
    "                else:\n",
    "                    # Handle other formats - iterate through items\n",
    "                    try:\n",
    "                        for key, group in facet_groups.items():\n",
    "                            if isinstance(group, (list, tuple)):\n",
    "                                result_dict[str(key)] = len(group)\n",
    "                            else:\n",
    "                                result_dict[str(key)] = group\n",
    "                    except AttributeError:\n",
    "                        # Handle list of tuples format\n",
    "                        try:\n",
    "                            for key, group in facet_groups:\n",
    "                                if hasattr(group, '__len__') and not isinstance(group, str):\n",
    "                                    result_dict[str(key)] = len(group)\n",
    "                                else:\n",
    "                                    result_dict[str(key)] = 1\n",
    "                        except (ValueError, TypeError):\n",
    "                            # Last resort: empty result\n",
    "                            result_dict = {}\n",
    "                \n",
    "                aggregations[facet_name] = result_dict\n",
    "            \n",
    "            return aggregations\n",
    "\n",
    "\n",
    "class ElasticsearchSparseStore(SparseStore):\n",
    "    \"\"\"\n",
    "    A sparse vector store based on Elasticsearch.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                persist_location: Optional[str]=None,\n",
    "                index_name:str = 'myindex',\n",
    "                basic_auth: Optional[tuple] = None,\n",
    "                verify_certs: bool = True,\n",
    "                ca_certs: Optional[str] = None,\n",
    "                timeout: int = 30,\n",
    "                max_retries: int = 3,\n",
    "                retry_on_timeout: bool = True,\n",
    "                maxsize: int = 25,\n",
    "                # Field mapping parameters for existing indices\n",
    "                content_field: str = 'page_content',\n",
    "                source_field: Optional[str] = 'source',\n",
    "                id_field: str = 'id',\n",
    "                content_analyzer: str = 'standard',\n",
    "                # Dynamic chunking for semantic search\n",
    "                chunk_for_semantic_search: bool = False,\n",
    "                chunk_size: int = 500,\n",
    "                chunk_overlap: int = 50,\n",
    "                n_candidates:Optional[int]=None,\n",
    "                **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes Elasticsearch-based full-text search engine.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *persist_location*: Elasticsearch URL (e.g., 'http://localhost:9200'). If None, defaults to 'http://localhost:9200'\n",
    "        - *index_name*: name of Elasticsearch index\n",
    "        - *basic_auth*: tuple of (username, password) for basic authentication\n",
    "        - *verify_certs*: whether to verify SSL certificates\n",
    "        - *ca_certs*: path to CA certificate file\n",
    "        - *timeout*: connection timeout in seconds (becomes request_timeout internally for Elasticsearch v9+ compatibility)\n",
    "        - *max_retries*: maximum number of retries\n",
    "        - *retry_on_timeout*: whether to retry on timeout\n",
    "        - *maxsize*: maximum number of connections in the pool (removed for Elasticsearch v9+ compatibility)\n",
    "        - *content_field*: field name for document content (default: 'page_content')\n",
    "        - *source_field*: field name for document source (default: 'source', set to None to disable)\n",
    "        - *id_field*: field name for document ID (default: 'id')\n",
    "        - *content_analyzer*: analyzer for content field (default: 'standard')\n",
    "        - *chunk_for_semantic_search*: if True, dynamically chunk large documents for semantic search (default: False)\n",
    "        - *chunk_size*: size of chunks when chunk_for_semantic_search=True (default: 500)\n",
    "        - *chunk_overlap*: overlap between chunks when chunk_for_semantic_search=True (default: 50)\n",
    "        - *n_candidates*:  number of candidate documents to consider for answer. Defaults to n*10\n",
    "        - *embedding_model*: name of sentence-transformers model\n",
    "        - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`). If None, GPU used if available.\n",
    "        - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                     embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "        \"\"\"\n",
    "        if not ELASTICSEARCH_INSTALLED:\n",
    "            raise ImportError('Please install a version of elasticsearch compatible with your running Elasticsearch instance. For latest: pip install elasticsearch')\n",
    "\n",
    "        # Use persist_location as Elasticsearch URL\n",
    "        self.elasticsearch_url = persist_location if persist_location else 'http://localhost:9200'\n",
    "        self.persist_location = self.elasticsearch_url  # for interface compatibility\n",
    "        self.index_name = index_name\n",
    "        \n",
    "        # Store field mappings for custom field names\n",
    "        self.content_field = content_field\n",
    "        self.source_field = source_field\n",
    "        self.id_field = id_field\n",
    "        self.content_analyzer = content_analyzer\n",
    "        \n",
    "        # Store dynamic chunking settings\n",
    "        self.chunk_for_semantic_search = chunk_for_semantic_search\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        if n_candidates: self.n_candidates = n_candidates\n",
    "        \n",
    "        # Prepare Elasticsearch client parameters\n",
    "        # Use request_timeout (works in both v8 and v9) instead of timeout (removed in v9)\n",
    "        # Remove maxsize (removed in v9) for compatibility\n",
    "        es_params = {\n",
    "            'request_timeout': timeout,\n",
    "            'max_retries': max_retries,\n",
    "            'retry_on_timeout': retry_on_timeout,\n",
    "        }\n",
    "        \n",
    "        # Add authentication if provided\n",
    "        if basic_auth:\n",
    "            es_params['basic_auth'] = basic_auth\n",
    "            \n",
    "        # Add SSL parameters\n",
    "        if not verify_certs:\n",
    "            es_params['verify_certs'] = False\n",
    "        if ca_certs:\n",
    "            es_params['ca_certs'] = ca_certs\n",
    "            \n",
    "        # Filter out embedding-related kwargs before passing to Elasticsearch\n",
    "        embedding_kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if k.startswith('embedding_'):\n",
    "                embedding_kwargs[k] = v\n",
    "            else:\n",
    "                es_params[k] = v\n",
    "        \n",
    "        # Initialize Elasticsearch client\n",
    "        self.es = Elasticsearch([self.elasticsearch_url], **es_params)\n",
    "        \n",
    "        # Handle index creation or validation\n",
    "        if not self.es.indices.exists(index=self.index_name):\n",
    "            self._create_index()\n",
    "        else:\n",
    "            # Validate existing index has required fields\n",
    "            self._validate_existing_index()\n",
    "        \n",
    "        self.init_embedding_model(**embedding_kwargs)  # stored as self.embeddings\n",
    "\n",
    "    def _create_index(self):\n",
    "        \"\"\"Create Elasticsearch index with appropriate mapping using custom field names and analyzer\"\"\"\n",
    "        properties = {\n",
    "            # Essential fields for core functionality using custom field names and analyzer\n",
    "            self.content_field: {\"type\": \"text\", \"analyzer\": self.content_analyzer},\n",
    "            self.id_field: {\"type\": \"keyword\"},\n",
    "        }\n",
    "        \n",
    "        # Add source field only if specified\n",
    "        if self.source_field:\n",
    "            properties[self.source_field] = {\"type\": \"keyword\"}\n",
    "            properties[f\"{self.source_field}_search\"] = {\"type\": \"text\", \"analyzer\": self.content_analyzer}\n",
    "        \n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": properties\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.es.indices.create(index=self.index_name, body=mapping)\n",
    "\n",
    "    def _validate_existing_index(self):\n",
    "        \"\"\"Validate that existing index has required fields with compatible types\"\"\"\n",
    "        try:\n",
    "            # Get index mapping\n",
    "            mapping_response = self.es.indices.get_mapping(index=self.index_name)\n",
    "            index_mapping = mapping_response[self.index_name]['mappings']\n",
    "            properties = index_mapping.get('properties', {})\n",
    "            \n",
    "            # Check required fields exist\n",
    "            missing_fields = []\n",
    "            incompatible_fields = []\n",
    "            \n",
    "            # Validate content field\n",
    "            if self.content_field not in properties:\n",
    "                missing_fields.append(f\"Content field '{self.content_field}' not found\")\n",
    "            else:\n",
    "                field_props = properties[self.content_field]\n",
    "                if field_props.get('type') != 'text':\n",
    "                    incompatible_fields.append(f\"Content field '{self.content_field}' is not text type (found: {field_props.get('type')})\")\n",
    "            \n",
    "            # Validate ID field  \n",
    "            if self.id_field not in properties:\n",
    "                missing_fields.append(f\"ID field '{self.id_field}' not found\")\n",
    "            else:\n",
    "                field_props = properties[self.id_field]\n",
    "                if field_props.get('type') not in ['keyword', '_id']:\n",
    "                    incompatible_fields.append(f\"ID field '{self.id_field}' is not keyword type (found: {field_props.get('type')})\")\n",
    "            \n",
    "            # Validate source field (only if specified)\n",
    "            if self.source_field:\n",
    "                if self.source_field not in properties:\n",
    "                    missing_fields.append(f\"Source field '{self.source_field}' not found\")\n",
    "                else:\n",
    "                    field_props = properties[self.source_field]\n",
    "                    if field_props.get('type') not in ['keyword', 'text']:\n",
    "                        incompatible_fields.append(f\"Source field '{self.source_field}' is not keyword or text type (found: {field_props.get('type')})\")\n",
    "            \n",
    "            # Report validation results\n",
    "            if missing_fields:\n",
    "                warnings.warn(f\"Index validation warnings - Missing fields: {'; '.join(missing_fields)}\")\n",
    "            \n",
    "            if incompatible_fields:\n",
    "                warnings.warn(f\"Index validation warnings - Incompatible field types: {'; '.join(incompatible_fields)}\")\n",
    "                \n",
    "            # Log analyzer info for content field if it exists\n",
    "            if self.content_field in properties:\n",
    "                content_props = properties[self.content_field]\n",
    "                existing_analyzer = content_props.get('analyzer', 'default')\n",
    "                if existing_analyzer != self.content_analyzer:\n",
    "                    warnings.warn(f\"Content field '{self.content_field}' uses analyzer '{existing_analyzer}' but you specified '{self.content_analyzer}'. Search behavior may be affected.\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not validate index mapping: {e}\")\n",
    "\n",
    "    @classmethod\n",
    "    def index_exists_in(cls, index_path: str, index_name: Optional[str] = None, elasticsearch_url: str = 'http://localhost:9200'):\n",
    "        \"\"\"\n",
    "        Returns True if index exists in Elasticsearch cluster.\n",
    "        \n",
    "        **Args:**\n",
    "        - *index_path*: Not used for Elasticsearch (kept for interface compatibility)\n",
    "        - *index_name*: name of Elasticsearch index\n",
    "        - *elasticsearch_url*: Elasticsearch URL (e.g., 'http://localhost:9200')\n",
    "        \"\"\"\n",
    "        if not ELASTICSEARCH_INSTALLED:\n",
    "            raise ImportError(\"Please install the elasticsearch package version for your \"\n",
    "                              \"Elasticsearch instance: e.g., pip install elasticsearch==9)\")\n",
    "        \n",
    "        es = Elasticsearch([elasticsearch_url])\n",
    "        return es.indices.exists(index=index_name)\n",
    "\n",
    "    @classmethod\n",
    "    def initialize_index(cls, index_path: str, index_name: str, elasticsearch_url: str = 'http://localhost:9200'):\n",
    "        \"\"\"\n",
    "        Initialize Elasticsearch index\n",
    "        \n",
    "        **Args:**\n",
    "        - *index_path*: Not used for Elasticsearch (kept for interface compatibility) \n",
    "        - *index_name*: name of Elasticsearch index\n",
    "        - *elasticsearch_url*: Elasticsearch URL (e.g., 'http://localhost:9200')\n",
    "        \"\"\"\n",
    "        store = cls(persist_location=elasticsearch_url, index_name=index_name)\n",
    "        return store.es\n",
    "\n",
    "\n",
    "    def doc2dict(self, doc: Document):\n",
    "        \"\"\"\n",
    "        Convert LangChain Document to expected format using custom field mappings\n",
    "        \"\"\"\n",
    "        d = {}\n",
    "        for k, v in doc.metadata.items():\n",
    "            d[k] = v\n",
    "        \n",
    "        # Use custom field mappings\n",
    "        d[self.id_field] = uuid.uuid4().hex if not doc.metadata.get('id', '') else doc.metadata['id']\n",
    "        d[self.content_field] = self.normalize_text(doc.page_content)\n",
    "        \n",
    "        # Handle source field mapping (only if source field is configured)\n",
    "        if self.source_field and 'source' in d:\n",
    "            d[self.source_field] = d.pop('source')  # Replace 'source' with custom field name\n",
    "            d[f\"{self.source_field}_search\"] = d[self.source_field]\n",
    "        if 'filepath' in d:\n",
    "            d['filepath_search'] = d['filepath']\n",
    "        return d\n",
    "\n",
    "    def delete_by_prefix(self, prefix: str, field: str):\n",
    "        \"\"\"\n",
    "        Deletes all documents from Elasticsearch index where the field starts with the given prefix.\n",
    "        \"\"\"\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"prefix\": {\n",
    "                    field: prefix\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = self.es.delete_by_query(index=self.index_name, body=query)\n",
    "        return response.get('deleted', 0)\n",
    "\n",
    "\n",
    "    # overrides of abstract methods\n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if documents have been added to search index\n",
    "        \"\"\"\n",
    "        return self.get_size() > 0\n",
    "\n",
    "    def add_documents(self,\n",
    "                      docs: Sequence[Document],\n",
    "                      limitmb: int = 1024,  # Not used in Elasticsearch\n",
    "                      optimize: bool = False,  # Not used in Elasticsearch\n",
    "                      verbose: bool = True,\n",
    "                      **kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Indexes documents in Elasticsearch\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for doc in tqdm(docs, total=len(docs), disable=not verbose):\n",
    "            d = self.doc2dict(doc)\n",
    "            action = {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_id\": d['id'],\n",
    "                \"_source\": d\n",
    "            }\n",
    "            actions.append(action)\n",
    "        \n",
    "        if actions:\n",
    "            from elasticsearch.helpers import bulk\n",
    "            bulk(self.es, actions)\n",
    "            \n",
    "            # Force refresh to make documents immediately searchable\n",
    "            self.es.indices.refresh(index=self.index_name)\n",
    "            \n",
    "        if optimize:\n",
    "            self.es.indices.forcemerge(index=self.index_name)\n",
    "\n",
    "    def remove_document(self, value: str, field: str = 'id', optimize: bool = False):\n",
    "        \"\"\"\n",
    "        Remove document with corresponding value and field.\n",
    "        \"\"\"\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"term\": {\n",
    "                    field: value\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.es.delete_by_query(index=self.index_name, body=query)\n",
    "        \n",
    "        if optimize:\n",
    "            self.es.indices.forcemerge(index=self.index_name)\n",
    "\n",
    "    def remove_source(self, source: str, optimize: bool = False):\n",
    "        \"\"\"\n",
    "        remove all documents associated with `source`.\n",
    "        \"\"\"\n",
    "        if not self.source_field:\n",
    "            raise ValueError(\"Cannot remove by source: no source field configured. Set source_field parameter when creating the store.\")\n",
    "        return self.delete_by_prefix(source, field=self.source_field)\n",
    "\n",
    "    def update_documents(self,\n",
    "                         doc_dicts: List[dict], # list of dictionaries with keys 'page_content', 'source', 'id', etc.\n",
    "                         **kwargs):\n",
    "        \"\"\"\n",
    "        Update a set of documents (doc in index with same ID will be over-written)\n",
    "        \"\"\"\n",
    "        docs = [doc_from_dict(d) for d in doc_dicts]\n",
    "        self.add_documents(docs)\n",
    "\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a generator to iterate through all indexed documents\n",
    "        \"\"\"\n",
    "        query = {\"query\": {\"match_all\": {}}}\n",
    "        response = self.es.search(index=self.index_name, body=query, scroll='1m', size=1000)\n",
    "        \n",
    "        while response['hits']['hits']:\n",
    "            for hit in response['hits']['hits']:\n",
    "                yield hit['_source']\n",
    "            \n",
    "            # Get next batch\n",
    "            response = self.es.scroll(scroll_id=response['_scroll_id'], scroll='1m')\n",
    "\n",
    "    def get_doc(self, id: str):\n",
    "        \"\"\"\n",
    "        Get an indexed record by ID\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.get(index=self.index_name, id=id)\n",
    "            return response['_source']\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_size(self, include_deleted: bool = False) -> int:\n",
    "        \"\"\"\n",
    "        Gets size of index\n",
    "        \"\"\"\n",
    "        response = self.es.count(index=self.index_name)\n",
    "        return response['count']\n",
    "\n",
    "    def erase(self, confirm=True):\n",
    "        \"\"\"\n",
    "        Clears index\n",
    "        \"\"\"\n",
    "        shall = True\n",
    "        if confirm:\n",
    "            msg = (\n",
    "                f\"You are about to remove all documents from the search index.\"\n",
    "                + f\"(Original documents on file system will remain.) Are you sure?\"\n",
    "            )\n",
    "            shall = input(\"%s (Y/n) \" % msg) == \"Y\"\n",
    "        \n",
    "        if shall:\n",
    "            try:\n",
    "                self.es.indices.delete(index=self.index_name)\n",
    "                self._create_index()\n",
    "                return True\n",
    "            except:\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "    def search(self,\n",
    "              query: str,\n",
    "              fields: Sequence = None,\n",
    "              highlight: bool = True,\n",
    "              limit: int = 10,\n",
    "              page: int = 1,\n",
    "              return_dict: bool = True,\n",
    "              filters: Optional[Dict[str, Any]] = None,\n",
    "              where_document: Optional[str] = None,\n",
    "              return_generator=False,\n",
    "              **kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Queries the Elasticsearch index\n",
    "\n",
    "        **Args**\n",
    "\n",
    "        - *query*: the query string\n",
    "        - *fields*: a list of fields to search\n",
    "        - *highlight*: If True, highlight hits\n",
    "        - *limit*: results per page\n",
    "        - *page*: page of hits to return\n",
    "        - *return_dict*: If True, return list of dictionaries instead of LangChain Document objects\n",
    "        - *filters*: filter results by field values (e.g., {'extension':'pdf'})\n",
    "        - *where_document*: optional query to further filter results\n",
    "        - *return_generator*: If True, returns a generator, not a list\n",
    "        \"\"\"\n",
    "        # Use custom content field as default if no fields specified\n",
    "        if fields is None:\n",
    "            fields = [self.content_field]\n",
    "        \n",
    "        q = self._preprocess_query(query)\n",
    "        \n",
    "        es_query = {\n",
    "            \"query_string\": {\n",
    "                \"query\": q,\n",
    "                \"fields\": fields,\n",
    "                \"default_operator\": \"OR\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add additional document filter\n",
    "        if where_document:\n",
    "            es_query = {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        es_query,\n",
    "                        {\"query_string\": {\"query\": where_document}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Add filters\n",
    "        if filters:\n",
    "            filter_clauses = []\n",
    "            for k, v in filters.items():\n",
    "                filter_type = \"terms\" if isinstance(v, list) else \"term\"\n",
    "                filter_clauses.append({filter_type: {k: v}})\n",
    "            \n",
    "            if \"bool\" not in es_query:\n",
    "                es_query = {\"bool\": {\"must\": [es_query]}}\n",
    "            \n",
    "            es_query[\"bool\"][\"filter\"] = filter_clauses\n",
    "        \n",
    "        # Calculate from parameter for pagination\n",
    "        from_param = (page - 1) * limit\n",
    "        \n",
    "        # Build search body\n",
    "        search_body = {\n",
    "            \"query\": es_query,\n",
    "            \"from\": from_param,\n",
    "            \"size\": limit\n",
    "        }\n",
    "        \n",
    "        # Add highlighting if requested\n",
    "        if highlight:\n",
    "            search_body[\"highlight\"] = {\n",
    "                \"fields\": {field: {} for field in fields}\n",
    "            }\n",
    "        \n",
    "        def process_result(hit):\n",
    "            \"\"\"Helper to process individual search result\"\"\"\n",
    "            d = hit['_source'].copy()\n",
    "            if highlight and 'highlight' in hit:\n",
    "                for field in fields:\n",
    "                    if field in hit['highlight']:\n",
    "                        d[f'hl_{field}'] = ' '.join(hit['highlight'][field])\n",
    "                    else:\n",
    "                        d[f'hl_{field}'] = d.get(field, '')\n",
    "            return d if return_dict else doc_from_dict(d)\n",
    "        \n",
    "        if return_generator:\n",
    "            def result_generator():\n",
    "                response = self.es.search(index=self.index_name, body=search_body)\n",
    "                for hit in response['hits']['hits']:\n",
    "                    yield process_result(hit)\n",
    "            \n",
    "            # Get total count\n",
    "            count_response = self.es.count(index=self.index_name, body={\"query\": es_query})\n",
    "            total_hits = count_response['count']\n",
    "            \n",
    "            return {'hits': result_generator(), 'total_hits': total_hits}\n",
    "        else:\n",
    "            response = self.es.search(index=self.index_name, body=search_body)\n",
    "            hits = [process_result(hit) for hit in response['hits']['hits']]\n",
    "            total_hits = response['hits']['total']['value']\n",
    "            \n",
    "            return {'hits': hits, 'total_hits': total_hits}\n",
    "    \n",
    "    # semantic_search is now inherited from base SparseStore class\n",
    "    # The base class automatically detects chunk_for_semantic_search attribute\n",
    "    # and uses the appropriate chunking behavior\n",
    "\n",
    "    # get_db() method removed - use store methods instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L269){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.exists\n",
       "\n",
       ">      WhooshStore.exists ()\n",
       "\n",
       "*Returns True if documents have been added to search index*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L269){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.exists\n",
       "\n",
       ">      WhooshStore.exists ()\n",
       "\n",
       "*Returns True if documents have been added to search index*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L276){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.add_documents\n",
       "\n",
       ">      WhooshStore.add_documents\n",
       ">                                 (docs:Sequence[langchain_core.documents.base.D\n",
       ">                                 ocument], limitmb:int=1024,\n",
       ">                                 optimize:bool=False, verbose:bool=True,\n",
       ">                                 **kwargs)\n",
       "\n",
       "*Indexes documents. Extra kwargs supplied to `TextStore.ix.writer`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| docs | Sequence |  | list of LangChain Documents |\n",
       "| limitmb | int | 1024 | maximum memory in  megabytes to use |\n",
       "| optimize | bool | False | whether or not to also opimize index |\n",
       "| verbose | bool | True | Set to False to disable progress bar |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L276){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.add_documents\n",
       "\n",
       ">      WhooshStore.add_documents\n",
       ">                                 (docs:Sequence[langchain_core.documents.base.D\n",
       ">                                 ocument], limitmb:int=1024,\n",
       ">                                 optimize:bool=False, verbose:bool=True,\n",
       ">                                 **kwargs)\n",
       "\n",
       "*Indexes documents. Extra kwargs supplied to `TextStore.ix.writer`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| docs | Sequence |  | list of LangChain Documents |\n",
       "| limitmb | int | 1024 | maximum memory in  megabytes to use |\n",
       "| optimize | bool | False | whether or not to also opimize index |\n",
       "| verbose | bool | True | Set to False to disable progress bar |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.add_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L293){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.remove_document\n",
       "\n",
       ">      WhooshStore.remove_document (value:str, field:str='id',\n",
       ">                                   optimize:bool=False)\n",
       "\n",
       "*Remove document with corresponding value and field.\n",
       "Default field is the id field.\n",
       "If optimize is True, index will be optimized.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L293){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.remove_document\n",
       "\n",
       ">      WhooshStore.remove_document (value:str, field:str='id',\n",
       ">                                   optimize:bool=False)\n",
       "\n",
       "*Remove document with corresponding value and field.\n",
       "Default field is the id field.\n",
       "If optimize is True, index will be optimized.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.remove_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L305){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.remove_source\n",
       "\n",
       ">      WhooshStore.remove_source (source:str, optimize:bool=False)\n",
       "\n",
       "*remove all documents associated with `source`.\n",
       "The `source` argument can either be the full path to\n",
       "document or a parent folder.  In the latter case,\n",
       "ALL documents in parent folder will be removed.\n",
       "If optimize is True, index will be optimized.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L305){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.remove_source\n",
       "\n",
       ">      WhooshStore.remove_source (source:str, optimize:bool=False)\n",
       "\n",
       "*remove all documents associated with `source`.\n",
       "The `source` argument can either be the full path to\n",
       "document or a parent folder.  In the latter case,\n",
       "ALL documents in parent folder will be removed.\n",
       "If optimize is True, index will be optimized.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.remove_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L316){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.update_documents\n",
       "\n",
       ">      WhooshStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update a set of documents (doc in index with same ID will be over-written)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| doc_dicts | dict | dictionary with keys 'page_content', 'source', 'id', etc. |\n",
       "| kwargs | VAR_KEYWORD |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L316){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.update_documents\n",
       "\n",
       ">      WhooshStore.update_documents (doc_dicts:dict, **kwargs)\n",
       "\n",
       "*Update a set of documents (doc in index with same ID will be over-written)*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| doc_dicts | dict | dictionary with keys 'page_content', 'source', 'id', etc. |\n",
       "| kwargs | VAR_KEYWORD |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.update_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L326){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.get_all_docs\n",
       "\n",
       ">      WhooshStore.get_all_docs ()\n",
       "\n",
       "*Returns a generator to iterate through all indexed documents*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L326){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.get_all_docs\n",
       "\n",
       ">      WhooshStore.get_all_docs ()\n",
       "\n",
       "*Returns a generator to iterate through all indexed documents*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.get_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L333){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.get_doc\n",
       "\n",
       ">      WhooshStore.get_doc (id:str)\n",
       "\n",
       "*Get an indexed record by ID*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L333){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.get_doc\n",
       "\n",
       ">      WhooshStore.get_doc (id:str)\n",
       "\n",
       "*Get an indexed record by ID*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.get_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L341){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.get_size\n",
       "\n",
       ">      WhooshStore.get_size (include_deleted:bool=False)\n",
       "\n",
       "*Gets size of index\n",
       "\n",
       "If include_deleted is True, will include deletd detects (prior to optimization).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L341){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.get_size\n",
       "\n",
       ">      WhooshStore.get_size (include_deleted:bool=False)\n",
       "\n",
       "*Gets size of index\n",
       "\n",
       "If include_deleted is True, will include deletd detects (prior to optimization).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.get_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L350){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.erase\n",
       "\n",
       ">      WhooshStore.erase (confirm=True)\n",
       "\n",
       "*Clears index*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L350){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.erase\n",
       "\n",
       ">      WhooshStore.erase (confirm=True)\n",
       "\n",
       "*Clears index*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.erase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L373){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.query\n",
       "\n",
       ">      WhooshStore.query (q:str, fields:Sequence=['page_content'],\n",
       ">                         highlight:bool=True, limit:int=10, page:int=1,\n",
       ">                         return_dict:bool=False,\n",
       ">                         filters:Optional[Dict[str,str]]=None,\n",
       ">                         where_document:Optional[str]=None,\n",
       ">                         return_generator=False, **kwargs)\n",
       "\n",
       "*Queries the index\n",
       "\n",
       "**Args**\n",
       "\n",
       "- *q*: the query string\n",
       "- *fields*: a list of fields to search\n",
       "- *highlight*: If True, highlight hits\n",
       "- *limit*: results per page\n",
       "- *page*: page of hits to return\n",
       "- *return_dict*: If True, return list of dictionaries instead of LangChain Document objects\n",
       "- *filters*: filter results by field values (e.g., {'extension':'pdf'})\n",
       "- *where_document*: optional query to further filter results\n",
       "- *return_generator*: If True, returns a generator, not a list*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L373){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.query\n",
       "\n",
       ">      WhooshStore.query (q:str, fields:Sequence=['page_content'],\n",
       ">                         highlight:bool=True, limit:int=10, page:int=1,\n",
       ">                         return_dict:bool=False,\n",
       ">                         filters:Optional[Dict[str,str]]=None,\n",
       ">                         where_document:Optional[str]=None,\n",
       ">                         return_generator=False, **kwargs)\n",
       "\n",
       "*Queries the index\n",
       "\n",
       "**Args**\n",
       "\n",
       "- *q*: the query string\n",
       "- *fields*: a list of fields to search\n",
       "- *highlight*: If True, highlight hits\n",
       "- *limit*: results per page\n",
       "- *page*: page of hits to return\n",
       "- *return_dict*: If True, return list of dictionaries instead of LangChain Document objects\n",
       "- *filters*: filter results by field values (e.g., {'extension':'pdf'})\n",
       "- *where_document*: optional query to further filter results\n",
       "- *return_generator*: If True, returns a generator, not a list*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L464){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.semantic_search\n",
       "\n",
       ">      WhooshStore.semantic_search (query, k:int=4, n_candidates=50,\n",
       ">                                   filters:Optional[Dict[str,str]]=None,\n",
       ">                                   where_document:Optional[str]=None, **kwargs)\n",
       "\n",
       "*Retrieves results based on semantic similarity to supplied `query`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query |  |  |  |\n",
       "| k | int | 4 | number of records to return based on highest semantic similarity scores. |\n",
       "| n_candidates | int | 50 | Number of records to consider (for which we compute embeddings on-the-fly) |\n",
       "| filters | Optional | None | filter sources by field values (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | a boolean query to filter results further (e.g., \"climate\" AND extension:pdf) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/stores/sparse.py#L464){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### WhooshStore.semantic_search\n",
       "\n",
       ">      WhooshStore.semantic_search (query, k:int=4, n_candidates=50,\n",
       ">                                   filters:Optional[Dict[str,str]]=None,\n",
       ">                                   where_document:Optional[str]=None, **kwargs)\n",
       "\n",
       "*Retrieves results based on semantic similarity to supplied `query`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query |  |  |  |\n",
       "| k | int | 4 | number of records to return based on highest semantic similarity scores. |\n",
       "| n_candidates | int | 50 | Number of records to consider (for which we compute embeddings on-the-fly) |\n",
       "| filters | Optional | None | filter sources by field values (e.g., {'table':True}) |\n",
       "| where_document | Optional | None | a boolean query to filter results further (e.g., \"climate\" AND extension:pdf) |\n",
       "| kwargs | VAR_KEYWORD |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.check\n",
       "\n",
       ">      VectorStore.check ()\n",
       "\n",
       "*Raise exception if `VectorStore.exists()` returns False*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest/base.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### VectorStore.ingest\n",
       "\n",
       ">      VectorStore.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                          chunk_overlap:int=50,\n",
       ">                          ignore_fn:Optional[Callable]=None,\n",
       ">                          batch_size:int=41000, **kwargs)\n",
       "\n",
       "*Ingests all documents in `source_directory` (previously-ingested documents are\n",
       "ignored). When retrieved, the\n",
       "[Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
       "objects will each have a `metadata` dict with the absolute path to the file\n",
       "in `metadata[\"source\"]`.\n",
       "Extra kwargs fed to `ingest.load_single_document`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| source_directory | str |  | path to folder containing document store |\n",
       "| chunk_size | int | 500 | text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) |\n",
       "| chunk_overlap | int | 50 | character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter` |\n",
       "| ignore_fn | Optional | None | Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested. |\n",
       "| batch_size | int | 41000 | batch size used when processing documents |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(WhooshStore.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
