{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.summarizer\n",
    "\n",
    "> Pipelines for specific tasks like summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union, Callable\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from onprem.ingest import load_single_document, load_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_MAP_PROMPT = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please write a concise summary. \n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_REDUCE_PROMPT = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary. \n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_BASE_REFINE_PROMPT = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "DEFAULT_REFINE_PROMPT = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary.\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class Summarizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        prompt_template: Optional[str] = None,              \n",
    "        map_prompt: Optional[str] = None,\n",
    "        reduce_prompt: Optional[str] = None,\n",
    "        refine_prompt: Optional[str] = None, \n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        `Summarizer` summarizes one or more documents\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *llm*: An `onprem.LLM` object\n",
    "        - *prompt_template*: A model specific prompt_template with a single placeholder named \"{prompt}\".\n",
    "                             All prompts (e.g., Map-Reduce prompts) are wrapped within this prompt.\n",
    "                             If supplied, overrides the `prompt_template` supplied to the `LLM` constructor.\n",
    "        - *map_prompt*: Map prompt for Map-Reduce summarization. If None, default is used.\n",
    "        - *reduce_prompt*: Reduce prompt for Map-Reduce summarization. If None, default is used.\n",
    "        - *refine_prompt*: Refine prompt for Refine-based summarization. If None, default is used.\n",
    "\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template if prompt_template is not None else llm.prompt_template\n",
    "        self.map_prompt = map_prompt if map_prompt else DEFAULT_MAP_PROMPT\n",
    "        self.reduce_prompt = reduce_prompt if reduce_prompt else DEFAULT_REDUCE_PROMPT\n",
    "        self.refine_prompt = refine_prompt if refine_prompt else DEFAULT_REFINE_PROMPT\n",
    "\n",
    "\n",
    "    def summarize(self, \n",
    "                  fpath:str, \n",
    "                  strategy:str='map_reduce',\n",
    "                  chunk_size:int=1000, \n",
    "                  chunk_overlap:int=0, \n",
    "                  token_max:int=2000,\n",
    "                  max_chunks_to_use: Optional[int] = None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
    "        using either Langchain's Map-Reduce strategy or Refine strategy.\n",
    "\n",
    "        **Args:**\n",
    "\n",
    "        - *fpath*: A path to either a folder of documents or a single file.\n",
    "        - *strategy*: One of {'map_reduce', 'refine'}. \n",
    "        - *chunk_size*: Number of characters of each chunk to summarize\n",
    "        - *chunk_overlap*: Number of characters that overlap between chunks\n",
    "        - *token_max*: Maximum number of tokens to group documents into\n",
    "        - *max_chunks_to_use*: Maximum number of chunks (starting from beginning) to use.\n",
    "                               Useful for documents that have abstracts or informative introductions.\n",
    "                               If None, all chunks are considered for summarizer.\n",
    "\n",
    "        **Returns:**\n",
    "\n",
    "        - str: a summary of your documents\n",
    "        \"\"\"\n",
    "          \n",
    "        if os.path.isfile(fpath):\n",
    "            docs = load_single_document(fpath)\n",
    "        else:\n",
    "            docs = load_documents(fpath)\n",
    "\n",
    "        if strategy == 'map_reduce':\n",
    "            summary = self._map_reduce(docs, \n",
    "                                      chunk_size=chunk_size, \n",
    "                                      chunk_overlap=chunk_overlap, \n",
    "                                      token_max=token_max,\n",
    "                                      max_chunks_to_use=max_chunks_to_use)\n",
    "        elif strategy == 'refine':\n",
    "            summary = self._refine(docs, \n",
    "                                   chunk_size=chunk_size, \n",
    "                                   chunk_overlap=chunk_overlap, \n",
    "                                   token_max=token_max,\n",
    "                                   max_chunks_to_use=max_chunks_to_use)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'Unknown strategy: {self.strategy}')\n",
    "        return summary\n",
    "\n",
    "    \n",
    "    def _map_reduce(self, docs, chunk_size=1000, chunk_overlap=0, token_max=1000, \n",
    "                    max_chunks_to_use = None, **kwargs):\n",
    "        \"\"\" Map-Reduce summarization\"\"\"\n",
    "        langchain_llm = self.llm.llm\n",
    "\n",
    "        # Map\n",
    "        # map_template = \"\"\"The following is a set of documents\n",
    "        # {docs}\n",
    "        # Based on this list of docs, please identify the main themes \n",
    "        # Helpful Answer:\"\"\"\n",
    "        # map_template = \"\"\"The following is a set of documents\n",
    "        # {docs}\n",
    "        # Based on this list of docs, please write a concise summary.\n",
    "        # CONCISE SUMMARY:\"\"\"\n",
    "        map_template = self.map_prompt\n",
    "        if self.prompt_template: \n",
    "            map_template = self.prompt_template.format(**{'prompt':map_template})\n",
    "        map_prompt = PromptTemplate.from_template(map_template)\n",
    "        map_chain = LLMChain(llm=langchain_llm, prompt=map_prompt)\n",
    "\n",
    "        # Reduce\n",
    "        # reduce_template = \"\"\"The following is set of summaries:\n",
    "        # {docs}\n",
    "        # Take these and distill it into a final, consolidated summary. \n",
    "        # SUMMARY:\"\"\"\n",
    "        reduce_template = self.reduce_prompt\n",
    "        if self.prompt_template:\n",
    "            reduce_template = self.prompt_template.format(**{'prompt':reduce_template})\n",
    "        reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "        # Run chain\n",
    "        reduce_chain = LLMChain(llm=langchain_llm, prompt=reduce_prompt)\n",
    "        \n",
    "        # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "        combine_documents_chain = StuffDocumentsChain(\n",
    "            llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    "        )\n",
    "        \n",
    "        # Combines and iteravely reduces the mapped documents\n",
    "        reduce_documents_chain = ReduceDocumentsChain(\n",
    "            # This is final chain that is called.\n",
    "            combine_documents_chain=combine_documents_chain,\n",
    "            # If documents exceed context for `StuffDocumentsChain`\n",
    "            collapse_documents_chain=combine_documents_chain,\n",
    "            # The maximum number of tokens to group documents into.\n",
    "            token_max=token_max)\n",
    "\n",
    "        # Combining documents by mapping a chain over them, then combining results\n",
    "        map_reduce_chain = MapReduceDocumentsChain(\n",
    "            # Map chain\n",
    "            llm_chain=map_chain,\n",
    "            # Reduce chain\n",
    "            reduce_documents_chain=reduce_documents_chain,\n",
    "            # The variable name in the llm_chain to put the documents in\n",
    "            document_variable_name=\"docs\",\n",
    "            # Return the results of the map steps in the output\n",
    "            return_intermediate_steps=False,\n",
    "        )\n",
    "        \n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        split_docs = split_docs[:max_chunks_to_use] if max_chunks_to_use else split_docs\n",
    "\n",
    "        return map_reduce_chain.invoke(split_docs)\n",
    "\n",
    "    def _refine(self, docs, chunk_size=1000, chunk_overlap=0, \n",
    "                max_chunks_to_use = None, **kwargs):\n",
    "        \"\"\" Refine summarization\"\"\"\n",
    "\n",
    "        # initial_template = \"\"\"Write a concise summary of the following:\n",
    "        # {text}\n",
    "        # CONCISE SUMMARY:\"\"\"\n",
    "        initial_template = self.refine_base_prompt\n",
    "        if self.prompt_template:\n",
    "            initial_template = self.prompt_template.format(**{'prompt':initial_template})\n",
    "        prompt = PromptTemplate.from_template(initial_template)\n",
    "        \n",
    "        # refine_template = (\n",
    "        #     \"Your job is to produce a final summary\\n\"\n",
    "        #     \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "        #     \"We have the opportunity to refine the existing summary\"\n",
    "        #     \"(only if needed) with some more context below.\\n\"\n",
    "        #     \"------------\\n\"\n",
    "        #     \"{text}\\n\"\n",
    "        #     \"------------\\n\"\n",
    "        #     \"Given the new context, refine the original summary.\"\n",
    "        #     \"If the context isn't useful, return the original summary.\"\n",
    "        # )\n",
    "        refine_template = self.refine_prompt\n",
    "        if self.prompt_template:\n",
    "            refine_template = self.prompt_template.format(**{'prompt':refine_template})\n",
    "        refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "        chain = load_summarize_chain(\n",
    "            llm=self.llm.llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=prompt,\n",
    "            refine_prompt=refine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            input_key=\"input_documents\",\n",
    "            output_key=\"output_text\",\n",
    "        )\n",
    "        \n",
    "        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        split_docs = split_docs[:max_chunks_to_use] if max_chunks_to_use else split_docs\n",
    "        result = chain({\"input_documents\": split_docs}, return_only_outputs=True)\n",
    "        print(result)\n",
    "        return result['output_text']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/summarizer.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Summarizer.summarize\n",
       "\n",
       ">      Summarizer.summarize (fpath:str, strategy:str='map_reduce',\n",
       ">                            chunk_size:int=1000, chunk_overlap:int=0,\n",
       ">                            token_max:int=2000,\n",
       ">                            max_chunks_to_use:Optional[int]=None)\n",
       "\n",
       "*Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
       "using either Langchain's Map-Reduce strategy or Refine strategy.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *fpath*: A path to either a folder of documents or a single file.\n",
       "- *strategy*: One of {'map_reduce', 'refine'}. \n",
       "- *chunk_size*: Number of characters of each chunk to summarize\n",
       "- *chunk_overlap*: Number of characters that overlap between chunks\n",
       "- *token_max*: Maximum number of tokens to group documents into\n",
       "- *max_chunks_to_use*: Maximum number of chunks (starting from beginning) to use.\n",
       "                       Useful for documents that have abstracts or informative introductions.\n",
       "                       If None, all chunks are considered for summarizer.\n",
       "\n",
       "**Returns:**\n",
       "\n",
       "- str: a summary of your documents*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/summarizer.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Summarizer.summarize\n",
       "\n",
       ">      Summarizer.summarize (fpath:str, strategy:str='map_reduce',\n",
       ">                            chunk_size:int=1000, chunk_overlap:int=0,\n",
       ">                            token_max:int=2000,\n",
       ">                            max_chunks_to_use:Optional[int]=None)\n",
       "\n",
       "*Summarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text)\n",
       "using either Langchain's Map-Reduce strategy or Refine strategy.\n",
       "\n",
       "**Args:**\n",
       "\n",
       "- *fpath*: A path to either a folder of documents or a single file.\n",
       "- *strategy*: One of {'map_reduce', 'refine'}. \n",
       "- *chunk_size*: Number of characters of each chunk to summarize\n",
       "- *chunk_overlap*: Number of characters that overlap between chunks\n",
       "- *token_max*: Maximum number of tokens to group documents into\n",
       "- *max_chunks_to_use*: Maximum number of chunks (starting from beginning) to use.\n",
       "                       Useful for documents that have abstracts or informative introductions.\n",
       "                       If None, all chunks are considered for summarizer.\n",
       "\n",
       "**Returns:**\n",
       "\n",
       "- str: a summary of your documents*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Summarizer.summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
