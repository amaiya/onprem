{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os.path\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "#--------------------------------------\n",
    "# App Utilities\n",
    "#--------------------------------------\n",
    "\n",
    "def download(url, filename, verify=False):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        response = requests.get(url, stream=True, verify=verify)\n",
    "        total = response.headers.get(\"content-length\")\n",
    "\n",
    "        if total is None:\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            downloaded = 0\n",
    "            total = int(total)\n",
    "            # print(total)\n",
    "            for data in response.iter_content(\n",
    "                chunk_size=max(int(total / 1000), 1024 * 1024)\n",
    "            ):\n",
    "                downloaded += len(data)\n",
    "                f.write(data)\n",
    "                done = int(50 * downloaded / total)\n",
    "                sys.stdout.write(\"\\r[{}{}]\".format(\"â–ˆ\" * done, \".\" * (50 - done)))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "def get_datadir():\n",
    "    home = os.path.expanduser(\"~\")\n",
    "    datadir = os.path.join(home, \"onprem_data\")\n",
    "    if not os.path.isdir(datadir):\n",
    "        os.mkdir(datadir)\n",
    "    return datadir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "#--------------------------------------\n",
    "# Data Utilities\n",
    "#--------------------------------------\n",
    "def split_list(input_list, chunk_size):\n",
    "    \"\"\"\n",
    "    Split list into chunks\n",
    "    \"\"\"\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        yield input_list[i : i + chunk_size]\n",
    "\n",
    "\n",
    "from syntok import segmenter\n",
    "import textwrap\n",
    "def segment(text:str, unit:str='paragraph', maxchars:int=2048):\n",
    "    \"\"\"\n",
    "    Segments text into a list of paragraphs or sentences depending on value of `unit` \n",
    "    (one of `{'paragraph', 'sentence'}`. The `maxchars` parameter is the maximum size\n",
    "    of any unit of text.\n",
    "    \"\"\"\n",
    "    units = []\n",
    "    for paragraph in segmenter.analyze(text):\n",
    "        sentences = []\n",
    "        for sentence in paragraph:\n",
    "            text = \"\"\n",
    "            for token in sentence:\n",
    "                text += f'{token.spacing}{token.value}'\n",
    "            sentences.append(text)\n",
    "        if unit == 'sentence':\n",
    "            units.extend(sentences)\n",
    "        else:\n",
    "            units.append(\" \".join(sentences))\n",
    "    chunks = []\n",
    "    for s in units:\n",
    "        parts = textwrap.wrap(s, maxchars, break_long_words=False)\n",
    "        chunks.extend(parts)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def md_to_df(md_str: str) -> Any:\n",
    "    \"\"\"Convert Markdown to dataframe.\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"You must install the `pandas` package to use this node parser.\"\n",
    "        )\n",
    "\n",
    "    # Replace \" by \"\" in md_str\n",
    "    md_str = md_str.replace('\"', '\"\"')\n",
    "\n",
    "    # Replace markdown pipe tables with commas\n",
    "    md_str = md_str.replace(\"|\", '\",\"')\n",
    "\n",
    "    # Remove the second line (table header separator)\n",
    "    lines = md_str.split(\"\\n\")\n",
    "    md_str = \"\\n\".join(lines[:1] + lines[2:])\n",
    "\n",
    "    # Remove the first and last second char of the line (the pipes, transformed to \",\")\n",
    "    lines = md_str.split(\"\\n\")\n",
    "    md_str = \"\\n\".join([line[2:-2] for line in lines])\n",
    "\n",
    "    # Check if the table is empty\n",
    "    if len(md_str) == 0:\n",
    "        return None\n",
    "\n",
    "    # Use pandas to read the CSV string into a DataFrame\n",
    "    return pd.read_csv(StringIO(md_str))\n",
    "\n",
    "\n",
    "def html_to_df(html_str: str) -> Any:\n",
    "    \"\"\"Convert HTML to dataframe.\"\"\"\n",
    "    try:\n",
    "        from lxml import html\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"You must install the `lxml` package to use this node parser.\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"You must install the `pandas` package to use this node parser.\"\n",
    "        )\n",
    "\n",
    "    tree = html.fromstring(html_str)\n",
    "    table_element = tree.xpath(\"//table\")[0]\n",
    "    rows = table_element.xpath(\".//tr\")\n",
    "    try:\n",
    "        colnames = table_element.xpath(\".//th\")\n",
    "        colnames = [col.text for col in colnames]\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        colnames = []\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.xpath(\".//td\")\n",
    "        cols = [c.text.strip() if c.text is not None else \"\" for c in cols]\n",
    "        if len(cols) == 0: continue\n",
    "        data.append(cols)\n",
    "\n",
    "    # Check if the table is empty\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "\n",
    "    # Check if the all rows have the same number of columns\n",
    "    if not all(len(row) == len(data[0]) for row in data):\n",
    "        return None\n",
    "    if colnames:\n",
    "        return pd.DataFrame(data[0:], columns=colnames)\n",
    "    else:\n",
    "        return pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "\n",
    "CAPTION_STR = \"The following table in markdown format has the caption\"\n",
    "def df_to_md(df, caption=None):\n",
    "    \"\"\"\n",
    "    Converts pd.Dataframe to markdown\n",
    "    \"\"\"\n",
    "    table_md = \"|\"\n",
    "    for col_name, col in df.items():\n",
    "        table_md += f\"{col_name}|\"\n",
    "    table_md += \"\\n|\"\n",
    "    for col_name, col in df.items():\n",
    "        table_md += f\"---|\"\n",
    "    table_md += \"\\n\"\n",
    "    for row in df.itertuples():\n",
    "        table_md += \"|\"\n",
    "        for col in row[1:]:\n",
    "            table_md += f\"{col}|\"\n",
    "        table_md += \"\\n\"\n",
    "    if caption:\n",
    "        table_summary = (\n",
    "            f\"{CAPTION_STR}: {caption}.\"\n",
    "        )\n",
    "        print('\\n\\n')\n",
    "    table_summary += f\"The following table in markdown format includes this list of columns:\\n\"\n",
    "    for col in df.columns:\n",
    "        table_summary += f\"- {col}\\n\"\n",
    "\n",
    "    return table_summary + \"\\n\" + table_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "\n",
    "#--------------------------------------\n",
    "# Prompt Utilities\n",
    "#--------------------------------------\n",
    "\n",
    "class SafeFormatter:\n",
    "    \"\"\"\n",
    "    Safe string formatter that does not raise KeyError if key is missing.\n",
    "    Adapted from llama_index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, format_dict: Optional[Dict[str, str]] = None):\n",
    "        self.format_dict = format_dict or {}\n",
    "\n",
    "    def format(self, format_string: str) -> str:\n",
    "        return re.sub(r\"\\{([^{}]+)\\}\", self._replace_match, format_string)\n",
    "\n",
    "    def parse(self, format_string: str) -> List[str]:\n",
    "        return re.findall(r\"\\{([^{}]+)\\}\", format_string)\n",
    "\n",
    "    def _replace_match(self, match: re.Match) -> str:\n",
    "        key = match.group(1)\n",
    "        return str(self.format_dict.get(key, match.group(0)))\n",
    "\n",
    "\n",
    "def format_string(string_to_format: str, **kwargs: str) -> str:\n",
    "    \"\"\"Format a string with kwargs\"\"\"\n",
    "    formatter = SafeFormatter(format_dict=kwargs)\n",
    "    return formatter.format(string_to_format)\n",
    "\n",
    "\n",
    "def get_template_vars(template_str: str) -> List[str]:\n",
    "    \"\"\"Get template variables from a template string.\"\"\"\n",
    "    variables = []\n",
    "    formatter = SafeFormatter()\n",
    "\n",
    "    for variable_name in formatter.parse(template_str):\n",
    "        if variable_name:\n",
    "            variables.append(variable_name)\n",
    "\n",
    "    return [v for v in variables if \" \" not in v and \"\\n\" not in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
