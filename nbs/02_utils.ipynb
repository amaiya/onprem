{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import requests\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "#--------------------------------------\n",
    "# App Utilities\n",
    "#--------------------------------------\n",
    "DEFAULT_DB = \"vectordb\"\n",
    "\n",
    "def download(url, filename, verify=False):\n",
    "    folder = os.path.dirname(os.path.abspath(filename))\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        response = requests.get(url, stream=True, verify=verify)\n",
    "        total = response.headers.get(\"content-length\")\n",
    "\n",
    "        if total is None:\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            downloaded = 0\n",
    "            total = int(total)\n",
    "            # print(total)\n",
    "            for data in response.iter_content(\n",
    "                chunk_size=max(int(total / 1000), 1024 * 1024)\n",
    "            ):\n",
    "                downloaded += len(data)\n",
    "                f.write(data)\n",
    "                done = int(50 * downloaded / total)\n",
    "                sys.stdout.write(\"\\r[{}{}]\".format(\"â–ˆ\" * done, \".\" * (50 - done)))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "# Common subfolder names\n",
    "MODELS_SUBDIR = \"models\"\n",
    "WEBAPP_SUBDIR = \"webapp\"\n",
    "\n",
    "def get_datadir(subfolder=None):\n",
    "    \"\"\"\n",
    "    Get the data directory path, optionally with a subfolder.\n",
    "    Creates the main data dir and any requested subfolder if they don't exist.\n",
    "    \n",
    "    Args:\n",
    "        subfolder: Optional subfolder name to append to the data directory path\n",
    "        \n",
    "    Returns:\n",
    "        Path to the data directory or subfolder\n",
    "    \"\"\"\n",
    "    home = os.path.expanduser(\"~\")\n",
    "    datadir = os.path.join(home, \"onprem_data\")\n",
    "    if not os.path.isdir(datadir):\n",
    "        os.mkdir(datadir)\n",
    "    \n",
    "    # If no subfolder requested, just return the main data dir\n",
    "    if subfolder is None:\n",
    "        return datadir\n",
    "    \n",
    "    # Create and return the subfolder path\n",
    "    subfolder_path = os.path.join(datadir, subfolder)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        os.mkdir(subfolder_path)\n",
    "    \n",
    "    return subfolder_path\n",
    "\n",
    "def get_models_dir():\n",
    "    \"\"\"Get the models directory path\"\"\"\n",
    "    return get_datadir(MODELS_SUBDIR)\n",
    "\n",
    "def get_webapp_dir():\n",
    "    \"\"\"Get the webapp directory path\"\"\"\n",
    "    return get_datadir(WEBAPP_SUBDIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "#--------------------------------------\n",
    "# Data Utilities\n",
    "#--------------------------------------\n",
    "def batch_list(input_list, batch_size):\n",
    "    \"\"\"\n",
    "    Split list into chunks\n",
    "    \"\"\"\n",
    "    for i in range(0, len(input_list), batch_size):\n",
    "        yield input_list[i : i + batch_size]\n",
    "\n",
    "\n",
    "def batch_generator(iterable, batch_size):\n",
    "    \"\"\"\n",
    "    Batched results from generator\n",
    "    \"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        batch = list(itertools.islice(iterator, batch_size))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def filtered_generator(generator, criteria=[]):\n",
    "    \"\"\"\n",
    "    Filters a generator based on a given predicate function.\n",
    "\n",
    "    Args:\n",
    "        generator: The generator to filter.\n",
    "        criteria: List of functions that take an element from the generator \n",
    "                   and return True if the element should be included, \n",
    "                   False otherwise.\n",
    "\n",
    "    Yields:\n",
    "        Elements from the original generator that satisfy the predicate.\n",
    "    \"\"\"\n",
    "    for item in generator:\n",
    "        if all(criterion(item) for criterion in criteria):\n",
    "            yield item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from syntok import segmenter\n",
    "import textwrap\n",
    "def segment(text:str, unit:str='paragraph', maxchars:int=2048):\n",
    "    \"\"\"\n",
    "    Segments text into a list of paragraphs or sentences depending on value of `unit` \n",
    "    (one of `{'paragraph', 'sentence'}`. The `maxchars` parameter is the maximum size\n",
    "    of any unit of text.\n",
    "    \"\"\"\n",
    "    units = []\n",
    "    for paragraph in segmenter.analyze(text):\n",
    "        sentences = []\n",
    "        for sentence in paragraph:\n",
    "            text = \"\"\n",
    "            for token in sentence:\n",
    "                text += f'{token.spacing}{token.value}'\n",
    "            sentences.append(text)\n",
    "        if unit == 'sentence':\n",
    "            units.extend(sentences)\n",
    "        else:\n",
    "            units.append(\" \".join(sentences))\n",
    "    chunks = []\n",
    "    for s in units:\n",
    "        parts = textwrap.wrap(s, maxchars, break_long_words=False)\n",
    "        chunks.extend(parts)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def remove_sentence(sentence, text, remove_follow=False, flags=re.IGNORECASE):\n",
    "    \"\"\"\n",
    "    Removes a sentence or phrase from text ignoring whether\n",
    "    tokens are delimited by spaces or newlines or tabs.\n",
    "\n",
    "    If  `remove_follow=True`, then subsequent text until the first newline\n",
    "    is also removed.\n",
    "    \"\"\"\n",
    "    if remove_follow:\n",
    "    \tpattern = r'\\s*'.join(map(re.escape, sentence.split())) + r'[\\s\\S]*?(?:\\n\\s*){2,}'\n",
    "    \treturn re.sub(pattern, '\\n\\n', text, flags=flags).strip()\n",
    "    else:\n",
    "        pattern = r'\\s*'.join(map(re.escape, sentence.split())) + r'\\s*'\n",
    "        return re.sub(pattern, '', text, flags=flags)\n",
    "\n",
    "\n",
    "def contains_sentence(sentence, text):\n",
    "    \"\"\"\n",
    "    Returns True if sentence is contained in text ignoring whether\n",
    "    tokens are delmited by spaces or newlines or tabs.\n",
    "    \"\"\"\n",
    "    pattern = r'\\s*'.join(map(re.escape, sentence.split())) + r'\\s*'\n",
    "    return re.search(pattern, text, flags=re.IGNORECASE) is not None\n",
    "\n",
    "\n",
    "\n",
    "def extract_noun_phrases(text: str):\n",
    "    \"\"\"\n",
    "    Extracts noun phrases from text, including coordinated phrases like\n",
    "    \"generative AI and live fire testing\", and removes subphrases like \"AI\"\n",
    "    if \"generative AI\" is also found.\n",
    "    Example:\n",
    "    text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, \"\n",
    "           \"and computational linguistics concerned with the interactions between computers and human \"\n",
    "           \"(natural) languages.\"\n",
    "    extract_noun_phrases(text)\n",
    "    ['Natural language processing',\n",
    "\t 'NLP',\n",
    "\t 'field',\n",
    "\t 'computer science',\n",
    "\t 'artificial intelligence',\n",
    "\t 'computational linguistics',\n",
    "\t 'interactions',\n",
    "\t 'computers',\n",
    "\t 'languages',\n",
    "\t 'human']\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "    import contextlib\n",
    "\n",
    "    # Ensure NLTK data is available\n",
    "    RESOURCE_PATHS = {\n",
    "        'punkt': 'tokenizers/punkt',\n",
    "        'averaged_perceptron_tagger': 'taggers/averaged_perceptron_tagger'\n",
    "    }\n",
    "\n",
    "    def safe_nltk_download(resource_id):\n",
    "        path = RESOURCE_PATHS[resource_id]\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            with contextlib.redirect_stdout(None), contextlib.redirect_stderr(None):\n",
    "                nltk.download(resource_id, quiet=True)\n",
    "\n",
    "    safe_nltk_download(\"punkt\")\n",
    "    safe_nltk_download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    # Grammar: includes compound nouns, gerunds, and adjectives\n",
    "    grammar = r\"\"\"\n",
    "        NP:\n",
    "            {<JJ.*>*<NN.*>+}           # adjectives followed by nouns\n",
    "            {<NN.*>+<NN.*>}            # noun-noun compounds\n",
    "            {<JJ.*>*<VBG><NN.*>?}      # gerund-based phrases\n",
    "    \"\"\"\n",
    "\n",
    "    chunker = RegexpParser(grammar)\n",
    "    tree = chunker.parse(tagged)\n",
    "\n",
    "    # Extract basic NPs\n",
    "    raw_noun_phrases = [\n",
    "        \" \".join(word for word, tag in subtree.leaves())\n",
    "        for subtree in tree.subtrees()\n",
    "        if subtree.label() == \"NP\"\n",
    "    ]\n",
    "\n",
    "    # Recover coordinated phrases: X and Y => [\"X\", \"Y\"]\n",
    "    def recover_coord_phrases(tagged_tokens):\n",
    "        phrases = []\n",
    "        buffer = []\n",
    "        for word, tag in tagged_tokens:\n",
    "            if tag.startswith('JJ') or tag.startswith('NN') or tag == 'VBG':\n",
    "                buffer.append(word)\n",
    "            elif word.lower() == 'and' and buffer:\n",
    "                phrases.append(\" \".join(buffer))\n",
    "                buffer = []\n",
    "            else:\n",
    "                if buffer:\n",
    "                    phrases.append(\" \".join(buffer))\n",
    "                    buffer = []\n",
    "        if buffer:\n",
    "            phrases.append(\" \".join(buffer))\n",
    "        return phrases\n",
    "\n",
    "    fuzzy_phrases = recover_coord_phrases(tagged)\n",
    "\n",
    "    # Merge and deduplicate\n",
    "    seen = set()\n",
    "    all_phrases = []\n",
    "    for phrase in raw_noun_phrases + fuzzy_phrases:\n",
    "        phrase = phrase.strip()\n",
    "        lower = phrase.lower()\n",
    "        if lower not in seen:\n",
    "            seen.add(lower)\n",
    "            all_phrases.append(phrase)\n",
    "\n",
    "    # Remove subphrases of longer phrases\n",
    "    final_phrases = []\n",
    "    for phrase in all_phrases:\n",
    "        if not any(\n",
    "            phrase != other and phrase.lower() in other.lower()\n",
    "            for other in all_phrases\n",
    "        ):\n",
    "            final_phrases.append(phrase)\n",
    "\n",
    "    return final_phrases\n",
    "\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def md_to_df(md_str: str) -> Any:\n",
    "    \"\"\"Convert Markdown to dataframe.\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"You must install the `pandas` package to use this node parser.\"\n",
    "        )\n",
    "\n",
    "    # Replace \" by \"\" in md_str\n",
    "    md_str = md_str.replace('\"', '\"\"')\n",
    "\n",
    "    # Replace markdown pipe tables with commas\n",
    "    md_str = md_str.replace(\"|\", '\",\"')\n",
    "\n",
    "    # Remove the second line (table header separator)\n",
    "    lines = md_str.split(\"\\n\")\n",
    "    md_str = \"\\n\".join(lines[:1] + lines[2:])\n",
    "\n",
    "    # Remove the first and last second char of the line (the pipes, transformed to \",\")\n",
    "    lines = md_str.split(\"\\n\")\n",
    "    md_str = \"\\n\".join([line[2:-2] for line in lines])\n",
    "\n",
    "    # Check if the table is empty\n",
    "    if len(md_str) == 0:\n",
    "        return None\n",
    "\n",
    "    # Use pandas to read the CSV string into a DataFrame\n",
    "    return pd.read_csv(StringIO(md_str))\n",
    "\n",
    "\n",
    "def html_to_df(html_str: str) -> Any:\n",
    "    \"\"\"Convert HTML to dataframe.\"\"\"\n",
    "    try:\n",
    "        from lxml import html\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"You must install the `lxml` package to use this node parser.\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"You must install the `pandas` package to use this node parser.\"\n",
    "        )\n",
    "\n",
    "    tree = html.fromstring(html_str)\n",
    "    table_element = tree.xpath(\"//table\")[0]\n",
    "    rows = table_element.xpath(\".//tr\")\n",
    "    try:\n",
    "        colnames = table_element.xpath(\".//th\")\n",
    "        colnames = [col.text for col in colnames]\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        colnames = []\n",
    "\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.xpath(\".//td\")\n",
    "        cols = [c.text.strip() if c.text is not None else \"\" for c in cols]\n",
    "        if len(cols) == 0: continue\n",
    "        data.append(cols)\n",
    "\n",
    "    # Check if the table is empty\n",
    "    if len(data) == 0:\n",
    "        return None\n",
    "\n",
    "    # Check if the all rows have the same number of columns\n",
    "    if not all(len(row) == len(data[0]) for row in data):\n",
    "        return None\n",
    "    if colnames:\n",
    "        return pd.DataFrame(data[0:], columns=colnames)\n",
    "    else:\n",
    "        return pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "\n",
    "CAPTION_STR = \"The following table in markdown format has the caption\"\n",
    "def df_to_md(df, caption=None):\n",
    "    \"\"\"\n",
    "    Converts pd.Dataframe to markdown\n",
    "    \"\"\"\n",
    "    table_md = \"|\"\n",
    "    for col_name, col in df.items():\n",
    "        table_md += f\"{col_name}|\"\n",
    "    table_md += \"\\n|\"\n",
    "    for col_name, col in df.items():\n",
    "        table_md += f\"---|\"\n",
    "    table_md += \"\\n\"\n",
    "    for row in df.itertuples():\n",
    "        table_md += \"|\"\n",
    "        for col in row[1:]:\n",
    "            table_md += f\"{col}|\"\n",
    "        table_md += \"\\n\"\n",
    "    if caption:\n",
    "        table_summary = f\"{CAPTION_STR}: {caption}\\n\"\n",
    "    else:\n",
    "        table_summary = \"\"\n",
    "    table_summary += f\"The following table in markdown format includes this list of columns:\\n\"\n",
    "    for col in df.columns:\n",
    "        table_summary += f\"- {col}\\n\"\n",
    "\n",
    "    return f'{caption}\\n\\n{table_summary}\\n{table_md}' if caption else f'{table_summary}\\n{table_md}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "\n",
    "#--------------------------------------\n",
    "# Prompt Utilities\n",
    "#--------------------------------------\n",
    "\n",
    "class SafeFormatter:\n",
    "    \"\"\"\n",
    "    Safe string formatter that does not raise KeyError if key is missing.\n",
    "    Adapted from llama_index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, format_dict: Optional[Dict[str, str]] = None):\n",
    "        self.format_dict = format_dict or {}\n",
    "\n",
    "    def format(self, format_string: str) -> str:\n",
    "        return re.sub(r\"\\{([^{}]+)\\}\", self._replace_match, format_string)\n",
    "\n",
    "    def parse(self, format_string: str) -> List[str]:\n",
    "        return re.findall(r\"\\{([^{}]+)\\}\", format_string)\n",
    "\n",
    "    def _replace_match(self, match: re.Match) -> str:\n",
    "        key = match.group(1)\n",
    "        return str(self.format_dict.get(key, match.group(0)))\n",
    "\n",
    "\n",
    "def format_string(string_to_format: str, **kwargs: str) -> str:\n",
    "    \"\"\"Format a string with kwargs\"\"\"\n",
    "    formatter = SafeFormatter(format_dict=kwargs)\n",
    "    return formatter.format(string_to_format)\n",
    "\n",
    "\n",
    "def get_template_vars(template_str: str) -> List[str]:\n",
    "    \"\"\"Get template variables from a template string.\"\"\"\n",
    "    variables = []\n",
    "    formatter = SafeFormatter()\n",
    "\n",
    "    for variable_name in formatter.parse(template_str):\n",
    "        if variable_name:\n",
    "            variables.append(variable_name)\n",
    "\n",
    "    return [v for v in variables if \" \" not in v and \"\\n\" not in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
