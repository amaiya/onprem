{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Built-In Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**[OnPrem.LLM](https://github.com/amaiya/onprem)** includes a built-in Web app to access and use LLMs. You can follow these steps to set it up and start it:\n",
    "\n",
    "#### Step 1: Ingest some documents using the Python API:\n",
    "```python\n",
    "# run at Python prompt\n",
    "from onprem import LLM\n",
    "llm = LLM()\n",
    "llm.ingest('/your/folder/of/documents')\n",
    "```\n",
    "\n",
    "#### Step 2: Start the Web app:\n",
    "\n",
    "```shell\n",
    "# run at command-line\n",
    "onprem --port 8000\n",
    "```\n",
    "Then, enter `localhost:8000` (or `<domain_name>:8000` if running on remote server) in your Web browser to access the application:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_screenshot.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>\n",
    "\n",
    "The Web app is implemented with [streamlit](https://streamlit.io/): `pip install streamlit`.  If it is not already installed, the `onprem` command will ask you to install it.\n",
    "Here is more information on the `onprem` command:\n",
    "```sh\n",
    "$:~/projects/github/onprem$ onprem --help\n",
    "usage: onprem [-h] [-p PORT] [-a ADDRESS] [-v]\n",
    "\n",
    "Start the OnPrem.LLM web app\n",
    "Example: onprem --port 8000\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -p PORT, --port PORT  Port to use; default is 8501\n",
    "  -a ADDRESS, --address ADDRESS\n",
    "                        Address to bind; default is 0.0.0.0\n",
    "  -v, --version         Print a version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app requires a file called `webapp.yml` exists in the `onprem_data` folder in the user's home directory. This file stores information used by the Web app such as the model to use. If one does not exist, then a default one will be created for you and is also shown below:\n",
    "\n",
    "```yaml\n",
    "llm:\n",
    "  # model url (or model file name if previously downloaded)\n",
    "  model_url: https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q4_K_M.gguf\n",
    "  # number of layers offloaded to GPU\n",
    "  n_gpu_layers: 32\n",
    "  # path to vector db folder\n",
    "  vectordb_path: /home/your_user_name/onprem_data/vectordb\n",
    "  # path to model download folder\n",
    "  model_download_path: /home/your_user_name/onprem_data\n",
    "  # number of source documents used by LLM.ask and LLM.chat\n",
    "  rag_num_source_docs: 6\n",
    "  # minimum similarity score for a source to be used by LLM.ask and LLM.chat\n",
    "  rag_score_threshold: 0.0\n",
    "ui:\n",
    "  # title of application\n",
    "  title: OnPrem.LLM\n",
    "  # subtitle in \"Talk to Your Documents\" screen\n",
    "  rag_title:\n",
    "  # path to folder containing raw documents (used to construct direct links to document sources)\n",
    "  rag_source_path:\n",
    "  # base url (used to construct direct links to document sources)\n",
    "  rag_base_url:\n",
    "```\n",
    "\n",
    "You can edit the file based on your requirements. The default model in the auto-created YAML file is a [13B parameter model](https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q4_K_M.gguf).  If this is too large and slow for your system, you can edit `model_url` above to use a [7B parameter model](https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGUF/resolve/main/Wizard-Vicuna-7B-Uncensored.Q4_K_M.gguf) or [3B parameter model](https://huggingface.co/juanjgit/orca_mini_3B-GGUF/resolve/main/orca-mini-3b.q4_0.gguf) with faster speed at the expense of some performance. Of course, you can also edit `model_url` to use a larger model, as well. Any model in GGUF format can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk To Your Documents\n",
    "The Web app has two screens.  The first screen (shown above) is a UI for [retrieval augmented generation](https://arxiv.org/abs/2005.11401) or RAG (i.e., chatting with documents). Sources considered by the LLM when generating answers are displayed and ranked by answer-to-source similarity. Hovering over the question marks in the sources will display the snippets of text from a document considered by the LLM when generating answers.\n",
    "\n",
    "**Hover Example:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/hover-example.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source Hyperlinks:** On Linux and Mac systems where Python is installed in your home directory (e.g., `~/mambaforge`, `~/anaconda3`), displayed sources for the answer should automatically appear as hyperlinks to the original documents (e.g, PDFs, TXTs, etc.) if you populate the `rag_source_path` variable in `webapp.yml` with the the absolute path of the folder supplied to `LLM.ingest`.  You should leave `rag_base_url` blank in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Prompts to Solve Problems\n",
    "\n",
    "The second screen is a UI for general prompting and allows you to supply prompts to the LLM to solve problems.\n",
    "\n",
    "**Grammar Correction Example:**\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/grammar.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/cat_names.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
