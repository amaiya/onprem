{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest.stores.base\n",
    "\n",
    "> Base VectorStore class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest.stores.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "from typing import Optional, Callable\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from onprem.ingest.base import DEFAULT_CHUNK_SIZE, DEFAULT_CHUNK_OVERLAP, TABLE_CHUNK_SIZE, CHROMA_MAX, process_folder\n",
    "\n",
    "\n",
    "class VectorStore(ABC):\n",
    "\n",
    "    def ingest(\n",
    "        self,\n",
    "        source_directory: str, # path to folder containing document store\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE, # text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n",
    "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "        ignore_fn:Optional[Callable] = None, # Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested.\n",
    "        batch_size:int=CHROMA_MAX, # batch size used when processing documents\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_directory` (previously-ingested documents are\n",
    "        ignored). When retrieved, the\n",
    "        [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)\n",
    "        objects will each have a `metadata` dict with the absolute path to the file\n",
    "        in `metadata[\"source\"]`.\n",
    "        Extra kwargs fed to `ingest.load_single_document`.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(source_directory):\n",
    "            raise ValueError(\"The source_directory does not exist.\")\n",
    "        elif os.path.isfile(source_directory):\n",
    "            raise ValueError(\n",
    "                \"The source_directory argument must be a folder, not a file.\"\n",
    "            )\n",
    "        texts = None\n",
    "        if self.exists():\n",
    "            # Update and store locally vectorstore\n",
    "            print(f\"Appending to existing vectorstore at {self.persist_location}\")\n",
    "            # Get existing sources and normalize them for consistent comparison\n",
    "            ignored_files = set([os.path.normpath(d['source']).replace('\\\\', '/') \n",
    "                                for d in self.get_all_docs()])\n",
    "        else:\n",
    "            print(f\"Creating new vectorstore at {self.persist_location}\")\n",
    "            ignored_files = []\n",
    "\n",
    "        texts = process_folder(\n",
    "            source_directory,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            ignored_files=ignored_files,\n",
    "            ignore_fn=ignore_fn,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs\n",
    "\n",
    "        )\n",
    "\n",
    "        texts = list(texts)\n",
    "        print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} chars each for text; max. {TABLE_CHUNK_SIZE} chars for tables)\")\n",
    "\n",
    "        self.add_documents(texts, batch_size=batch_size)\n",
    "\n",
    "        if texts:\n",
    "            print(\n",
    "                \"Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\"\n",
    "            )\n",
    "        db = None\n",
    "        return\n",
    "    \n",
    "    def init_embedding_model(self, \n",
    "                             embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                             embedding_model_kwargs: Optional[dict] = None,\n",
    "                             embedding_encode_kwargs: dict = {\"normalize_embeddings\": False},\n",
    "                             **kwargs\n",
    "                             ):\n",
    "        \"\"\"\n",
    "        Instantiate embedding model\n",
    "        \"\"\"\n",
    "        if not embedding_model_kwargs:\n",
    "            import torch\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            embedding_model_kwargs = {\"device\": device}          \n",
    "        self.embeddings =  HuggingFaceEmbeddings(model_name=embedding_model_name, \n",
    "                                     model_kwargs=embedding_model_kwargs,\n",
    "                                     encode_kwargs=embedding_encode_kwargs)\n",
    "\n",
    "    def get_embedding_model(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain_huggingface.HuggingFaceEmbeddings` instance\n",
    "        \"\"\"\n",
    "        return self.embeddings\n",
    "\n",
    "\n",
    "    def compute_similarity(self, query:str, texts:list):\n",
    "        \"\"\"\n",
    "        Computes semantic similarity between a query and a list of texts\n",
    "        \"\"\"\n",
    "        from sentence_transformers import util\n",
    "        import torch\n",
    "\n",
    "        embeddings = self.get_embedding_model()\n",
    "\n",
    "        # Compute embeddings\n",
    "        query_emb = torch.tensor(embeddings.embed_query(query)).unsqueeze(0)  # Shape (1, embedding_dim)\n",
    "        text_embs = torch.tensor(embeddings.embed_documents(texts))  # Shape (len(texts), embedding_dim)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cos_scores = util.pytorch_cos_sim(query_emb, text_embs).squeeze(0).tolist()  # Shape (len(texts),)\n",
    "\n",
    "        return cos_scores\n",
    "\n",
    "\n",
    "    def check(self):\n",
    "        \"\"\"\n",
    "        Raise exception if `VectorStore.exists()` returns False\n",
    "        \"\"\"\n",
    "        if not self.exists():\n",
    "            raise Exception('The vector store is either empty or does not yet exist. '+\\\n",
    "                            'Please invoke the `ingest` method or `add_document` method.')\n",
    "\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def exists(self):\n",
    "        \"\"\"\n",
    "        Returns True if vector store has been initialized and contains documents.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Stores instances of `langchain_core.documents.base.Document` in vector store\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    @abstractmethod\n",
    "    def remove_document(self, id_to_delete):\n",
    "        \"\"\"\n",
    "        Remove a single document.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def remove_source(self, source):\n",
    "        \"\"\"\n",
    "        Remove documents by source\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update_documents(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Update a set of documents.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a list of files previously added to vector store.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_doc(self, id):\n",
    "        \"\"\"\n",
    "        Retrieve a document by ID\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_size(self):\n",
    "        \"\"\"\n",
    "        Get total number of records added to vector store\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def erase(self):\n",
    "        \"\"\"\n",
    "        Removes all documents in vector store\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def query(self, query, limit=4):\n",
    "        \"\"\"\n",
    "        Queries the vector store.\n",
    "        For sparse stores, this is simply a keyword-search.\n",
    "        For dense stores, this is equivalent to semantic_search except results\n",
    "        are in the form of dictionary with keys 'hits' and 'total_hits'.\n",
    "        Method must include 'query' as first positional argument and \"limit\" as keyword argument.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def semantic_search(self):\n",
    "        \"\"\"\n",
    "        Semantic search of vector store.\n",
    "        Expected to return a list of Langchain Document objects.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def search(self, query: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Generic search method that invokes the store's query method.\n",
    "        This provides a consistent interface across all store types.\n",
    "        \"\"\"\n",
    "        return self.query(query, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
