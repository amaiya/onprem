{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.rag\n",
    "\n",
    "> A pipeline module for Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Optional, Dict, List, Any\n",
    "from langchain_core.documents import Document\n",
    "from onprem.utils import format_string\n",
    "from onprem.llm import helpers\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_QA_PROMPT = \"\"\"Use the following pieces of context delimited by three backticks to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "```{context}```\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "DEFAULT_ROUTER_PROMPT = \"\"\"Given the following query/question, select the most appropriate category that would contain the relevant information.\n",
    "\n",
    "Query: {question}\n",
    "\n",
    "Available categories:\n",
    "{categories}\n",
    "\n",
    "Select the best category from the list above, or 'none' if no category is appropriate.\"\"\"\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation pipeline for answering questions based on source documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, qa_template: str = DEFAULT_QA_PROMPT):\n",
    "        \"\"\"\n",
    "        Initialize RAG pipeline.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model instance (LLM object)\n",
    "            qa_template: Question-answering prompt template\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.qa_template = qa_template\n",
    "    \n",
    "    def semantic_search(self,\n",
    "                       query: str, # search query as string\n",
    "                       limit: int = 4, # number of sources to retrieve\n",
    "                       score_threshold: float = 0.0, # minimum threshold for score\n",
    "                       filters: Optional[Dict[str, str]] = None, # metadata filters\n",
    "                       where_document = None, # filter search results based syntax of underlying store\n",
    "                       folders: Optional[list] = None, # list of folders to consider\n",
    "                       **kwargs) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform a semantic search of the vector DB.\n",
    "\n",
    "        The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
    "        If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
    "        to filter results.\n",
    "        If `LLM.store_type` is 'sparse', then `where_document` should be a boolean search string to filter query in Lucene syntax.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        store = self.llm.load_vectorstore()\n",
    "        if folders:\n",
    "            folders = [folders] if isinstance(folders, str) else folders\n",
    "            # This is needed because only the where argument supports the $like operator\n",
    "            # and Langchain does not properly forward the where parameter to Chroma\n",
    "            n_candidates = store.get_size() if store.get_size() < 10000 else 10000\n",
    "            results = store.semantic_search(query, \n",
    "                                            filters=filters,\n",
    "                                            where_document=where_document,\n",
    "                                            limit = n_candidates, **kwargs)\n",
    "            # Handle path separator differences between Windows and Unix\n",
    "            if os.name == 'nt':  # Windows\n",
    "                # Normalize paths for case-insensitive comparison on Windows\n",
    "                normalized_folders = [os.path.normpath(f).lower().replace('\\\\', '/') for f in folders]\n",
    "                results = [d for d in results if any(os.path.normpath(d.metadata['source']).lower().replace('\\\\', '/').startswith(nf) for nf in normalized_folders)]\n",
    "            else:\n",
    "                # On Unix systems, use direct path comparison\n",
    "                results = [d for d in results if any(d.metadata['source'].startswith(f) for f in folders)]\n",
    "            results = results[:limit]\n",
    "            \n",
    "        else:\n",
    "            results = store.semantic_search(query, \n",
    "                                            filters=filters,\n",
    "                                            where_document=where_document,\n",
    "                                            limit = limit, **kwargs)\n",
    "\n",
    "        return [d for d in results if d.metadata['score'] >= score_threshold]\n",
    "\n",
    "    def _retrieve_documents(self, \n",
    "                          question: str,\n",
    "                          filters: Optional[Dict[str, str]] = None,\n",
    "                          where_document = None,\n",
    "                          folders: Optional[list] = None,\n",
    "                          limit: int = 4,\n",
    "                          score_threshold: float = 0.0,\n",
    "                          table_k: int = 1,\n",
    "                          table_score_threshold: float = 0.35) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from vector database.\n",
    "        \"\"\"\n",
    "        docs = self.semantic_search(\n",
    "            question, \n",
    "            filters=filters, \n",
    "            where_document=where_document, \n",
    "            folders=folders,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        # Add table documents if requested\n",
    "        if table_k > 0:\n",
    "            table_filters = filters.copy() if filters else {}\n",
    "            table_filters = dict(table_filters, table=True)\n",
    "            table_docs = self.semantic_search(\n",
    "                f'{question} (table)', \n",
    "                filters=table_filters, \n",
    "                where_document=where_document,\n",
    "                folders=folders,\n",
    "                limit=table_k,\n",
    "                score_threshold=table_score_threshold\n",
    "            )\n",
    "            if table_docs:\n",
    "                docs.extend(table_docs[:limit])\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def _generate_answer(self, question: str, context: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using the language model.\n",
    "        \"\"\"\n",
    "        prompt = format_string(\n",
    "            self.qa_template,\n",
    "            question=question,\n",
    "            context=context\n",
    "        )\n",
    "        return self.llm.prompt(prompt, **kwargs)\n",
    "    \n",
    "    def ask(self,\n",
    "            question: str, # question as string\n",
    "            contexts: Optional[list] = None, # optional list of contexts to answer question. If None, retrieve from vectordb.\n",
    "            qa_template: Optional[str] = None, # question-answering prompt template to use\n",
    "            filters: Optional[Dict[str, str]] = None, # filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True})\n",
    "            where_document = None, # filter sources by document content (syntax varies by store type)\n",
    "            folders: Optional[list] = None, # folders to search (needed because LangChain does not forward \"where\" parameter)\n",
    "            limit: Optional[int] = None, # Number of sources to consider. If None, use `LLM.rag_num_source_docs`.\n",
    "            score_threshold: Optional[float] = None, # minimum similarity score of source. If None, use `LLM.rag_score_threshold`.\n",
    "            table_k: int = 1, # maximum number of tables to consider when generating answer\n",
    "            table_score_threshold: float = 0.35, # minimum similarity score for table to be considered in answer\n",
    "            selfask: bool = False, # If True, use an agentic Self-Ask prompting strategy.\n",
    "            router = None, # Optional KVRouter instance for automatic filtering\n",
    "            **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer a question using RAG approach.\n",
    "        \n",
    "        Args:\n",
    "            question: Question to answer\n",
    "            contexts: Optional list of contexts. If None, retrieve from vectordb\n",
    "            qa_template: Optional custom QA prompt template\n",
    "            filters: Filter sources by metadata values\n",
    "            where_document: Filter sources by document content\n",
    "            folders: Folders to search\n",
    "            limit: Number of sources to consider\n",
    "            score_threshold: Minimum similarity score\n",
    "            table_k: Maximum number of tables to consider\n",
    "            table_score_threshold: Minimum similarity score for tables\n",
    "            selfask: Use agentic Self-Ask prompting strategy\n",
    "            **kwargs: Additional arguments passed to LLM.prompt\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with keys: answer, source_documents, question\n",
    "        \"\"\"\n",
    "        template = qa_template or self.qa_template\n",
    "        limit = limit if limit is not None else self.llm.rag_num_source_docs\n",
    "        score_threshold = score_threshold if score_threshold is not None else self.llm.rag_score_threshold\n",
    "        \n",
    "        # Apply router if provided\n",
    "        if router and not filters:\n",
    "            router_filters = router.route(question)\n",
    "            if router_filters:\n",
    "                filters = router_filters\n",
    "        elif router and filters:\n",
    "            # Merge router filters with existing filters\n",
    "            router_filters = router.route(question)\n",
    "            if router_filters:\n",
    "                combined_filters = filters.copy()\n",
    "                combined_filters.update(router_filters)\n",
    "                filters = combined_filters\n",
    "        \n",
    "        if selfask and helpers.needs_followup(question, self.llm):\n",
    "            return self._ask_with_decomposition(\n",
    "                question, template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            return self._ask_direct(\n",
    "                question, contexts, template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, **kwargs\n",
    "            )\n",
    "    \n",
    "    def _ask_direct(self,\n",
    "                   question: str,\n",
    "                   contexts: Optional[list],\n",
    "                   qa_template: str,\n",
    "                   filters: Optional[Dict[str, str]],\n",
    "                   where_document,\n",
    "                   folders: Optional[list],\n",
    "                   limit: int,\n",
    "                   score_threshold: float,\n",
    "                   table_k: int,\n",
    "                   table_score_threshold: float,\n",
    "                   **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Direct RAG without decomposition.\"\"\"\n",
    "        if contexts is None:\n",
    "            docs = self._retrieve_documents(\n",
    "                question, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold\n",
    "            )\n",
    "            context = '\\n\\n'.join([d.page_content for d in docs])\n",
    "        else:\n",
    "            docs = [Document(page_content=c, metadata={'source': '<SUBANSWER>'}) for c in contexts]\n",
    "            context = \"\\n\\n\".join(contexts)\n",
    "        \n",
    "        answer = self._generate_answer(question, context, **kwargs)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'source_documents': docs\n",
    "        }\n",
    "    \n",
    "    def _ask_with_decomposition(self,\n",
    "                               question: str,\n",
    "                               qa_template: str,\n",
    "                               filters: Optional[Dict[str, str]],\n",
    "                               where_document,\n",
    "                               folders: Optional[list],\n",
    "                               limit: int,\n",
    "                               score_threshold: float,\n",
    "                               table_k: int,\n",
    "                               table_score_threshold: float,\n",
    "                               **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"RAG with question decomposition (Self-Ask).\"\"\"\n",
    "        subquestions = helpers.decompose_question(question, self.llm)\n",
    "        subanswers = []\n",
    "        sources = []\n",
    "        \n",
    "        for q in subquestions:\n",
    "            res = self._ask_direct(\n",
    "                q, None, qa_template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, **kwargs\n",
    "            )\n",
    "            subanswers.append(res['answer'])\n",
    "            for doc in res['source_documents']:\n",
    "                doc.metadata = dict(doc.metadata, subquestion=q)\n",
    "            sources.extend(res['source_documents'])\n",
    "        \n",
    "        # Generate final answer based on subanswers\n",
    "        res = self._ask_direct(\n",
    "            question, subanswers, qa_template, filters, where_document, folders,\n",
    "            limit, score_threshold, table_k, table_score_threshold, **kwargs\n",
    "        )\n",
    "        res['source_documents'] = sources\n",
    "        \n",
    "        return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L143){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.ask\n",
       "\n",
       ">      RAGPipeline.ask (question:str, contexts:Optional[list]=None,\n",
       ">                       qa_template:Optional[str]=None,\n",
       ">                       filters:Optional[Dict[str,str]]=None,\n",
       ">                       where_document=None, folders:Optional[list]=None,\n",
       ">                       limit:Optional[int]=None,\n",
       ">                       score_threshold:Optional[float]=None, table_k:int=1,\n",
       ">                       table_score_threshold:float=0.35, selfask:bool=False,\n",
       ">                       router=None, **kwargs)\n",
       "\n",
       "*Answer a question using RAG approach.\n",
       "\n",
       "Args:\n",
       "    question: Question to answer\n",
       "    contexts: Optional list of contexts. If None, retrieve from vectordb\n",
       "    qa_template: Optional custom QA prompt template\n",
       "    filters: Filter sources by metadata values\n",
       "    where_document: Filter sources by document content\n",
       "    folders: Folders to search\n",
       "    limit: Number of sources to consider\n",
       "    score_threshold: Minimum similarity score\n",
       "    table_k: Maximum number of tables to consider\n",
       "    table_score_threshold: Minimum similarity score for tables\n",
       "    selfask: Use agentic Self-Ask prompting strategy\n",
       "    **kwargs: Additional arguments passed to LLM.prompt\n",
       "\n",
       "Returns:\n",
       "    Dictionary with keys: answer, source_documents, question*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| question | str |  | question as string |\n",
       "| contexts | Optional | None | optional list of contexts to answer question. If None, retrieve from vectordb. |\n",
       "| qa_template | Optional | None | question-answering prompt template to use |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | NoneType | None | filter sources by document content (syntax varies by store type) |\n",
       "| folders | Optional | None | folders to search (needed because LangChain does not forward \"where\" parameter) |\n",
       "| limit | Optional | None | Number of sources to consider. If None, use `LLM.rag_num_source_docs`. |\n",
       "| score_threshold | Optional | None | minimum similarity score of source. If None, use `LLM.rag_score_threshold`. |\n",
       "| table_k | int | 1 | maximum number of tables to consider when generating answer |\n",
       "| table_score_threshold | float | 0.35 | minimum similarity score for table to be considered in answer |\n",
       "| selfask | bool | False | If True, use an agentic Self-Ask prompting strategy. |\n",
       "| router | NoneType | None | Optional KVRouter instance for automatic filtering |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **Dict** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L143){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.ask\n",
       "\n",
       ">      RAGPipeline.ask (question:str, contexts:Optional[list]=None,\n",
       ">                       qa_template:Optional[str]=None,\n",
       ">                       filters:Optional[Dict[str,str]]=None,\n",
       ">                       where_document=None, folders:Optional[list]=None,\n",
       ">                       limit:Optional[int]=None,\n",
       ">                       score_threshold:Optional[float]=None, table_k:int=1,\n",
       ">                       table_score_threshold:float=0.35, selfask:bool=False,\n",
       ">                       router=None, **kwargs)\n",
       "\n",
       "*Answer a question using RAG approach.\n",
       "\n",
       "Args:\n",
       "    question: Question to answer\n",
       "    contexts: Optional list of contexts. If None, retrieve from vectordb\n",
       "    qa_template: Optional custom QA prompt template\n",
       "    filters: Filter sources by metadata values\n",
       "    where_document: Filter sources by document content\n",
       "    folders: Folders to search\n",
       "    limit: Number of sources to consider\n",
       "    score_threshold: Minimum similarity score\n",
       "    table_k: Maximum number of tables to consider\n",
       "    table_score_threshold: Minimum similarity score for tables\n",
       "    selfask: Use agentic Self-Ask prompting strategy\n",
       "    **kwargs: Additional arguments passed to LLM.prompt\n",
       "\n",
       "Returns:\n",
       "    Dictionary with keys: answer, source_documents, question*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| question | str |  | question as string |\n",
       "| contexts | Optional | None | optional list of contexts to answer question. If None, retrieve from vectordb. |\n",
       "| qa_template | Optional | None | question-answering prompt template to use |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | NoneType | None | filter sources by document content (syntax varies by store type) |\n",
       "| folders | Optional | None | folders to search (needed because LangChain does not forward \"where\" parameter) |\n",
       "| limit | Optional | None | Number of sources to consider. If None, use `LLM.rag_num_source_docs`. |\n",
       "| score_threshold | Optional | None | minimum similarity score of source. If None, use `LLM.rag_score_threshold`. |\n",
       "| table_k | int | 1 | maximum number of tables to consider when generating answer |\n",
       "| table_score_threshold | float | 0.35 | minimum similarity score for table to be considered in answer |\n",
       "| selfask | bool | False | If True, use an agentic Self-Ask prompting strategy. |\n",
       "| router | NoneType | None | Optional KVRouter instance for automatic filtering |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **Dict** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RAGPipeline.ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L49){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.semantic_search\n",
       "\n",
       ">      RAGPipeline.semantic_search (query:str, limit:int=4,\n",
       ">                                   score_threshold:float=0.0,\n",
       ">                                   filters:Optional[Dict[str,str]]=None,\n",
       ">                                   where_document=None,\n",
       ">                                   folders:Optional[list]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB.\n",
       "\n",
       "The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
       "If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
       "to filter results.\n",
       "If `LLM.store_type` is 'sparse', then `where_document` should be a boolean search string to filter query in Lucene syntax.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | search query as string |\n",
       "| limit | int | 4 | number of sources to retrieve |\n",
       "| score_threshold | float | 0.0 | minimum threshold for score |\n",
       "| filters | Optional | None | metadata filters |\n",
       "| where_document | NoneType | None | filter search results based syntax of underlying store |\n",
       "| folders | Optional | None | list of folders to consider |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **List** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L49){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.semantic_search\n",
       "\n",
       ">      RAGPipeline.semantic_search (query:str, limit:int=4,\n",
       ">                                   score_threshold:float=0.0,\n",
       ">                                   filters:Optional[Dict[str,str]]=None,\n",
       ">                                   where_document=None,\n",
       ">                                   folders:Optional[list]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB.\n",
       "\n",
       "The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
       "If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
       "to filter results.\n",
       "If `LLM.store_type` is 'sparse', then `where_document` should be a boolean search string to filter query in Lucene syntax.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | search query as string |\n",
       "| limit | int | 4 | number of sources to retrieve |\n",
       "| score_threshold | float | 0.0 | minimum threshold for score |\n",
       "| filters | Optional | None | metadata filters |\n",
       "| where_document | NoneType | None | filter search results based syntax of underlying store |\n",
       "| folders | Optional | None | list of folders to consider |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **List** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RAGPipeline.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class CategorySelection(BaseModel):\n",
    "    \"\"\"Pydantic model for category selection response.\"\"\"\n",
    "    category: str = Field(description=\"Selected category value or 'none' if no appropriate category\")\n",
    "\n",
    "\n",
    "class KVRouter:\n",
    "    \"\"\"\n",
    "    Key-Value Router for intelligent filtering based on query content.\n",
    "    \n",
    "    Uses an LLM to select the most appropriate field value for filtering\n",
    "    based on the query/question content.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 field_name: str,\n",
    "                 field_descriptions: Dict[str, str],\n",
    "                 llm,\n",
    "                 router_prompt: str = DEFAULT_ROUTER_PROMPT):\n",
    "        \"\"\"\n",
    "        Initialize KV Router.\n",
    "        \n",
    "        Args:\n",
    "            field_name: The metadata field name to filter on (e.g., 'folder')\n",
    "            field_descriptions: Dict mapping field values to descriptions\n",
    "                               (e.g., {'sotu': \"Biden's State of the Union Address\"})\n",
    "            llm: The LLM instance to use for routing decisions\n",
    "            router_prompt: Template for the routing prompt\n",
    "        \"\"\"\n",
    "        self.field_name = field_name\n",
    "        self.field_descriptions = field_descriptions\n",
    "        self.llm = llm\n",
    "        self.router_prompt = router_prompt\n",
    "    \n",
    "    def _format_categories(self) -> str:\n",
    "        \"\"\"Format field descriptions for the prompt.\"\"\"\n",
    "        categories = []\n",
    "        for value, description in self.field_descriptions.items():\n",
    "            categories.append(f\"- {value}: {description}\")\n",
    "        return \"\\n\".join(categories)\n",
    "    \n",
    "    def route(self, question: str) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Select the best field value for the given question.\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question/query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary for filters parameter, or None if no appropriate category\n",
    "            Example: {'folder': 'sotu'} or None\n",
    "        \"\"\"\n",
    "        # Format the prompt\n",
    "        categories_text = self._format_categories()\n",
    "        prompt = format_string(\n",
    "            self.router_prompt,\n",
    "            question=question,\n",
    "            categories=categories_text\n",
    "        )\n",
    "        \n",
    "        # Use pydantic_prompt for structured output\n",
    "        try:\n",
    "            response = self.llm.pydantic_prompt(\n",
    "                prompt, \n",
    "                pydantic_model=CategorySelection,\n",
    "                attempt_fix=True  # Try to fix malformed responses\n",
    "            )\n",
    "            \n",
    "            selected_category = response.category.strip().lower()\n",
    "            \n",
    "            # Check if it's a valid category (case-insensitive)\n",
    "            valid_values = {v.lower(): v for v in self.field_descriptions.keys()}\n",
    "            \n",
    "            if selected_category in valid_values:\n",
    "                return {self.field_name: valid_values[selected_category]}\n",
    "            elif selected_category == 'none':\n",
    "                return None\n",
    "            else:\n",
    "                # Fallback: if response doesn't match, return None\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Fallback to None if pydantic parsing fails\n",
    "            return None\n",
    "    \n",
    "    def route_and_search(self, \n",
    "                        query: str,\n",
    "                        rag_pipeline,\n",
    "                        **search_kwargs) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Convenience method that routes and performs semantic search.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            rag_pipeline: RAGPipeline instance to search with\n",
    "            **search_kwargs: Additional arguments passed to semantic_search\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects\n",
    "        \"\"\"\n",
    "        # Get routing decision\n",
    "        route_filters = self.route(query)\n",
    "        \n",
    "        # Merge with existing filters if any\n",
    "        existing_filters = search_kwargs.get('filters', {})\n",
    "        if route_filters:\n",
    "            if existing_filters:\n",
    "                # Combine filters\n",
    "                combined_filters = existing_filters.copy()\n",
    "                combined_filters.update(route_filters)\n",
    "                search_kwargs['filters'] = combined_filters\n",
    "            else:\n",
    "                search_kwargs['filters'] = route_filters\n",
    "        \n",
    "        # Perform search\n",
    "        return rag_pipeline.semantic_search(query, **search_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Query Routing with RAG\n",
    "\n",
    "In this example, we use the `KVRouter` to route RAG queries to the correct set of ingested documents.\n",
    "\n",
    "First, when we ingest documents, we assign a folder field to each document chunk. (You can also use the `text_callables` parameter to assign a field value based on text content.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |  notest\n",
    "\n",
    "from onprem import LLM\n",
    "from onprem.pipelines import KVRouter\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /tmp/tmpdx_zjj3j/dense\n",
      "Loading documents from tests/sample_data/sotu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00, 87.50it/s]\n",
      "Processing and chunking 1 new documents: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 446.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 43 chunks of text (max. 1000 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n",
      "Appending to existing vectorstore at /tmp/tmpdx_zjj3j/dense\n",
      "Loading documents from tests/sample_data/ktrain_paper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  7.51it/s]\n",
      "Processing and chunking 6 new documents: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1258.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 22 chunks of text (max. 1000 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "\n",
    "# Setup LLM and ingest with custom metadata\n",
    "llm = LLM('openai/gpt-4o-mini', vectordb_path=tempfile.mkdtemp())\n",
    "def set_folder(filepath):\n",
    "    if 'sotu' in filepath:\n",
    "        return 'sotu'\n",
    "    elif 'ktrain_paper' in filepath:\n",
    "        return 'ktrain'\n",
    "    else:\n",
    "        return 'na'\n",
    "        \n",
    "llm.ingest('tests/sample_data/sotu', file_callables={'folder': set_folder})\n",
    "llm.ingest('tests/sample_data/ktrain_paper', file_callables={'folder': set_folder})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we setup a `KVRouter` that returns the best key-value paper (in this case, a specific `folder` value) based on the question or query.  The key-value pair is then used to filter the documents appropriately when retrieving source documents for answer generation.  The router can be supplied direclty to the `ask` method so that only docuents in the appropriate folder are considered when generating answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"category\":\"ktrain\"}\n",
      "```\n",
      "{'folder': 'ktrain'}\n"
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "\n",
    "# Create router\n",
    "router = KVRouter(\n",
    "  field_name='folder',\n",
    "  field_descriptions={\n",
    "      'sotu': \"Biden's State of the Union Address\",\n",
    "      'ktrain': \"Research papers about ktrain library, a toolkit for machine learning, text classification, and computer vision.\"\n",
    "  },\n",
    "  llm=llm\n",
    ")\n",
    "\n",
    "# Example of router\n",
    "filter_dict = router.route('Tell me about image classification')\n",
    "print()\n",
    "print(filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"category\":\"sotu\"}\n",
      "```Biden stated that the previous trickle-down economic theory resulted in weaker economic growth, lower wages, larger deficits, and an increasing wealth gap. He emphasized a new economic vision focused on investing in America, educating Americans, and growing the workforce from the bottom up and the middle out, rather than from the top down. He highlighted the importance of infrastructure investment, noted the need for competition to lower costs, and mentioned the creation of manufacturing jobs and significant corporate investments in electric vehicles. He acknowledged that, despite record job growth and higher wages, many families are still struggling with inflation, and his top priority is to bring prices under control."
     ]
    }
   ],
   "source": [
    "# |notest\n",
    "\n",
    "# Use router with ask() - Method 1: Direct parameter\n",
    "result = llm.ask(\n",
    "  \"What did Biden say about the economy?\",\n",
    "  router=router\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"category\": \"ktrain\"}\n",
      "```To use ktrain for text classification, you can follow these general steps based on the provided context:\n",
      "\n",
      "1. **Load and Preprocess Data**: Start by loading your text data. You may need to preprocess it to fit the model's requirements, which could involve tokenization and other language-specific preprocessing steps. Use the appropriate Preprocessor instance that ktrain provides for text data.\n",
      "\n",
      "2. **Create Model**: You can create and customize your model using `tf.keras`. ktrain supports various model architectures suitable for text classification.\n",
      "\n",
      "3. **Train the Model**: After creating the model, you will need to train it using your preprocessed data. This step involves setting hyperparameters such as learning rates and the number of epochs.\n",
      "\n",
      "4. **Inspect the Model**: Once trained, inspect the model’s performance. You can generate classification reports to evaluate both its successes and failures.\n",
      "\n",
      "5. **Apply the Model**: Finally, save the model and any preprocessing steps required to apply it to new, unseen text data.\n",
      "\n",
      "For detailed code examples and specific functions, you can refer to the ktrain documentation or its GitHub repository."
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "\n",
    "# Use router with RAG pipeline - Method 2: Direct on pipeline\n",
    "rag_pipeline = llm.load_rag_pipeline()\n",
    "result = rag_pipeline.ask(\n",
    "  \"How do I use ktrain for text classification?\",\n",
    "  router=router\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
