{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6eddee9",
   "metadata": {},
   "source": [
    "# pipelines.rag\n",
    "\n",
    "> A pipeline module for Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ecd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Optional, Dict, List, Any\n",
    "from langchain_core.documents import Document\n",
    "from onprem.utils import format_string, SafeFormatter\n",
    "from onprem.llm import helpers\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_QA_PROMPT = \"\"\"Use the following pieces of context delimited by three backticks to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "```{context}```\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "DEFAULT_ROUTER_PROMPT = \"\"\"Given the following query/question, select the most appropriate category that would contain the relevant information.\n",
    "\n",
    "Query: {question}\n",
    "\n",
    "Available categories:\n",
    "{categories}\n",
    "\n",
    "Select the best category from the list above, or 'none' if no category is appropriate.\n",
    "Do not provide an explanation for the categorization. Only output the category as a single string\"\"\"\n",
    "\n",
    "# Question decomposition prompt template\n",
    "SUBQUESTION_PROMPT = \"\"\"\\\n",
    "Given a user question, output a list of relevant sub-questions \\\n",
    "in json markdown that when composed can help answer the full user question.\n",
    "Only return the JSON response, with no additional text or explanations.\n",
    "Generate between 2-5 sub-questions.  Do not exceed 5 sub-questions.\n",
    "\n",
    "# Example 1\n",
    "<User Question>\n",
    "Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\n",
    "\n",
    "<Output>\n",
    "```json\n",
    "{\n",
    "    \"items\": [\n",
    "        {\n",
    "            \"sub_question\": \"What is the revenue growth of Uber\",\n",
    "        },\n",
    "        {\n",
    "            \"sub_question\": \"What is the EBITDA of Uber\",\n",
    "        },\n",
    "        {\n",
    "            \"sub_question\": \"What is the revenue growth of Lyft\",\n",
    "        },\n",
    "        {\n",
    "            \"sub_question\": \"What is the EBITDA of Lyft\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Example 2\n",
    "<User Question>\n",
    "{query_str}\n",
    "\n",
    "<Output>\n",
    "\"\"\"\n",
    "\n",
    "# Followup question determination prompt template\n",
    "FOLLOWUP_PROMPT = \"\"\"\\\n",
    "Given a question, answer \"yes\" only if the question is complex and follow-up questions are needed or \"no\" if not.\n",
    "Always respond with \"no\" for short questions that are less than 8 words.\n",
    "Answer only with either \"yes\" or \"no\" with no additional text or explanations.\n",
    "\n",
    "# Example 1\n",
    "<User Question>\n",
    "Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\n",
    "\n",
    "<Output>\n",
    "yes\n",
    "\n",
    "# Example 2\n",
    "<User Question>\n",
    "How is the Coast Guard using artificial intelligence?\n",
    "\n",
    "<Output>\n",
    "No\n",
    "\n",
    "# Example 3\n",
    "<User Question\n",
    "What is AutoGluon?\n",
    "\n",
    "<Output>\n",
    "No\n",
    "\n",
    "# Example 4\n",
    "<User Question>\n",
    "{query_str}\n",
    "\n",
    "<Output>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fd800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation pipeline for answering questions based on source documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, qa_template: str = DEFAULT_QA_PROMPT):\n",
    "        \"\"\"\n",
    "        Initialize RAG pipeline.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model instance (LLM object)\n",
    "            qa_template: Question-answering prompt template\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.qa_template = qa_template\n",
    "    \n",
    "    def semantic_search(self,\n",
    "                       query: str, # search query as string\n",
    "                       limit: int = 4, # number of sources to retrieve\n",
    "                       score_threshold: float = 0.0, # minimum threshold for score\n",
    "                       filters: Optional[Dict[str, str]] = None, # metadata filters\n",
    "                       where_document = None, # filter search results based syntax of underlying store\n",
    "                       folders: Optional[list] = None, # list of folders to consider\n",
    "                       **kwargs) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform a semantic search of the vector DB.\n",
    "\n",
    "        The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
    "        If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
    "        to filter results.\n",
    "        If `LLM.store_type` is 'sparse', then `where_document` should be a boolean search string to filter query in Lucene syntax.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        store = self.llm.load_vectorstore()\n",
    "        if folders:\n",
    "            folders = [folders] if isinstance(folders, str) else folders\n",
    "            # This is needed because only the where argument supports the $like operator\n",
    "            # and Langchain does not properly forward the where parameter to Chroma\n",
    "            n_candidates = store.get_size() if store.get_size() < 10000 else 10000\n",
    "            results = store.semantic_search(query, \n",
    "                                            filters=filters,\n",
    "                                            where_document=where_document,\n",
    "                                            limit = n_candidates, **kwargs)\n",
    "            # Handle path separator differences between Windows and Unix\n",
    "            if os.name == 'nt':  # Windows\n",
    "                # Normalize paths for case-insensitive comparison on Windows\n",
    "                normalized_folders = [os.path.normpath(f).lower().replace('\\\\', '/') for f in folders]\n",
    "                results = [d for d in results if any(os.path.normpath(d.metadata['source']).lower().replace('\\\\', '/').startswith(nf) for nf in normalized_folders)]\n",
    "            else:\n",
    "                # On Unix systems, use direct path comparison\n",
    "                results = [d for d in results if any(d.metadata['source'].startswith(f) for f in folders)]\n",
    "            results = results[:limit]\n",
    "            \n",
    "        else:\n",
    "            results = store.semantic_search(query, \n",
    "                                            filters=filters,\n",
    "                                            where_document=where_document,\n",
    "                                            limit = limit, **kwargs)\n",
    "\n",
    "        return [d for d in results if d.metadata['score'] >= score_threshold]\n",
    "\n",
    "    def _retrieve_documents(self, \n",
    "                          question: str,\n",
    "                          filters: Optional[Dict[str, str]] = None,\n",
    "                          where_document = None,\n",
    "                          folders: Optional[list] = None,\n",
    "                          limit: int = 4,\n",
    "                          score_threshold: float = 0.0,\n",
    "                          table_k: int = 1,\n",
    "                          table_score_threshold: float = 0.35) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from vector database.\n",
    "        \"\"\"\n",
    "        docs = self.semantic_search(\n",
    "            question, \n",
    "            filters=filters, \n",
    "            where_document=where_document, \n",
    "            folders=folders,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        # Add table documents if requested\n",
    "        if table_k > 0:\n",
    "            table_filters = filters.copy() if filters else {}\n",
    "            table_filters = dict(table_filters, table=True)\n",
    "            table_docs = self.semantic_search(\n",
    "                f'{question} (table)', \n",
    "                filters=table_filters, \n",
    "                where_document=where_document,\n",
    "                folders=folders,\n",
    "                limit=table_k,\n",
    "                score_threshold=table_score_threshold\n",
    "            )\n",
    "            if table_docs:\n",
    "                docs.extend(table_docs[:limit])\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def _generate_answer(self, question: str, context: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using the language model.\n",
    "        \"\"\"\n",
    "        prompt = format_string(\n",
    "            self.qa_template,\n",
    "            question=question,\n",
    "            context=context\n",
    "        )\n",
    "        return self.llm.prompt(prompt, **kwargs)\n",
    "    \n",
    "    def decompose_question(self, question: str, parse=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Decompose a question into subquestions\n",
    "        \"\"\"\n",
    "        prompt = SafeFormatter({'query_str': question}).format(SUBQUESTION_PROMPT)\n",
    "        json_string = self.llm.prompt(prompt)\n",
    "        json_dict = helpers.parse_json_markdown(json_string)\n",
    "        subquestions = [d['sub_question'] for d in json_dict['items']]\n",
    "        return subquestions\n",
    "\n",
    "\n",
    "    def needs_followup(self, question: str, parse=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Decide if follow-up questions are needed\n",
    "        \"\"\"\n",
    "        prompt = SafeFormatter({'query_str': question}).format(FOLLOWUP_PROMPT)\n",
    "        output = self.llm.prompt(prompt)\n",
    "        return \"yes\" in output.lower()\n",
    "    \n",
    "\n",
    "    def ask(self,\n",
    "            question: str, # question as string\n",
    "            contexts: Optional[list] = None, # optional list of contexts to answer question. If None, retrieve from vectordb.\n",
    "            qa_template: Optional[str] = None, # question-answering prompt template to use\n",
    "            filters: Optional[Dict[str, str]] = None, # filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True})\n",
    "            where_document = None, # filter sources by document content (syntax varies by store type)\n",
    "            folders: Optional[list] = None, # folders to search (needed because LangChain does not forward \"where\" parameter)\n",
    "            limit: Optional[int] = None, # Number of sources to consider. If None, use `LLM.rag_num_source_docs`.\n",
    "            score_threshold: Optional[float] = None, # minimum similarity score of source. If None, use `LLM.rag_score_threshold`.\n",
    "            table_k: int = 1, # maximum number of tables to consider when generating answer\n",
    "            table_score_threshold: float = 0.35, # minimum similarity score for table to be considered in answer\n",
    "            selfask: bool = False, # If True, use an agentic Self-Ask prompting strategy.\n",
    "            router = None, # Optional KVRouter instance for automatic filtering\n",
    "            **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer a question using RAG approach.\n",
    "        Additional kwargs arguments passed to LLM.prompt\n",
    "        Returns dictionary with keys: answer, source_documents, question.\n",
    "        \"\"\"\n",
    "        template = qa_template or self.qa_template\n",
    "        limit = limit if limit is not None else self.llm.rag_num_source_docs\n",
    "        score_threshold = score_threshold if score_threshold is not None else self.llm.rag_score_threshold\n",
    "        \n",
    "        if selfask and self.needs_followup(question):\n",
    "            return self._ask_with_decomposition(\n",
    "                question, template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, router, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            return self._ask_direct(\n",
    "                question, contexts, template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, router, **kwargs\n",
    "            )\n",
    "    \n",
    "    def _ask_direct(self,\n",
    "                   question: str,\n",
    "                   contexts: Optional[list],\n",
    "                   qa_template: str,\n",
    "                   filters: Optional[Dict[str, str]],\n",
    "                   where_document,\n",
    "                   folders: Optional[list],\n",
    "                   limit: int,\n",
    "                   score_threshold: float,\n",
    "                   table_k: int,\n",
    "                   table_score_threshold: float,\n",
    "                   router = None,\n",
    "                   **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Direct RAG without decomposition.\"\"\"\n",
    "        # Apply router if provided\n",
    "        if router and not filters:\n",
    "            router_filters = router.route(question)\n",
    "            if router_filters:\n",
    "                filters = router_filters\n",
    "        elif router and filters:\n",
    "            # Merge router filters with existing filters\n",
    "            router_filters = router.route(question)\n",
    "            if router_filters:\n",
    "                combined_filters = filters.copy()\n",
    "                combined_filters.update(router_filters)\n",
    "                filters = combined_filters\n",
    "        \n",
    "        if contexts is None:\n",
    "            docs = self._retrieve_documents(\n",
    "                question, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold\n",
    "            )\n",
    "            context = '\\n\\n'.join([d.page_content for d in docs])\n",
    "        else:\n",
    "            docs = [Document(page_content=c, metadata={'source': '<SUBANSWER>'}) for c in contexts]\n",
    "            context = \"\\n\\n\".join(contexts)\n",
    "        \n",
    "        answer = self._generate_answer(question, context, **kwargs)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'source_documents': docs\n",
    "        }\n",
    "    \n",
    "    def _ask_with_decomposition(self,\n",
    "                               question: str,\n",
    "                               qa_template: str,\n",
    "                               filters: Optional[Dict[str, str]],\n",
    "                               where_document,\n",
    "                               folders: Optional[list],\n",
    "                               limit: int,\n",
    "                               score_threshold: float,\n",
    "                               table_k: int,\n",
    "                               table_score_threshold: float,\n",
    "                               router = None,\n",
    "                               **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"RAG with question decomposition (Self-Ask).\"\"\"\n",
    "        subquestions = self.decompose_question(question)\n",
    "        subanswers = []\n",
    "        sources = []\n",
    "        \n",
    "        for q in subquestions:\n",
    "            res = self._ask_direct(\n",
    "                q, None, qa_template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, router, **kwargs\n",
    "            )\n",
    "            subanswers.append(res['answer'])\n",
    "            for doc in res['source_documents']:\n",
    "                doc.metadata = dict(doc.metadata, subquestion=q)\n",
    "            sources.extend(res['source_documents'])\n",
    "        \n",
    "        # Generate final answer based on subanswers\n",
    "        res = self._ask_direct(\n",
    "            question, subanswers, qa_template, filters, where_document, folders,\n",
    "            limit, score_threshold, table_k, table_score_threshold, None, **kwargs\n",
    "        )\n",
    "        res['source_documents'] = sources\n",
    "        \n",
    "        return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202116f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L233){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.ask\n",
       "\n",
       ">      RAGPipeline.ask (question:str, contexts:Optional[list]=None,\n",
       ">                       qa_template:Optional[str]=None,\n",
       ">                       filters:Optional[Dict[str,str]]=None,\n",
       ">                       where_document=None, folders:Optional[list]=None,\n",
       ">                       limit:Optional[int]=None,\n",
       ">                       score_threshold:Optional[float]=None, table_k:int=1,\n",
       ">                       table_score_threshold:float=0.35, selfask:bool=False,\n",
       ">                       router=None, **kwargs)\n",
       "\n",
       "*Answer a question using RAG approach.\n",
       "\n",
       "Args:\n",
       "    question: Question to answer\n",
       "    contexts: Optional list of contexts. If None, retrieve from vectordb\n",
       "    qa_template: Optional custom QA prompt template\n",
       "    filters: Filter sources by metadata values\n",
       "    where_document: Filter sources by document content\n",
       "    folders: Folders to search\n",
       "    limit: Number of sources to consider\n",
       "    score_threshold: Minimum similarity score\n",
       "    table_k: Maximum number of tables to consider\n",
       "    table_score_threshold: Minimum similarity score for tables\n",
       "    selfask: Use agentic Self-Ask prompting strategy\n",
       "    **kwargs: Additional arguments passed to LLM.prompt\n",
       "\n",
       "Returns:\n",
       "    Dictionary with keys: answer, source_documents, question*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| question | str |  | question as string |\n",
       "| contexts | Optional | None | optional list of contexts to answer question. If None, retrieve from vectordb. |\n",
       "| qa_template | Optional | None | question-answering prompt template to use |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | NoneType | None | filter sources by document content (syntax varies by store type) |\n",
       "| folders | Optional | None | folders to search (needed because LangChain does not forward \"where\" parameter) |\n",
       "| limit | Optional | None | Number of sources to consider. If None, use `LLM.rag_num_source_docs`. |\n",
       "| score_threshold | Optional | None | minimum similarity score of source. If None, use `LLM.rag_score_threshold`. |\n",
       "| table_k | int | 1 | maximum number of tables to consider when generating answer |\n",
       "| table_score_threshold | float | 0.35 | minimum similarity score for table to be considered in answer |\n",
       "| selfask | bool | False | If True, use an agentic Self-Ask prompting strategy. |\n",
       "| router | NoneType | None | Optional KVRouter instance for automatic filtering |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **Dict** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L233){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.ask\n",
       "\n",
       ">      RAGPipeline.ask (question:str, contexts:Optional[list]=None,\n",
       ">                       qa_template:Optional[str]=None,\n",
       ">                       filters:Optional[Dict[str,str]]=None,\n",
       ">                       where_document=None, folders:Optional[list]=None,\n",
       ">                       limit:Optional[int]=None,\n",
       ">                       score_threshold:Optional[float]=None, table_k:int=1,\n",
       ">                       table_score_threshold:float=0.35, selfask:bool=False,\n",
       ">                       router=None, **kwargs)\n",
       "\n",
       "*Answer a question using RAG approach.\n",
       "\n",
       "Args:\n",
       "    question: Question to answer\n",
       "    contexts: Optional list of contexts. If None, retrieve from vectordb\n",
       "    qa_template: Optional custom QA prompt template\n",
       "    filters: Filter sources by metadata values\n",
       "    where_document: Filter sources by document content\n",
       "    folders: Folders to search\n",
       "    limit: Number of sources to consider\n",
       "    score_threshold: Minimum similarity score\n",
       "    table_k: Maximum number of tables to consider\n",
       "    table_score_threshold: Minimum similarity score for tables\n",
       "    selfask: Use agentic Self-Ask prompting strategy\n",
       "    **kwargs: Additional arguments passed to LLM.prompt\n",
       "\n",
       "Returns:\n",
       "    Dictionary with keys: answer, source_documents, question*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| question | str |  | question as string |\n",
       "| contexts | Optional | None | optional list of contexts to answer question. If None, retrieve from vectordb. |\n",
       "| qa_template | Optional | None | question-answering prompt template to use |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | NoneType | None | filter sources by document content (syntax varies by store type) |\n",
       "| folders | Optional | None | folders to search (needed because LangChain does not forward \"where\" parameter) |\n",
       "| limit | Optional | None | Number of sources to consider. If None, use `LLM.rag_num_source_docs`. |\n",
       "| score_threshold | Optional | None | minimum similarity score of source. If None, use `LLM.rag_score_threshold`. |\n",
       "| table_k | int | 1 | maximum number of tables to consider when generating answer |\n",
       "| table_score_threshold | float | 0.35 | minimum similarity score for table to be considered in answer |\n",
       "| selfask | bool | False | If True, use an agentic Self-Ask prompting strategy. |\n",
       "| router | NoneType | None | Optional KVRouter instance for automatic filtering |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **Dict** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RAGPipeline.ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db110a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L121){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.semantic_search\n",
       "\n",
       ">      RAGPipeline.semantic_search (query:str, limit:int=4,\n",
       ">                                   score_threshold:float=0.0,\n",
       ">                                   filters:Optional[Dict[str,str]]=None,\n",
       ">                                   where_document=None,\n",
       ">                                   folders:Optional[list]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB.\n",
       "\n",
       "The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
       "If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
       "to filter results.\n",
       "If `LLM.store_type` is 'sparse', then `where_document` should be a boolean search string to filter query in Lucene syntax.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | search query as string |\n",
       "| limit | int | 4 | number of sources to retrieve |\n",
       "| score_threshold | float | 0.0 | minimum threshold for score |\n",
       "| filters | Optional | None | metadata filters |\n",
       "| where_document | NoneType | None | filter search results based syntax of underlying store |\n",
       "| folders | Optional | None | list of folders to consider |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **List** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L121){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.semantic_search\n",
       "\n",
       ">      RAGPipeline.semantic_search (query:str, limit:int=4,\n",
       ">                                   score_threshold:float=0.0,\n",
       ">                                   filters:Optional[Dict[str,str]]=None,\n",
       ">                                   where_document=None,\n",
       ">                                   folders:Optional[list]=None, **kwargs)\n",
       "\n",
       "*Perform a semantic search of the vector DB.\n",
       "\n",
       "The `where_document` parameter varies depending on the value of `LLM.store_type`.\n",
       "If `LLM.store_type` is 'dense', then `where_document` should be a dictionary in Chroma syntax (e.g., {\"$contains\": \"Canada\"})\n",
       "to filter results.\n",
       "If `LLM.store_type` is 'sparse', then `where_document` should be a boolean search string to filter query in Lucene syntax.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query | str |  | search query as string |\n",
       "| limit | int | 4 | number of sources to retrieve |\n",
       "| score_threshold | float | 0.0 | minimum threshold for score |\n",
       "| filters | Optional | None | metadata filters |\n",
       "| where_document | NoneType | None | filter search results based syntax of underlying store |\n",
       "| folders | Optional | None | list of folders to consider |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **List** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RAGPipeline.semantic_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb9ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L225){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.needs_followup\n",
       "\n",
       ">      RAGPipeline.needs_followup (question:str, parse=True, **kwargs)\n",
       "\n",
       "*Decide if follow-up questions are needed*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L225){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.needs_followup\n",
       "\n",
       ">      RAGPipeline.needs_followup (question:str, parse=True, **kwargs)\n",
       "\n",
       "*Decide if follow-up questions are needed*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RAGPipeline.needs_followup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a1e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L215){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.decompose_question\n",
       "\n",
       ">      RAGPipeline.decompose_question (question:str, parse=True, **kwargs)\n",
       "\n",
       "*Decompose a question into subquestions*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#L215){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RAGPipeline.decompose_question\n",
       "\n",
       ">      RAGPipeline.decompose_question (question:str, parse=True, **kwargs)\n",
       "\n",
       "*Decompose a question into subquestions*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RAGPipeline.decompose_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class CategorySelection(BaseModel):\n",
    "    \"\"\"Pydantic model for category selection response.\"\"\"\n",
    "    category: str = Field(description=\"Selected category value or 'none' if no appropriate category\")\n",
    "\n",
    "\n",
    "class KVRouter:\n",
    "    \"\"\"\n",
    "    Key-Value Router for intelligent filtering based on query content.\n",
    "    \n",
    "    Uses an LLM to select the most appropriate field value for filtering\n",
    "    based on the query/question content.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 field_name: str,\n",
    "                 field_descriptions: Dict[str, str],\n",
    "                 llm,\n",
    "                 router_prompt: str = DEFAULT_ROUTER_PROMPT):\n",
    "        \"\"\"\n",
    "        Initialize KV Router.\n",
    "        \n",
    "        Args:\n",
    "            field_name: The metadata field name to filter on (e.g., 'folder')\n",
    "            field_descriptions: Dict mapping field values to descriptions\n",
    "                               (e.g., {'sotu': \"Biden's State of the Union Address\"})\n",
    "            llm: The LLM instance to use for routing decisions\n",
    "            router_prompt: Template for the routing prompt\n",
    "        \"\"\"\n",
    "        self.field_name = field_name\n",
    "        self.field_descriptions = field_descriptions\n",
    "        self.llm = llm\n",
    "        self.router_prompt = router_prompt\n",
    "    \n",
    "    def _format_categories(self) -> str:\n",
    "        \"\"\"Format field descriptions for the prompt.\"\"\"\n",
    "        categories = []\n",
    "        for value, description in self.field_descriptions.items():\n",
    "            categories.append(f\"- {value}: {description}\")\n",
    "        return \"\\n\".join(categories)\n",
    "    \n",
    "\n",
    "    def route(self, question: str, **kwargs) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Select the best field value for the given question.\n",
    "        Extra **kwargs supplied to LLM.prompt.\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question/query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary for filters parameter, or None if no appropriate category\n",
    "            Example: {'folder': 'sotu'} or None\n",
    "        \"\"\"\n",
    "        # Format the prompt\n",
    "        categories_text = self._format_categories()\n",
    "        prompt = format_string(\n",
    "            self.router_prompt,\n",
    "            question=question,\n",
    "            categories=categories_text\n",
    "        )\n",
    "        \n",
    "        # Use response_format for structured output\n",
    "        try:\n",
    "            response = self.llm.pydantic_prompt(\n",
    "                prompt, \n",
    "                pydantic_model=CategorySelection,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            selected_category = response.category.strip().lower()\n",
    "            print(selected_category)\n",
    "            \n",
    "            # Check if it's a valid category (case-insensitive)\n",
    "            valid_values = {v.lower(): v for v in self.field_descriptions.keys()}\n",
    "            \n",
    "            if selected_category in valid_values:\n",
    "                return {self.field_name: valid_values[selected_category]}\n",
    "            elif selected_category == 'none':\n",
    "                return None\n",
    "            else:\n",
    "                # Fuzzy matching: check if any valid value appears in the selected_category\n",
    "                for valid_key, original_value in valid_values.items():\n",
    "                    if valid_key in selected_category:\n",
    "                        return {self.field_name: original_value}\n",
    "                \n",
    "                # Fallback: if response doesn't match, return None\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Fallback to None if pydantic parsing fails\n",
    "            import warnings\n",
    "            warnings.warn(f'KVRouter output parsing error - no KVRouter filters used: {str(e)}')\n",
    "            return None\n",
    "    \n",
    "    def route_and_search(self, \n",
    "                        query: str,\n",
    "                        rag_pipeline,\n",
    "                        **search_kwargs) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Convenience method that routes and performs semantic search.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            rag_pipeline: RAGPipeline instance to search with\n",
    "            **search_kwargs: Additional arguments passed to semantic_search\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects\n",
    "        \"\"\"\n",
    "        # Get routing decision\n",
    "        route_filters = self.route(query)\n",
    "        \n",
    "        # Merge with existing filters if any\n",
    "        existing_filters = search_kwargs.get('filters', {})\n",
    "        if route_filters:\n",
    "            if existing_filters:\n",
    "                # Combine filters\n",
    "                combined_filters = existing_filters.copy()\n",
    "                combined_filters.update(route_filters)\n",
    "                search_kwargs['filters'] = combined_filters\n",
    "            else:\n",
    "                search_kwargs['filters'] = route_filters\n",
    "        \n",
    "        # Perform search\n",
    "        return rag_pipeline.semantic_search(query, **search_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d29e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### KVRouter.route\n",
       "\n",
       ">      KVRouter.route (question:str)\n",
       "\n",
       "*Select the best field value for the given question.\n",
       "\n",
       "Args:\n",
       "    question: The user's question/query\n",
       "\n",
       "Returns:\n",
       "    Dictionary for filters parameter, or None if no appropriate category\n",
       "    Example: {'folder': 'sotu'} or None*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### KVRouter.route\n",
       "\n",
       ">      KVRouter.route (question:str)\n",
       "\n",
       "*Select the best field value for the given question.\n",
       "\n",
       "Args:\n",
       "    question: The user's question/query\n",
       "\n",
       "Returns:\n",
       "    Dictionary for filters parameter, or None if no appropriate category\n",
       "    Example: {'folder': 'sotu'} or None*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(KVRouter.route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff96a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### KVRouter.route_and_search\n",
       "\n",
       ">      KVRouter.route_and_search (query:str, rag_pipeline, **search_kwargs)\n",
       "\n",
       "*Convenience method that routes and performs semantic search.\n",
       "\n",
       "Args:\n",
       "    query: The search query\n",
       "    rag_pipeline: RAGPipeline instance to search with\n",
       "    **search_kwargs: Additional arguments passed to semantic_search\n",
       "\n",
       "Returns:\n",
       "    List of Document objects*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/pipelines/rag.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### KVRouter.route_and_search\n",
       "\n",
       ">      KVRouter.route_and_search (query:str, rag_pipeline, **search_kwargs)\n",
       "\n",
       "*Convenience method that routes and performs semantic search.\n",
       "\n",
       "Args:\n",
       "    query: The search query\n",
       "    rag_pipeline: RAGPipeline instance to search with\n",
       "    **search_kwargs: Additional arguments passed to semantic_search\n",
       "\n",
       "Returns:\n",
       "    List of Document objects*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(KVRouter.route_and_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3aa06",
   "metadata": {},
   "source": [
    "## Example: Using Query Routing with RAG\n",
    "\n",
    "In this example, we use the `KVRouter` to route RAG queries to the correct set of ingested documents.\n",
    "\n",
    "First, when we ingest documents, we assign a folder field to each document chunk using the `file_callables` argument. (You can also use the `text_callables` parameter to assign a field value based on text content.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2549ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |  notest\n",
    "\n",
    "from onprem import LLM\n",
    "from onprem.pipelines import KVRouter\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ed73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /tmp/tmpzazbew9_/dense\n",
      "Loading documents from tests/sample_data/sotu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|█████████████████████| 1/1 [00:00<00:00, 215.95it/s]\n",
      "Processing and chunking 1 new documents: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 994.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 43 chunks of text (max. 1000 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n",
      "Appending to existing vectorstore at /tmp/tmpzazbew9_/dense\n",
      "Loading documents from tests/sample_data/ktrain_paper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Processing and chunking 6 new documents: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1353.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 22 chunks of text (max. 1000 chars each for text; max. 2000 chars for tables)\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "\n",
    "# Setup LLM and ingest with custom metadata\n",
    "llm = LLM('openai/gpt-4o-mini', vectordb_path=tempfile.mkdtemp())\n",
    "def set_folder(filepath):\n",
    "    if 'sotu' in filepath:\n",
    "        return 'sotu'\n",
    "    elif 'ktrain_paper' in filepath:\n",
    "        return 'ktrain'\n",
    "    else:\n",
    "        return 'na'\n",
    "        \n",
    "llm.ingest('tests/sample_data/sotu', file_callables={'folder': set_folder})\n",
    "llm.ingest('tests/sample_data/ktrain_paper', file_callables={'folder': set_folder})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9953c",
   "metadata": {},
   "source": [
    "Next, we setup a `KVRouter` that returns the best key-value pair (in this case, a specific `folder` value) based on the question or query.  The key-value pair is then used to filter the documents appropriately when retrieving source documents for answer generation.  The router can be supplied direclty to the `ask` method so that only documents in the appropriate folder are considered when generating answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5254c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"category\":\"ktrain\"}\n",
      "```\n",
      "{'folder': 'ktrain'}\n"
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "\n",
    "# Create router\n",
    "router = KVRouter(\n",
    "  field_name='folder',\n",
    "  field_descriptions={\n",
    "      'sotu': \"Biden's State of the Union Address\",\n",
    "      'ktrain': \"Research papers about ktrain library, a toolkit for machine learning, text classification, and computer vision.\"\n",
    "  },\n",
    "  llm=llm\n",
    ")\n",
    "\n",
    "# Example of router\n",
    "filter_dict = router.route('Tell me about image classification')\n",
    "print()\n",
    "print(filter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a042ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"category\":\"sotu\"}\n",
      "```Biden discussed a new economic vision focused on investing in America, educating Americans, and growing the workforce. He criticized the trickle-down economic theory, stating it led to weaker economic growth, lower wages, and a widening wealth gap. He emphasized the importance of infrastructure investment, asserting that it would help the U.S. compete globally, particularly against China. Biden highlighted job creation through significant investments from companies like Ford and GM in electric vehicles. He acknowledged the struggles families face due to inflation and stated that his top priority is to get prices under control."
     ]
    }
   ],
   "source": [
    "# |notest\n",
    "\n",
    "# Use router with ask() - Method 1: Direct parameter\n",
    "result = llm.ask(\n",
    "  \"What did Biden say about the economy?\",\n",
    "  router=router\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd72d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"category\":\"ktrain\"}\n",
      "```To use ktrain for text classification, you can follow these simplified steps:\n",
      "\n",
      "1. **Load and Preprocess Data**: Use ktrain's preprocessing functions to load your text data and preprocess it. This typically involves tokenization and converting texts into a format that the model can understand.\n",
      "\n",
      "2. **Create Model**: Define your model using ktrain's built-in functions. You can customize it according to your needs, such as choosing the architecture or adjusting hyperparameters.\n",
      "\n",
      "3. **Train the Model**: Use ktrain's training functions to fit the model on your preprocessed data. You'll specify the number of epochs and other training parameters.\n",
      "\n",
      "4. **Evaluate the Model**: After training, you can evaluate your model's performance using ktrain's evaluation tools, which can include generating classification reports.\n",
      "\n",
      "5. **Make Predictions**: Finally, use the trained model to make predictions on new, unseen text data, leveraging the preprocessor instance created earlier.\n",
      "\n",
      "This process can typically be done in just a few lines of code, making ktrain a low-code solution for text classification tasks. For detailed code examples, refer to the ktrain GitHub repository."
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "\n",
    "# Use router with RAG pipeline - Method 2: Direct on pipeline\n",
    "rag_pipeline = llm.load_rag_pipeline()\n",
    "result = rag_pipeline.ask(\n",
    "  \"How do I use ktrain for text classification?\",\n",
    "  router=router\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c857ec",
   "metadata": {},
   "source": [
    "## Example: Deciding On Follow-Up Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "rag_pipeline.needs_followup('What is ktrain?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0611c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "rag_pipeline.needs_followup('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25876193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "rag_pipeline.needs_followup(\"How was Paul Grahams life different before, during, and after YC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "rag_pipeline.needs_followup(\"Compare and contrast the customer segments and geographies of Lyft and Uber that grew the fastest.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6571282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |  notest\n",
    "\n",
    "rag_pipeline.needs_followup(\"Compare and contrast Uber and Lyft.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3aa31",
   "metadata": {},
   "source": [
    "## Example: Generating Follow-Up Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290162f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"items\": [\n",
      "        {\n",
      "            \"sub_question\": \"What are the customer segments of Lyft that grew the fastest\",\n",
      "        },\n",
      "        {\n",
      "            \"sub_question\": \"What are the customer segments of Uber that grew the fastest\",\n",
      "        },\n",
      "        {\n",
      "            \"sub_question\": \"Which geographies showed the fastest growth for Lyft\",\n",
      "        },\n",
      "        {\n",
      "            \"sub_question\": \"Which geographies showed the fastest growth for Uber\",\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "['What are the customer segments of Lyft that grew the fastest', 'What are the customer segments of Uber that grew the fastest', 'Which geographies showed the fastest growth for Lyft', 'Which geographies showed the fastest growth for Uber']\n"
     ]
    }
   ],
   "source": [
    "#|notest\n",
    "question = \"Compare and contrast the customer segments and geographies of Lyft and Uber that grew the fastest.\"\n",
    "subquestions = rag_pipeline.decompose_question(question, parse=False)\n",
    "print()\n",
    "print(subquestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8009558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bf426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb788d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
