{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelines.rag\n",
    "\n",
    "> A pipeline module for Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines.rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Optional, Dict, List, Any\n",
    "from langchain_core.documents import Document\n",
    "from onprem.utils import format_string\n",
    "from onprem.llm import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_QA_PROMPT = \"\"\"Use the following pieces of context delimited by three backticks to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "```{context}```\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation pipeline for answering questions based on source documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, qa_template: str = DEFAULT_QA_PROMPT):\n",
    "        \"\"\"\n",
    "        Initialize RAG pipeline.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model instance (LLM object)\n",
    "            qa_template: Question-answering prompt template\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.qa_template = qa_template\n",
    "    \n",
    "    def _retrieve_documents(self, \n",
    "                          question: str,\n",
    "                          filters: Optional[Dict[str, str]] = None,\n",
    "                          where_document = None,\n",
    "                          folders: Optional[list] = None,\n",
    "                          limit: int = 4,\n",
    "                          score_threshold: float = 0.0,\n",
    "                          table_k: int = 1,\n",
    "                          table_score_threshold: float = 0.35) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from vector database.\n",
    "        \"\"\"\n",
    "        docs = self.llm.semantic_search(\n",
    "            question, \n",
    "            filters=filters, \n",
    "            where_document=where_document, \n",
    "            folders=folders,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        # Add table documents if requested\n",
    "        if table_k > 0:\n",
    "            table_filters = filters.copy() if filters else {}\n",
    "            table_filters = dict(table_filters, table=True)\n",
    "            table_docs = self.llm.semantic_search(\n",
    "                f'{question} (table)', \n",
    "                filters=table_filters, \n",
    "                where_document=where_document,\n",
    "                folders=folders,\n",
    "                limit=table_k,\n",
    "                score_threshold=table_score_threshold\n",
    "            )\n",
    "            if table_docs:\n",
    "                docs.extend(table_docs[:limit])\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def _generate_answer(self, question: str, context: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using the language model.\n",
    "        \"\"\"\n",
    "        prompt = format_string(\n",
    "            self.qa_template,\n",
    "            question=question,\n",
    "            context=context\n",
    "        )\n",
    "        return self.llm.prompt(prompt, **kwargs)\n",
    "    \n",
    "    def ask(self,\n",
    "            question: str, # question as string\n",
    "            contexts: Optional[list] = None, # optional list of contexts to answer question. If None, retrieve from vectordb.\n",
    "            qa_template: Optional[str] = None, # question-answering prompt template to use\n",
    "            filters: Optional[Dict[str, str]] = None, # filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True})\n",
    "            where_document = None, # filter sources by document content (syntax varies by store type)\n",
    "            folders: Optional[list] = None, # folders to search (needed because LangChain does not forward \"where\" parameter)\n",
    "            limit: Optional[int] = None, # Number of sources to consider. If None, use `LLM.rag_num_source_docs`.\n",
    "            score_threshold: Optional[float] = None, # minimum similarity score of source. If None, use `LLM.rag_score_threshold`.\n",
    "            table_k: int = 1, # maximum number of tables to consider when generating answer\n",
    "            table_score_threshold: float = 0.35, # minimum similarity score for table to be considered in answer\n",
    "            selfask: bool = False, # If True, use an agentic Self-Ask prompting strategy.\n",
    "            **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Answer a question using RAG approach.\n",
    "        \n",
    "        Args:\n",
    "            question: Question to answer\n",
    "            contexts: Optional list of contexts. If None, retrieve from vectordb\n",
    "            qa_template: Optional custom QA prompt template\n",
    "            filters: Filter sources by metadata values\n",
    "            where_document: Filter sources by document content\n",
    "            folders: Folders to search\n",
    "            limit: Number of sources to consider\n",
    "            score_threshold: Minimum similarity score\n",
    "            table_k: Maximum number of tables to consider\n",
    "            table_score_threshold: Minimum similarity score for tables\n",
    "            selfask: Use agentic Self-Ask prompting strategy\n",
    "            **kwargs: Additional arguments passed to LLM.prompt\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with keys: answer, source_documents, question\n",
    "        \"\"\"\n",
    "        template = qa_template or self.qa_template\n",
    "        limit = limit if limit is not None else self.llm.rag_num_source_docs\n",
    "        score_threshold = score_threshold if score_threshold is not None else self.llm.rag_score_threshold\n",
    "        \n",
    "        if selfask and helpers.needs_followup(question, self.llm):\n",
    "            return self._ask_with_decomposition(\n",
    "                question, template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            return self._ask_direct(\n",
    "                question, contexts, template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, **kwargs\n",
    "            )\n",
    "    \n",
    "    def _ask_direct(self,\n",
    "                   question: str,\n",
    "                   contexts: Optional[list],\n",
    "                   qa_template: str,\n",
    "                   filters: Optional[Dict[str, str]],\n",
    "                   where_document,\n",
    "                   folders: Optional[list],\n",
    "                   limit: int,\n",
    "                   score_threshold: float,\n",
    "                   table_k: int,\n",
    "                   table_score_threshold: float,\n",
    "                   **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Direct RAG without decomposition.\"\"\"\n",
    "        if contexts is None:\n",
    "            docs = self._retrieve_documents(\n",
    "                question, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold\n",
    "            )\n",
    "            context = '\\n\\n'.join([d.page_content for d in docs])\n",
    "        else:\n",
    "            docs = [Document(page_content=c, metadata={'source': '<SUBANSWER>'}) for c in contexts]\n",
    "            context = \"\\n\\n\".join(contexts)\n",
    "        \n",
    "        answer = self._generate_answer(question, context, **kwargs)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'source_documents': docs\n",
    "        }\n",
    "    \n",
    "    def _ask_with_decomposition(self,\n",
    "                               question: str,\n",
    "                               qa_template: str,\n",
    "                               filters: Optional[Dict[str, str]],\n",
    "                               where_document,\n",
    "                               folders: Optional[list],\n",
    "                               limit: int,\n",
    "                               score_threshold: float,\n",
    "                               table_k: int,\n",
    "                               table_score_threshold: float,\n",
    "                               **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"RAG with question decomposition (Self-Ask).\"\"\"\n",
    "        subquestions = helpers.decompose_question(question, self.llm)\n",
    "        subanswers = []\n",
    "        sources = []\n",
    "        \n",
    "        for q in subquestions:\n",
    "            res = self._ask_direct(\n",
    "                q, None, qa_template, filters, where_document, folders,\n",
    "                limit, score_threshold, table_k, table_score_threshold, **kwargs\n",
    "            )\n",
    "            subanswers.append(res['answer'])\n",
    "            for doc in res['source_documents']:\n",
    "                doc.metadata = dict(doc.metadata, subquestion=q)\n",
    "            sources.extend(res['source_documents'])\n",
    "        \n",
    "        # Generate final answer based on subanswers\n",
    "        res = self._ask_direct(\n",
    "            question, subanswers, qa_template, filters, where_document, folders,\n",
    "            limit, score_threshold, table_k, table_score_threshold, **kwargs\n",
    "        )\n",
    "        res['source_documents'] = sources\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### RAGPipeline.ask\n",
       "\n",
       ">      RAGPipeline.ask (question:str, contexts:Optional[list]=None,\n",
       ">                       qa_template:Optional[str]=None,\n",
       ">                       filters:Optional[Dict[str,str]]=None,\n",
       ">                       where_document=None, folders:Optional[list]=None,\n",
       ">                       limit:Optional[int]=None,\n",
       ">                       score_threshold:Optional[float]=None, table_k:int=1,\n",
       ">                       table_score_threshold:float=0.35, selfask:bool=False,\n",
       ">                       **kwargs)\n",
       "\n",
       "*Answer a question using RAG approach.\n",
       "\n",
       "Args:\n",
       "    question: Question to answer\n",
       "    contexts: Optional list of contexts. If None, retrieve from vectordb\n",
       "    qa_template: Optional custom QA prompt template\n",
       "    filters: Filter sources by metadata values\n",
       "    where_document: Filter sources by document content\n",
       "    folders: Folders to search\n",
       "    limit: Number of sources to consider\n",
       "    score_threshold: Minimum similarity score\n",
       "    table_k: Maximum number of tables to consider\n",
       "    table_score_threshold: Minimum similarity score for tables\n",
       "    selfask: Use agentic Self-Ask prompting strategy\n",
       "    **kwargs: Additional arguments passed to LLM.prompt\n",
       "\n",
       "Returns:\n",
       "    Dictionary with keys: answer, source_documents, question*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| question | str |  | question as string |\n",
       "| contexts | Optional | None | optional list of contexts to answer question. If None, retrieve from vectordb. |\n",
       "| qa_template | Optional | None | question-answering prompt template to use |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | NoneType | None | filter sources by document content (syntax varies by store type) |\n",
       "| folders | Optional | None | folders to search (needed because LangChain does not forward \"where\" parameter) |\n",
       "| limit | Optional | None | Number of sources to consider. If None, use `LLM.rag_num_source_docs`. |\n",
       "| score_threshold | Optional | None | minimum similarity score of source. If None, use `LLM.rag_score_threshold`. |\n",
       "| table_k | int | 1 | maximum number of tables to consider when generating answer |\n",
       "| table_score_threshold | float | 0.35 | minimum similarity score for table to be considered in answer |\n",
       "| selfask | bool | False | If True, use an agentic Self-Ask prompting strategy. |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **Dict** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### RAGPipeline.ask\n",
       "\n",
       ">      RAGPipeline.ask (question:str, contexts:Optional[list]=None,\n",
       ">                       qa_template:Optional[str]=None,\n",
       ">                       filters:Optional[Dict[str,str]]=None,\n",
       ">                       where_document=None, folders:Optional[list]=None,\n",
       ">                       limit:Optional[int]=None,\n",
       ">                       score_threshold:Optional[float]=None, table_k:int=1,\n",
       ">                       table_score_threshold:float=0.35, selfask:bool=False,\n",
       ">                       **kwargs)\n",
       "\n",
       "*Answer a question using RAG approach.\n",
       "\n",
       "Args:\n",
       "    question: Question to answer\n",
       "    contexts: Optional list of contexts. If None, retrieve from vectordb\n",
       "    qa_template: Optional custom QA prompt template\n",
       "    filters: Filter sources by metadata values\n",
       "    where_document: Filter sources by document content\n",
       "    folders: Folders to search\n",
       "    limit: Number of sources to consider\n",
       "    score_threshold: Minimum similarity score\n",
       "    table_k: Maximum number of tables to consider\n",
       "    table_score_threshold: Minimum similarity score for tables\n",
       "    selfask: Use agentic Self-Ask prompting strategy\n",
       "    **kwargs: Additional arguments passed to LLM.prompt\n",
       "\n",
       "Returns:\n",
       "    Dictionary with keys: answer, source_documents, question*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| question | str |  | question as string |\n",
       "| contexts | Optional | None | optional list of contexts to answer question. If None, retrieve from vectordb. |\n",
       "| qa_template | Optional | None | question-answering prompt template to use |\n",
       "| filters | Optional | None | filter sources by metadata values using Chroma metadata syntax (e.g., {'table':True}) |\n",
       "| where_document | NoneType | None | filter sources by document content (syntax varies by store type) |\n",
       "| folders | Optional | None | folders to search (needed because LangChain does not forward \"where\" parameter) |\n",
       "| limit | Optional | None | Number of sources to consider. If None, use `LLM.rag_num_source_docs`. |\n",
       "| score_threshold | Optional | None | minimum similarity score of source. If None, use `LLM.rag_score_threshold`. |\n",
       "| table_k | int | 1 | maximum number of tables to consider when generating answer |\n",
       "| table_score_threshold | float | 0.35 | minimum similarity score for table to be considered in answer |\n",
       "| selfask | bool | False | If True, use an agentic Self-Ask prompting strategy. |\n",
       "| kwargs | VAR_KEYWORD |  |  |\n",
       "| **Returns** | **Dict** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RAGPipeline.ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
