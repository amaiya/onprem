{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Built-In Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**[OnPrem.LLM](https://github.com/amaiya/onprem)** includes a built-in web app to easily access and use LLMs. After [installing](https://github.com/amaiya/onprem#install) OnPrem.LLM, you can start it by running the following command at the command-line:\n",
    "\n",
    "```shell\n",
    "# run at command-line\n",
    "onprem --port 8000\n",
    "```\n",
    "Then, enter `localhost:8000` in your Web browser to access the application:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_welcome.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>\n",
    "\n",
    "The Web app is implemented with [streamlit](https://streamlit.io/): `pip install streamlit`.  If it is not already installed, the `onprem` command will ask you to install it.\n",
    "Here is more information on the `onprem` command:\n",
    "```sh\n",
    "$:~/projects/github/onprem$ onprem --help\n",
    "usage: onprem [-h] [-p PORT] [-a ADDRESS] [-v]\n",
    "\n",
    "Start the OnPrem.LLM web app\n",
    "Example: onprem --port 8000\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -p PORT, --port PORT  Port to use; default is 8501\n",
    "  -a ADDRESS, --address ADDRESS\n",
    "                        Address to bind; default is 0.0.0.0\n",
    "  -v, --version         Print a version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app requires a file called `config.yml` exists in the `onprem_data/webapp` folder in the user's home directory. This file stores information used by the Web app such as the model to use. If one does not exist, then a default one will be created for you and is also shown below:\n",
    "or FALSE)\n",
    "  show_manage: TRUE\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# Default YAML configuration\n",
    "llm:\n",
    "  # model url (or model file name if previously downloaded)\n",
    "  # if changing, be sure to update the prompt_template variable below\n",
    "  model_url: https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf\n",
    "  # number of layers offloaded to GPU\n",
    "  n_gpu_layers: -1\n",
    "  # the vector store type to use (dual, dense, or sparse)\n",
    "  # dual: a vector store where both Chroma semantic searches and conventional keyword searches are supported\n",
    "  store_type: dual\n",
    "  # path to vector db folder\n",
    "  vectordb_path: {datadir}/vectordb\n",
    "  # path to model download folder\n",
    "  model_download_path: {datadir}\n",
    "  # number of source documents used by LLM.ask and LLM.chat\n",
    "  rag_num_source_docs: 6\n",
    "  # minimum similarity score for source to be considered by LLM.ask/LLM.chat\n",
    "  rag_score_threshold: 0.0\n",
    "  # verbosity of Llama.cpp\n",
    "  # additional parameters added in the \"llm\" YAML section will be fed directly to LlamaCpp (e.g., temperature)\n",
    "  #temperature: 0.0\n",
    "prompt:\n",
    "  # The default prompt_template is specifically for Zephyr-7B.\n",
    "  # It will need to be changed if you change the model_url above.\n",
    "  prompt_template: <|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\n",
    "ui:\n",
    "  # title of application\n",
    "  title: OnPrem.LLM\n",
    "  # subtitle in \"Talk to Your Documents\" screen\n",
    "  rag_title:\n",
    "  # path to markdown file with contents that will be inserted below rag_title\n",
    "  rag_text_path:\n",
    "  # path to folder containing raw documents (i.e., absolute path of folder you supplied to LLM.ingest)\n",
    "  rag_source_path:\n",
    "  # base url (leave blank unless you're running your own separate web server to serve sour\n",
    "  # whether to show the Manage page in the sidebar\n",
    "  show_manage: TRUE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit the file based on your requirements. Variables in the `llm` section are automatically passed to the `onprem.LLM` constructor, which, in turn, passes extra `**kwargs` to `llama-cpp-python` or the `transformers.pipeline`.  For instance, you can add a `temperature` variable in the `llm` section to adjust temperature of the model in the web app (e.g., lower values closer to 0.0 for more deterministic output and higher values for more creativity). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default model is a 7B-parameter model called [Zephyr-7B](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF).\n",
    "\n",
    "\n",
    "If you'd like the LLM to support longer answers than the default 512 tokens, you can add a `max_tokens` parameter to the `llm` section.\n",
    "\n",
    "\n",
    "If [using Ollama as the LLM engine](https://amaiya.github.io/onprem/#ollama-example), you can replace the default `model_url` entry with something like:\n",
    "\n",
    "```\n",
    "llm:\n",
    "    model_url: http://localhost:11434/v1\n",
    "    model: llama3.2\n",
    "    api_key: na\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set the `model_url` or `model_id` parameter to point to a model of your choosing. Note that some models have particular prompt formats.  For instance, if using the default **Zephyr-7B** model above, as described on the [model's home page](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF#prompt-template-zephyr), the `prompt_template` in the YAML file must be set to:\n",
    "```yaml\n",
    "prompt:\n",
    "  prompt_template: <|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\n",
    "```\n",
    "\n",
    "If changing models, don't forget to update the `prompt_template` variable with the prompt format approrpriate for your chosen model.\n",
    "\n",
    "You do not need a `prompt_template` value if using **Ollama** or **transformers** as the LLM engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Prompts to Solve Problems\n",
    "\n",
    "The first app page is a UI for interactive chatting and prompting to solve problems various problems with local LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_prompting.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk To Your Documents\n",
    "The second screen in the app is a UI for [retrieval augmented generation](https://arxiv.org/abs/2005.11401) or RAG (i.e., chatting with documents). Sources considered by the LLM when generating answers are displayed and ranked by answer-to-source similarity. Hovering over the question marks in the sources will display the snippets of text from a document considered by the LLM when generating answers.  Documents you would like to consider as sources for question-answering can be uploaded through the Web UI and this is discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_rag.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Analysis \n",
    "\n",
    "The third screen is a UI for applying prompts to passages within uploaded documents. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_analysis.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Search \n",
    "\n",
    "The fourth screen is a UI for searching documents you've uploaded either through keyword searches or semantic searches. Documents that you would like to search can be uploaded through the Web app and is discussed next.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_search.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Documents\n",
    "\n",
    "Both document search an document question-answering, as discussed above, require you to ingest documents into a vector store.  By default, the web app uses a dual vector store that stores documents in both a conventional vector database (for semantic search) and a search index (for keyword searches).\n",
    "\n",
    "You can ingest documents either manually or through a Web interface.\n",
    "\n",
    "### Uploading Documents Through the Web Interface\n",
    "\n",
    "The Web UI includes a point-and-click interface to upload and index documents into the vector store(s). Documents can either be uploaded individually or as a zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_upload.png\" border=\"0\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting Documents Through the Python API\n",
    "\n",
    "You can also ingest documents through the Python API.  By default, the web app assumes that the original documents are stored in `<home directory>/onprem_data/webapp/documents` and assumes the vector stores reside in `<home directory>/onprem_data/webapp/vectordb`.  We just need to point `LLM` to these places when ingesting documents.  Let's assume you copied your project documents to `/home/<username>/onprem_data/webapp/documents/my_project_files`.  You can ingest them, as follows:\n",
    "\n",
    "```python\n",
    "from onprem import LLM\n",
    "llm = LLM(store_type='dual', vectordb_path='/home/<username>/onprem_data/webapp/vectordb')\n",
    "llm.ingest('/home/<username>/onprem_data/webapp/documents/my_project_files')\n",
    "```\n",
    "\n",
    "After the above commands complete, you should be able to search your documents and ask them questions after starting the Web app:\n",
    "\n",
    "```sh\n",
    "onprem --port 8000\n",
    "```\n",
    "\n",
    "## Tips\n",
    "\n",
    "- If you're finding answers get cut off, edit the configuration to set `max_tokens` to higher value than the default 512. (Not necessary when using Ollama, which uses a larger value by default.)\n",
    "- You can change the store type in the config to `store_type=\"sparse\"`, which stores documents in a traditional search engine index, instead of a vector database.  The advantage is that ingestion is a lot faster.  The drawback is that sparse vectorstores assume passages with answers will include at least one word in common with the question.\n",
    "\n",
    "\n",
    "\n",
    "- For reasons that are unclear, parallelized ingestion in the Web interface when running on Windows 11 tends to pause for a long while before starting.  For these reasons, parallelization is disabled for the Web interface when running it on Windows 11.  When running on Windows systems, we recommend ingesting documents through the Python interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
