{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingest\n",
    "\n",
    "> functionality for document ingestion into a vector database for question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from onprem import utils as U\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 500\n",
    "DEFAULT_CHUNK_OVERLAP = 50\n",
    "COLLECTION_NAME = \"onprem_chroma\"\n",
    "CHROMA_MAX = 41000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MyElmLoader(UnstructuredEmailLoader):\n",
    "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                doc = UnstructuredEmailLoader.load(self)\n",
    "            except ValueError as e:\n",
    "                if \"text/html content not found in email\" in str(e):\n",
    "                    # Try plain text\n",
    "                    self.unstructured_kwargs[\"content_source\"] = \"text/plain\"\n",
    "                    doc = UnstructuredEmailLoader.load(self)\n",
    "                else:\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            # Add file_path to exception message\n",
    "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
    "\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Map file extensions to document loaders and their arguments\n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".eml\": (MyElmLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyMuPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "\n",
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a single document (invoked by `load_documents`).\n",
    "    \"\"\"\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1].lower()\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
    "\n",
    "\n",
    "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source documents directory, ignoring specified files\n",
    "    \"\"\"\n",
    "    source_dir = os.path.abspath(source_dir)\n",
    "    all_files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext.lower()}\"), recursive=True)\n",
    "        )\n",
    "        all_files.extend(\n",
    "            glob.glob(os.path.join(source_dir, f\"**/*{ext.upper()}\"), recursive=True)\n",
    "        )\n",
    "    filtered_files = [\n",
    "        file_path for file_path in all_files if file_path not in ignored_files and not os.path.basename(file_path).startswith('~$')\n",
    "    ]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = []\n",
    "        with tqdm(\n",
    "            total=len(filtered_files), desc=\"Loading new documents\", ncols=80\n",
    "        ) as pbar:\n",
    "            for i, docs in enumerate(\n",
    "                pool.imap_unordered(load_single_document, filtered_files)\n",
    "            ):\n",
    "                results.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_documents(\n",
    "    source_directory: str,\n",
    "    ignored_files: List[str] = [],\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents and split in chunks\n",
    "\n",
    "    **Args**:\n",
    "\n",
    "      - *source_directory*: path to folder containing document store\n",
    "      - *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "      - *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "\n",
    "    **Returns:** list of `langchain.docstore.document.Document` objects\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from {source_directory}\")\n",
    "    documents = load_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        return\n",
    "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} chars each)\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def does_vectorstore_exist(db) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if not db.get()[\"documents\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def batchify_chunks(texts):\n",
    "    split_docs_chunked = U.split_list(texts, CHROMA_MAX)\n",
    "    total_chunks = sum(1 for _ in U.split_list(texts, CHROMA_MAX))\n",
    "    return split_docs_chunked, total_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Any, Dict, Generator, List, Optional, Tuple, Union\n",
    "from onprem.utils import get_datadir\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "DEFAULT_DB = \"vectordb\"\n",
    "\n",
    "\n",
    "class Ingester:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        embedding_model_kwargs: dict = {\"device\": \"cpu\"},\n",
    "        embedding_encode_kwargs: dict = {\"normalize_embeddings\": False},\n",
    "        persist_directory: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_folder` (previously-ingested documents are ignored)\n",
    "\n",
    "        **Args**:\n",
    "\n",
    "          - *embedding_model*: name of sentence-transformers model\n",
    "          - *embedding_model_kwargs*: arguments to embedding model (e.g., `{device':'cpu'}`)\n",
    "          - *embedding_encode_kwargs*: arguments to encode method of\n",
    "                                       embedding model (e.g., `{'normalize_embeddings': False}`).\n",
    "          - *persist_directory*: Path to vector database (created if it doesn't exist).\n",
    "                                 Default is `onprem_data/vectordb` in user's home directory.\n",
    "\n",
    "\n",
    "        **Returns**: `None`\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory or os.path.join(\n",
    "            get_datadir(), DEFAULT_DB\n",
    "        )\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,\n",
    "            model_kwargs=embedding_model_kwargs,\n",
    "            encode_kwargs=embedding_encode_kwargs,\n",
    "        )\n",
    "        self.chroma_settings = Settings(\n",
    "            persist_directory=self.persist_directory, anonymized_telemetry=False\n",
    "        )\n",
    "        self.chroma_client = chromadb.PersistentClient(\n",
    "            settings=self.chroma_settings, path=self.persist_directory\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain.vectorstores.Chroma` instance\n",
    "        \"\"\"\n",
    "        db = Chroma(\n",
    "            persist_directory=self.persist_directory,\n",
    "            embedding_function=self.embeddings,\n",
    "            client_settings=self.chroma_settings,\n",
    "            client=self.chroma_client,\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "            collection_name=COLLECTION_NAME,\n",
    "        )\n",
    "        return db if does_vectorstore_exist(db) else None\n",
    "\n",
    "    def get_embedding_model(self):\n",
    "        \"\"\"\n",
    "        Returns an instance to the `langchain.embeddings.huggingface.HuggingFaceEmbeddings` instance\n",
    "        \"\"\"\n",
    "        return self.embeddings\n",
    "\n",
    "    def ingest(\n",
    "        self,\n",
    "        source_directory: str,\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Ingests all documents in `source_directory` (previously-ingested documents are ignored).\n",
    "\n",
    "        **Args**:\n",
    "\n",
    "          - *source_directory*: path to folder containing document store\n",
    "          - *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "          - *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
    "\n",
    "        **Returns**: `None`\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(source_directory):\n",
    "            raise ValueError(\"The source_directory does not exist.\")\n",
    "        elif os.path.isfile(source_directory):\n",
    "            raise ValueError(\n",
    "                \"The source_directory argument must be a folder, not a file.\"\n",
    "            )\n",
    "        texts = None\n",
    "        db = self.get_db()\n",
    "        if db:\n",
    "            # Update and store locally vectorstore\n",
    "            print(f\"Appending to existing vectorstore at {self.persist_directory}\")\n",
    "            collection = db.get()\n",
    "            texts = process_documents(\n",
    "                source_directory,\n",
    "                ignored_files=[\n",
    "                    metadata[\"source\"] for metadata in collection[\"metadatas\"]\n",
    "                ],\n",
    "            )\n",
    "            if texts:\n",
    "                print(f\"Creating embeddings. May take some minutes...\")\n",
    "                chunk_batches, total_chunks = batchify_chunks(texts)\n",
    "                for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                    db.add_documents(lst)\n",
    "        else:\n",
    "            # Create and store locally vectorstore\n",
    "            print(f\"Creating new vectorstore at {self.persist_directory}\")\n",
    "            texts = process_documents(\n",
    "                source_directory, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "            )\n",
    "\n",
    "            if texts:\n",
    "                chunk_batches, total_chunks = batchify_chunks(texts)\n",
    "                print(\"Creating embeddings. May take some minutes...\")\n",
    "                db = None\n",
    "\n",
    "                for lst in tqdm(chunk_batches, total=total_chunks):\n",
    "                    if not db:\n",
    "                        db = Chroma.from_documents(\n",
    "                            lst,\n",
    "                            self.embeddings,\n",
    "                            persist_directory=self.persist_directory,\n",
    "                            client_settings=self.chroma_settings,\n",
    "                            client=self.chroma_client,\n",
    "                            collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                            collection_name=COLLECTION_NAME,\n",
    "                        )\n",
    "                    else:\n",
    "                        db.add_documents(lst)\n",
    "        if texts:\n",
    "            print(\n",
    "                \"Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\"\n",
    "            )\n",
    "        db = None\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L197){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_embedding_model\n",
       "\n",
       ">      Ingester.get_embedding_model ()\n",
       "\n",
       "Returns an instance to the `langchain.embeddings.huggingface.HuggingFaceEmbeddings` instance"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L197){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_embedding_model\n",
       "\n",
       ">      Ingester.get_embedding_model ()\n",
       "\n",
       "Returns an instance to the `langchain.embeddings.huggingface.HuggingFaceEmbeddings` instance"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.get_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L185){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_db\n",
       "\n",
       ">      Ingester.get_db ()\n",
       "\n",
       "Returns an instance to the `langchain.vectorstores.Chroma` instance"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L185){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.get_db\n",
       "\n",
       ">      Ingester.get_db ()\n",
       "\n",
       "Returns an instance to the `langchain.vectorstores.Chroma` instance"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.get_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L204){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.ingest\n",
       "\n",
       ">      Ingester.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                       chunk_overlap:int=50)\n",
       "\n",
       "Ingests all documents in `source_directory` (previously-ingested documents are ignored).\n",
       "\n",
       "**Args**:\n",
       "\n",
       "  - *source_directory*: path to folder containing document store\n",
       "  - *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "  - *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "\n",
       "**Returns**: `None`"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/amaiya/onprem/blob/master/onprem/ingest.py#L204){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Ingester.ingest\n",
       "\n",
       ">      Ingester.ingest (source_directory:str, chunk_size:int=500,\n",
       ">                       chunk_overlap:int=50)\n",
       "\n",
       "Ingests all documents in `source_directory` (previously-ingested documents are ignored).\n",
       "\n",
       "**Args**:\n",
       "\n",
       "  - *source_directory*: path to folder containing document store\n",
       "  - *chunk_size*: text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "  - *chunk_overlap*: character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n",
       "\n",
       "**Returns**: `None`"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Ingester.ingest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 11:35:20.660565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore at /home/amaiya/onprem_data/vectordb\n",
      "Loading documents from sample_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 2/2 [00:00<00:00, 16.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 new documents from sample_data\n",
      "Split into 62 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the LLM.ask method\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "ingester = Ingester()\n",
    "ingester.ingest(\"sample_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
