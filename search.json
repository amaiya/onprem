[
  {
    "objectID": "examples_agent.html",
    "href": "examples_agent.html",
    "title": "Agent-Based Task Execution",
    "section": "",
    "text": "First, we’ll set up our LLM instance that will power our agent. We’ll first use GPT-4o-mini, as cloud LLMs tend to handle agentic workflows faster (and sometimes better), but you can also try any number of tinier local models. (At the end of this notebook, we illustrate this using one of the examples.)\n\nfrom onprem import LLM\nfrom onprem.pipelines import Agent\nllm = LLM('openai/gpt-4o-mini', mute_stream=True) \nagent = Agent(llm)",
    "crumbs": [
      "Examples",
      "Agent-Based Task Execution"
    ]
  },
  {
    "objectID": "examples_agent.html#setup-the-llm-and-agent",
    "href": "examples_agent.html#setup-the-llm-and-agent",
    "title": "Agent-Based Task Execution",
    "section": "",
    "text": "First, we’ll set up our LLM instance that will power our agent. We’ll first use GPT-4o-mini, as cloud LLMs tend to handle agentic workflows faster (and sometimes better), but you can also try any number of tinier local models. (At the end of this notebook, we illustrate this using one of the examples.)\n\nfrom onprem import LLM\nfrom onprem.pipelines import Agent\nllm = LLM('openai/gpt-4o-mini', mute_stream=True) \nagent = Agent(llm)",
    "crumbs": [
      "Examples",
      "Agent-Based Task Execution"
    ]
  },
  {
    "objectID": "examples_agent.html#giving-the-agent-access-to-tools",
    "href": "examples_agent.html#giving-the-agent-access-to-tools",
    "title": "Agent-Based Task Execution",
    "section": "Giving the Agent Access to Tools",
    "text": "Giving the Agent Access to Tools\nNext, we will give the agent access tools when executing a given task. Examples of tool types include the ability to do the following: 1. perform a web search 2. visit a web page 3. search your documents stored within a vector store (e.g., agentic RAG) 4. accessing Python interpreter 5. accessing an MCP server 6. executing a custom function that you provide (i.e., implementing your own custom tools)\n\nExample: Using a Custom Tool with Web Search\nIn this example, we will give the agent access to web search and a function that returns today’s date to find historical events for the current day.\n\ndef today() -&gt; str:\n    \"\"\"\n    Gets the current date and time\n\n    Returns:\n        current date and time\n    \"\"\"\n    from datetime import datetime\n    return datetime.today().isoformat()\n\nagent.add_function_tool(today)\nagent.add_websearch_tool()\nfor tup in agent.tools.items():\n    print(tup)\n\n('today', &lt;smolagents.tools.tool.&lt;locals&gt;.SimpleTool object&gt;)\n('websearch', &lt;smolagents.default_tools.WebSearchTool object&gt;)\n\n\n\nanswer = agent.run(\"Any famous events that happened on today's date?\")\n\n╭──────────────────────────────────────────────────── New run ────────────────────────────────────────────────────╮\n│                                                                                                                 │\n│ Any famous events that happened on today's date?                                                                │\n│                                                                                                                 │\n╰─ AgentModel - None ─────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ Calling tool: 'today' with arguments: {}                                                                        │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nObservations: 2025-06-24T14:39:35.557320\n\n\n\n[Step 1: Duration 1.96 seconds]\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ Calling tool: 'web_search' with arguments: {'query': 'famous events on June 24th'}                              │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nObservations: ## Search Results\n\n|Historical Events on June 24 - On This Day](https://www.onthisday.com/events/june/24)\n June in History. How the Qwerty Keyboard was Born June 23, 1868; The Fatal Dance Manias of Medieval Europe June \n24, 1374; Custer's Last Stand June 25, 1876; Kennedy's Clarion Call for Freedom June 26, 1963\n\n|June 24th: All Facts & Events That Happened Today In \nHistory](https://facts.net/history/historical-events/june-24th-all-facts-events-that-happened-today-in-history/)\n On  June  24th , significant events like the start of the War of 1812 and the birth of famous personalities like \nLionel Messi and Mindy Kaling shaped history. June  24th witnessed the birth of legends, scientific breakthroughs, \nand cultural milestones, making it a day filled with diverse and impactful historical events .\n\n|On This Day - What Happened on June 24 | Britannica](https://www.britannica.com/on-this-day/June-24)\n On This Day In History - June 24: anniversaries, birthdays, major events , and time capsules. This day's facts in \nthe arts, politics, and sciences.\n\n|What Happened on June 24 - HISTORY](https://www.history.com/this-day-in-history/June-24)\n On  June 24, 1973, an arson fire at the UpStairs Lounge, a popular gathering spot for New Orleans' LGBT community \nin the French Quarter, results in 32 deaths and at least 15 injuries.\n\n|June 24: Facts & Historical Events On This Day - The Fact Site](https://www.thefactsite.com/day/june-24/)\n June  24th is the day we officially celebrate Fairy Day and Pralines Day. Today marks day 175 of the year, and we \nhave 190 days remaining until the end of the year. You're about to unearth some incredible historical events that \nall happened throughout history on June 24, including one of the longest sports matches of all time and some major \n...\n\n|This Day in History on June 24th - History and \nHeadlines](https://www.historyandheadlines.com/this-day-in-history-on-june-24th/)\nThis article presents a chronological list of notable events that happened on June  24th . For each date below, \nplease click on the date to be taken to an article covering that date's event . Digging Deeper. On June 24, 109 AD,\nRoman Emperor Trajan opened the aqueduct known as Aqua Traiana, bringing water to Rome from Lake Bracciano 25 miles\naway.\n\n|Events on June 24 - Key Moments Throughout History - Take Me Back To](https://takemeback.to/events/date/June/24)\nHistorical Events  on  June 24: Significant Moments in History. Throughout history, certain dates stand out for the\nremarkable events that unfolded on them. From turning points in politics to cultural breakthroughs, these days \noften carry significance that echoes through time. Explore the most important events that occurred on June 24, \nrevealing ...\n\n|June 24 Events in History - Have Fun With History](https://www.havefunwithhistory.com/june-24/)\nThis article explores significant historical events that occurred on June  24th , tracing pivotal moments from \nmedieval battles to modern achievements. Starting with the Battle of Bannockburn in 1314, which shaped Scottish \nindependence, the narrative moves through milestones in exploration, arts, and technology, concluding with a \nrecord-breaking Wimbledon match in 2010. Each entry examines the ...\n\n|Historical Events on June 24, Facts & Special Events On This Day \n...](https://www.calendarz.com/on-this-day/events/june/24)\nHistorical events for June 24. See what famous , interesting and special events & facts happened throughout history\non June 24 related to American Revolutionary War, Serbia, World War II, Apartheid, Thailand, and many more.\n\n|June 24 Holidays (2025/2026), Historical Events, Famous Birthdays](https://www.holidayscalendar.com/day/june-24/)\nHistorical Events  on  June 24. 1340: King Edward III and his fleet almost completely destroys the French fleet at \nthe Battle of Sluys during the 100 Years War. 1374: In Aachen, Germany, cases of St. John's Dance mysteriously \nerupt in the streets of the city. 1497: At Newfoundland, John Cabot lands. It's the first European landing at the \narea since the Vikings landed there previously.\n\n\n\n[Step 2: Duration 2.75 seconds]\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ Calling tool: 'final_answer' with arguments: {'answer': \"Several historical events occurred on June 24th,       │\n│ including:\\n1. In 1374, cases of St. John's Dance mysteriously erupted in Aachen, Germany.\\n2. In 1497, John    │\n│ Cabot landed in Newfoundland, marking the first European landing at the site since the Vikings.\\n3. In 1340,    │\n│ King Edward III's fleet defeated the French fleet at the Battle of Sluys during the 100 Years War.\\n4. On June  │\n│ 24, 1973, an arson fire at the UpStairs Lounge in New Orleans' LGBT community resulted in 32 deaths.\"}          │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nFinal answer: Several historical events occurred on June 24th, including:\n1. In 1374, cases of St. John's Dance mysteriously erupted in Aachen, Germany.\n2. In 1497, John Cabot landed in Newfoundland, marking the first European landing at the site since the Vikings.\n3. In 1340, King Edward III's fleet defeated the French fleet at the Battle of Sluys during the 100 Years War.\n4. On June 24, 1973, an arson fire at the UpStairs Lounge in New Orleans' LGBT community resulted in 32 deaths.\n\n\n\n[Step 3: Duration 3.53 seconds]\n\n\n\n\nfrom IPython.display import display, Markdown\ndisplay(Markdown(answer))\n\nSeveral historical events occurred on June 24th, including: 1. In 1374, cases of St. John’s Dance mysteriously erupted in Aachen, Germany. 2. In 1497, John Cabot landed in Newfoundland, marking the first European landing at the site since the Vikings. 3. In 1340, King Edward III’s fleet defeated the French fleet at the Battle of Sluys during the 100 Years War. 4. On June 24, 1973, an arson fire at the UpStairs Lounge in New Orleans’ LGBT community resulted in 32 deaths.\n\n\n\n\nExample: Web Information Extraction\nIn the next example, we will use the Web View tool to extract information from a Web page.\n\nagent = Agent(llm)\nagent.add_webview_tool()\nanswer = agent.run(\"What is the highest level of education of the person listed on this page: https://arun.maiya.net?\")\n\n╭──────────────────────────────────────────────────── New run ────────────────────────────────────────────────────╮\n│                                                                                                                 │\n│ What is the highest level of education of the person listed on this page: https://arun.maiya.net?               │\n│                                                                                                                 │\n╰─ AgentModel - None ─────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ Calling tool: 'visit_webpage' with arguments: {'url': 'https://arun.maiya.net'}                                 │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nObservations: Arun S. Maiya\n\nArun S. Maiya   \n    \narun |at] maiya |dot] net  \n|CV](asmcv.pdf) | |LinkedIn](https://www.linkedin.com/in/arun-maiya/) | |GitHub](https://github.com/amaiya)\n\n!|](asmpic.jpg)\n\nBasics\n======\n\nI am a computer scientist at the |Institute for Defense Analyses (IDA)](http://www.ida.org), a federally-funded \n|think tank](http://en.wikipedia.org/wiki/List_of_federally_funded_research_and_development_centers) in the \nWashington D.C. metro area. My research broadly focuses on the study of computational methods to *extract meaning \nfrom raw data* and includes the areas of natural language processing, machine learning, data mining, computer \nvision, and network science (e.g., social network analysis). I like building tools to make machine learning easier \nto apply in new ways and new areas. Through my work, I have contributed to national-level strategic-planning \nactivities and R&D roadmaps. I completed a Ph.D. in Computer Science at the |Laboratory for Computational \nPopulation Biology](http://compbio.cs.uic.edu/), which is within the |Department of Computer \nScience](http://www.cs.uic.edu/) at the\n|University of Illinois at Chicago (UIC)](http://www.uic.edu/). My CV is |here](http://arun.maiya.net/asmcv.pdf).\n\nSoftware\n========\n\n* **|ktrain](https://github.com/amaiya/ktrain)** is a Python library that makes deep learning and AI more \naccessible and easier to apply. With support for many different data types including text, images, and graphs, \nktrain has been used for a wide range of use cases in industry, government, and academia. Examples include analyses\nfor the U.S. Economic Census, financial crime analytics at Big 4 accounting firms, intelligence analyses, and \n|CoronaCentral.ai](https://coronacentral.ai), a machine-learning-enhanced search engine for coronavirus \npublications at Stanford University.\n* **|CausalNLP](https://github.com/amaiya/causalnlp)** is a practical toolkit for causal inference with text as \ntreatment, outcome, or \"controlled-for\" variable.\n* **|IDATA](https://arxiv.org/abs/1308.2359)** is a suite of software capabilities designed to facilitate search, \nexploration, and analyses of very large document sets using state-of-the-art machine learning, NLP, and information\nretrieval. It has been used for a variety of different application in the |DoD](https://www.defense.gov) including \ncyber damage assessments, biosurveillance, and policy analyses.\n* **|OnPrem.LLM](https://github.com/amaiya/onprem)** is a simple Python package for generative AI that makes it \neasier to run large language models (LLMs) on your own machine using non-public data.\n\nPublications\n============\n\n|**ktrain: A Low-Code Library for Augmented Machine \nLearning**](https://www.jmlr.org/papers/volume23/21-1124/21-1124.pdf)  \nA.S. Maiya  \n *Journal of Machine Learning Research (JMLR).*  May 2022.\n\n|**CausalNLP: A Practical Toolkit for Causal Inference with Text**](https://arxiv.org/abs/2106.08043)   \nA.S. Maiya  \n *arXiv preprint arXiv:2106.08043 .*  Jun 2021. |arXiv only]\n\n|**A Framework for Comparing Groups of Documents**](https://arxiv.org/abs/1508.05902)  \nA.S. Maiya  \n *Proc. 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP '15).*  Lisbon, Portugal. Sep \n2015.\n\n|**Mining Measured Information from Text**](https://arxiv.org/abs/1505.01072)  \nA.S. Maiya, D. Visser, and A. Wan  \n *Proc. 38th Annual ACM SIGIR Conference (SIGIR '15).* Santiago, Chile. Aug 2015.\n\n|**Topic Similarity Networks: Visual Analytics for Large Document Sets**](https://arxiv.org/abs/1409.7591)  \nA.S. Maiya and R.M. Rolfe  \n *Proc. 2014 IEEE International Conference on Big Data (IEEE BigData '14).* Washington, D.C., Oct 2014.\n\n|**Exploratory Analysis of Highly Heterogeneous Document Collections**](https://arxiv.org/abs/1308.2359)  \nA.S. Maiya, J.P. Thompson, F. Loaiza-Lemos, and R.M. Rolfe  \n *Proc. 19th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '13).* Chicago, IL, Aug 2013.\n\n|**Expansion and Decentralized Search in Complex \nNetworks**](http://link.springer.com/article/10.1007/s10115-012-0596-4)  \nA.S. Maiya and T.Y. Berger-Wolf  \n *Journal of Knowledge and Information Systems.* First published online January 2013.\n\n|**Supervised Learning in the Wild: Text Classification for Critical \nTechnologies**](https://ieeexplore.ieee.org/document/6415660)  \nA.S. Maiya, F. Loaiza-Lemos, and R.M. Rolfe  \n *Proc. IEEE Military Communications Conference (MILCOM '12).* Orlando, FL, Oct 2012.\n\n|**Benefits of Bias: Towards Better Characterization of Network Sampling**](https://arxiv.org/abs/1109.3911)  \nA.S. Maiya and T.Y. Berger-Wolf  \n *Proc. 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '11).* San Diego, CA, Aug 2011.\n\n|**Aggression, Grooming, and Group-level Cooperation in White-faced Capuchins: Insights from Social \nNetworks**](http://onlinelibrary.wiley.com/doi/10.1002/ajp.20959/abstract)  \nM.C. Crofoot, D.I. Rubenstein, A.S. Maiya, and T.Y. Berger-Wolf  \n *American Journal of Primatology.* First published online May 2011.\n\n|**Sampling and Inference in Complex Networks**](https://dl.acm.org/doi/abs/10.5555/2395432)  \nA.S. Maiya  \n *Ph.D. Dissertation, University of Illinois at Chicago (UIC).* Chicago, IL, Apr 2011.\n\n|**Expansion and Search in Networks**](https://arxiv.org/abs/1009.4383)  \nA.S. Maiya and T.Y. Berger-Wolf  \n *Proc. 19th ACM Intl. Conference on Information and Knowledge Management (CIKM '10).* Toronto, Canada, Oct 2010.\n\n|**Online Sampling of High Centrality Individuals in Social \nNetworks**](https://link.springer.com/chapter/10.1007/978-3-642-13657-3_12)  \nA.S. Maiya and T.Y. Berger-Wolf  \n *Proc. 14th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD '10).* Hyderabad, India, Jun \n2010.\n\n|**Sampling Community Structure**](https://dl.acm.org/doi/10.1145/1772690.1772762)  \nA.S. Maiya and T.Y. Berger-Wolf  \n *Proc. 19th ACM Intl. Conference on the World Wide Web (WWW '10).* Raleigh, NC, Apr 2010.\n\n|**Inferring the Maximum Likelihood Hierarchy in Social Networks**](https://ieeexplore.ieee.org/document/5284124)  \nA.S. Maiya and T.Y. Berger-Wolf  \n *Proc. 12th IEEE Intl. Conference on Computational Science and Engineering (CSE '09).* Vancouver, Canada, Aug \n2009.\n\n|**The Impact of Structural Changes on Predictions of Diffusion in \nNetworks**](https://www.computer.org/csdl/proceedings-article/icdmw/2008/3503a939/12OmNBU1jIK)  \nM. Lahiri, A.S. Maiya, R. Sulo, Habiba and T.Y. Berger-Wolf  \n*ICDM '08 Workshop on Analysis of Dynamic Networks*. Pisa, Italy, Dec 2008.\n\nHonors and Awards\n=================\n\n**Goodpaster Award for Excellence in Research**, Institute for Defense Analyses, 2021 ||News \nRelease](https://www.ida.org/research-and-publications/publications/all/a/ar/arun-maiya-receives-2021-goodpaster-aw\nard-for-excellence-in-research)]\n\n&gt; *This prize is named for Gen. Andrew J. Goodpaster (USA, retired) and is awarded to an individual demonstrating \n\"research excellence, exceptional analytic achievement and intellectual leadership.\"*\n\n**Welch Award for Best External Research**, Institute for Defense Analyses, 2016\n\n&gt; *Named in honor of General Larry D. Welch (USAF, ret.), this award \"honors individuals whose external research \npublications exemplify General Welch's high standards of analytic excellence and relevance.\"*\n\n**AFEI Award for Excellence in Enterprise Information**, NDIA (formerly Association for Enterprise Information), \n2015\n\n&gt; *This award is to \"recognize and reward the contributions and achievements of project teams that exemplify \nexcellence in achieving integrated enterprises. Winning teams are models of the best applications of technology and\nleadership to improve enterprise performance.\"*\n\n&lt;script\n\n\n\n[Step 1: Duration 1.50 seconds]\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ Calling tool: 'final_answer' with arguments: {'answer': 'Ph.D. in Computer Science'}                            │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nFinal answer: Ph.D. in Computer Science\n\n\n\n[Step 2: Duration 1.43 seconds]\n\n\n\n\nprint(answer)\n\nPh.D. in Computer Science\n\n\n\n\nExample: Agentic RAG\nYou can also give the agent access to a vector store containing our documents. In this example, we will ingest a document about Generative AI into a vector store and provide the store to the agent as a tool.\n\nfrom onprem.ingest.stores import DenseStore\n\nstore = DenseStore.create('/tmp/myvectordb')\nstore.ingest('tests/sample_data/docx_example/')\n\nCreating new vectorstore at /tmp/myvectordb\nLoading documents from tests/sample_data/docx_example/\n\n\nLoading new documents: 100%|██████████████████████| 1/1 [00:04&lt;00:00,  4.05s/it]\nProcessing and chunking 1 new documents: 100%|█████████████████████████████████████| 1/1 [00:00&lt;00:00, 2184.53it/s]\n\n\nSplit into 17 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\nCreating embeddings. May take some minutes...\n\n\n100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  2.67it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n\n\n\nSo far, we have been using the default tool-calling agent, which relies on the LLM to generate precise JSON structures that specify tool names and arguments required to complete a task.\nBy constrast, the code-agent generates and runs code to solve a task. We will use the code-agent in this final example.\n\nagent = Agent(llm, agent_type='code')\nagent.add_vectorstore_tool(name='genai_search', \n                           description='Searches a database of information on generative AI.',\n                           store=store)\n\nanswer = agent.run(\"Summarize some generative AI use cases in Markdown format. You may need to run at least three queries of the database.\")\n\n╭──────────────────────────────────────────────────── New run ────────────────────────────────────────────────────╮\n│                                                                                                                 │\n│ Summarize some generative AI use cases in Markdown format. You may need to run at least three queries of the    │\n│ database.                                                                                                       │\n│                                                                                                                 │\n╰─ AgentModel - None ─────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n ─ Executing parsed code: ──────────────────────────────────────────────────────────────────────────────────────── \n  use_cases_general = genai_search(query=\"generative AI use cases\")                                                \n  print(use_cases_general)                                                                                         \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n\nExecution logs:\n[{'id': '901846fb-fc97-42ef-810a-cedc8abfbcb8', 'score': 0.6182159185409546, 'text': '3. What is generative \nAI?\\n\\nEven though AI is not new, you have probably been hearing a lot about easy-to-use, publicly available \nGenerative AI tools like ChatGPT and Google Gemini.\\u202f \\n\\nBut what is generative AI?\\u202f\\u202f \\n\\nIt is a \ntype of AI technology that automatically generates (or creates) content in response to prompts given to it by \nusers.\\u202f \\n\\nThese tools can generate text, images, music, video, code and other formats.'}, {'id': \n'0b660b75-3bf3-47ab-a4bb-0f52039ec442', 'score': 0.5850328207015991, 'text': 'An introduction to generative \nAI\\n\\n1. Introduction\\n\\nThis activity will give a brief overview of what Generative AI is and how it is being \nused.\\n\\nUsing this tutorial\\n\\nUse the back and next buttons at the bottom right of the screen to navigate through\nthe tutorial. Alternatively, use the contents button to jump to a specific page.\\n\\nYou will need to allow \napproximately\\xa05\\xa0minutes\\xa0to complete the tutorial.\\n\\nLearning outcomes\\n\\nAfter completing this tutorial \nyou will be able to:'}, {'id': '57831af5-b767-4565-ba01-8845d8e6b261', 'score': 0.568540632724762, 'text': \n'Question 5 Feedback for Option 3\\n\\nTrue. Generative AI tools rely on pattern recognition and do not truly \nunderstand the content, which can lead to plausible but misleading or incorrect outputs.\\n\\nQuestion 5 Feedback for\nOption 4\\n\\nFalse. Each response is unique; generative AI tools do not provide the same response every \ntime.\\n\\nBeing digital Copyright © 2024 The Open University'}, {'id': '3ab09f45-f5d2-468e-ae8e-710c116f51b7', \n'score': 0.5618582367897034, 'text': 'Recognise generative AI in daily life: Identify common generative AI \napplications like chatbots, facial recognition, streaming services, and digital assistants.\\n\\nUnderstand \ngenerative AI: Explain generative AI and its ability to create diverse content such as text, images, music, video, \nand code.\\n\\nDescribe LLMs: Understand how\\xa0Large Language Models (LLMs) work.\\n\\n2. Examples of AI in everyday \nlife'}, {'id': 'ec272b15-9ff9-448b-a39b-3b89f0d6f524', 'score': 0.5566920042037964, 'text': '5. Quiz\\n\\nQuestion 1 \nof 5\\n\\nWhat is generative AI?\\n\\nAI that only processes data\\n\\nAI that creates content based on user \nprompts\\n\\nAI that cannot update its data\\n\\nAI that performs physical tasks\\n\\nQuestion 1 Feedback for Option \n1\\n\\nFalse. Typing on a keyboard is a manual activity that does not involve AI.\\n\\nQuestion 1 Feedback for Option \n2\\n\\nFalse. Watching a sunset is a natural experience, not related to generative AI.\\n\\nQuestion 1 Feedback for \nOption 3'}]\n\nOut: None\n\n\n\n[Step 1: Duration 2.20 seconds]\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n ─ Executing parsed code: ──────────────────────────────────────────────────────────────────────────────────────── \n  use_cases_content_creation = genai_search(query=\"generative AI content creation use cases\")                      \n  print(use_cases_content_creation)                                                                                \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n\nExecution logs:\n[{'id': '901846fb-fc97-42ef-810a-cedc8abfbcb8', 'score': 0.6875638961791992, 'text': '3. What is generative \nAI?\\n\\nEven though AI is not new, you have probably been hearing a lot about easy-to-use, publicly available \nGenerative AI tools like ChatGPT and Google Gemini.\\u202f \\n\\nBut what is generative AI?\\u202f\\u202f \\n\\nIt is a \ntype of AI technology that automatically generates (or creates) content in response to prompts given to it by \nusers.\\u202f \\n\\nThese tools can generate text, images, music, video, code and other formats.'}, {'id': \n'3d776586-b95b-4cad-8915-4d65926a5fe4', 'score': 0.6328049302101135, 'text': 'Question 2 Feedback for Option \n1\\n\\nFalse. This describes data processing AI, not generative AI.\\n\\nQuestion 2 Feedback for Option 2 \\n\\nTrue. \nGenerative AI creates content like text, images, music, etc., in response to user prompts.\\n\\nQuestion 2 Feedback \nfor Option 3\\n\\nFalse. Generative AI can update its data with new information.\\n\\nQuestion 2 Feedback for Option 4 \n\\n\\nFalse. Generative AI focuses on content creation, not performing physical tasks.\\n\\nQuestion 3 of 5'}, {'id': \n'cd369b7e-383b-42e2-bc5e-f514d260d7dd', 'score': 0.6124569177627563, 'text': 'Question 3 of 5\\n\\nWhich of the \nfollowing is a text generative AI tool?\\n\\nSpotify\\n\\nGoogle Maps\\n\\nDall-E2\\n\\nChatGPT\\n\\nQuestion 3 Feedback for \nOption 1\\n\\nFalse. Spotify uses AI for music recommendations, not text generation.\\n\\nQuestion 3 Feedback for \nOption 2\\n\\nFalse. Google Maps uses AI for navigation and travel information, not text generation.\\n\\n\\n\\nQuestion \n3 Feedback for Option 3\\n\\nFalse. DALL-E2 generates images and art, not text.\\n\\nQuestion 3 Feedback for Option \n4'}, {'id': '0b660b75-3bf3-47ab-a4bb-0f52039ec442', 'score': 0.582166314125061, 'text': 'An introduction to \ngenerative AI\\n\\n1. Introduction\\n\\nThis activity will give a brief overview of what Generative AI is and how it is\nbeing used.\\n\\nUsing this tutorial\\n\\nUse the back and next buttons at the bottom right of the screen to navigate \nthrough the tutorial. Alternatively, use the contents button to jump to a specific page.\\n\\nYou will need to allow \napproximately\\xa05\\xa0minutes\\xa0to complete the tutorial.\\n\\nLearning outcomes\\n\\nAfter completing this tutorial \nyou will be able to:'}, {'id': 'ec272b15-9ff9-448b-a39b-3b89f0d6f524', 'score': 0.5623699426651001, 'text': '5. \nQuiz\\n\\nQuestion 1 of 5\\n\\nWhat is generative AI?\\n\\nAI that only processes data\\n\\nAI that creates content based \non user prompts\\n\\nAI that cannot update its data\\n\\nAI that performs physical tasks\\n\\nQuestion 1 Feedback for \nOption 1\\n\\nFalse. Typing on a keyboard is a manual activity that does not involve AI.\\n\\nQuestion 1 Feedback for \nOption 2\\n\\nFalse. Watching a sunset is a natural experience, not related to generative AI.\\n\\nQuestion 1 Feedback \nfor Option 3'}]\n\nOut: None\n\n\n\n[Step 2: Duration 2.12 seconds]\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 3 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n ─ Executing parsed code: ──────────────────────────────────────────────────────────────────────────────────────── \n  use_cases_applications = genai_search(query=\"applications of generative AI in various domains\")                  \n  print(use_cases_applications)                                                                                    \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n\nExecution logs:\n[{'id': '0b660b75-3bf3-47ab-a4bb-0f52039ec442', 'score': 0.5789191722869873, 'text': 'An introduction to generative\nAI\\n\\n1. Introduction\\n\\nThis activity will give a brief overview of what Generative AI is and how it is being \nused.\\n\\nUsing this tutorial\\n\\nUse the back and next buttons at the bottom right of the screen to navigate through\nthe tutorial. Alternatively, use the contents button to jump to a specific page.\\n\\nYou will need to allow \napproximately\\xa05\\xa0minutes\\xa0to complete the tutorial.\\n\\nLearning outcomes\\n\\nAfter completing this tutorial \nyou will be able to:'}, {'id': '901846fb-fc97-42ef-810a-cedc8abfbcb8', 'score': 0.5656728744506836, 'text': '3. \nWhat is generative AI?\\n\\nEven though AI is not new, you have probably been hearing a lot about easy-to-use, \npublicly available Generative AI tools like ChatGPT and Google Gemini.\\u202f \\n\\nBut what is generative \nAI?\\u202f\\u202f \\n\\nIt is a type of AI technology that automatically generates (or creates) content in response to \nprompts given to it by users.\\u202f \\n\\nThese tools can generate text, images, music, video, code and other \nformats.'}, {'id': '3ab09f45-f5d2-468e-ae8e-710c116f51b7', 'score': 0.55094313621521, 'text': 'Recognise generative\nAI in daily life: Identify common generative AI applications like chatbots, facial recognition, streaming services,\nand digital assistants.\\n\\nUnderstand generative AI: Explain generative AI and its ability to create diverse \ncontent such as text, images, music, video, and code.\\n\\nDescribe LLMs: Understand how\\xa0Large Language Models \n(LLMs) work.\\n\\n2. Examples of AI in everyday life'}, {'id': '14de7734-74a7-43c7-aa72-75a42655e545', 'score': \n0.5244572162628174, 'text': 'surprisingly knowledgeable, or with an image / computing code. Although outputs \nproduced by Generative AI tools look plausible, they are often misleading, made up, or may be entirely \nwrong.\\n\\nEach response is unique, you will probably never get the exact same reply twice.\\n\\nResponses are \nconversational. Once you have asked a question you can just carry on the conversation by adding new questions or \nprompts, rather than having to redo your search, as you would in a search engine like Google.'}, {'id': \n'2751a020-1728-46d2-ae5c-0cfac3ee3d88', 'score': 0.5166183710098267, 'text': 'Online shopping – personalised \nrecommendations and adverts.\\n\\nDigital assistants, e.g.\\u202fAmazon Alexa, Google Assistant, Siri.\\n\\nSending an \nemail – spell check and spam filters.\\n\\n\\n\\nTravel and navigation, e.g. Google Maps.\\n\\nWriting assistants, e.g.  \nGrammarly.\\n\\nHealth and fitness apps, e.g. Fitbit.\\n\\nSocial media feeds – AI controls the feeds that you get to \nsee while browsing through social media platforms (e.g., Facebook, X, Instagram) or the notifications you \nreceive.\\n\\n3. What is generative AI?'}]\n\nOut: None\n\n\n\n[Step 3: Duration 2.56 seconds]\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 4 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n ─ Executing parsed code: ──────────────────────────────────────────────────────────────────────────────────────── \n  markdown_summary = \"\"\"                                                                                           \n  # Generative AI Use Cases                                                                                        \n                                                                                                                   \n  Generative AI refers to a category of artificial intelligence technologies that can create content in response   \n  to user prompts. Here are some key applications of generative AI:                                                \n                                                                                                                   \n  ## 1. Content Creation                                                                                           \n  - **Text Generation**: Tools like ChatGPT generate coherent and relevant text based on prompts.                  \n  - **Image Generation**: Models such as DALL-E create unique images from text descriptions.                       \n  - **Music and Video Creation**: Generative models can compose music or create video clips based on user inputs.  \n                                                                                                                   \n  ## 2. Business Applications                                                                                      \n  - **Personalized Recommendations**: In online shopping, generative AI analyzes user behavior to recommend        \n  products tailored to individual preferences.                                                                     \n  - **Chatbots and Digital Assistants**: Applications like Amazon Alexa and Google Assistant help users by         \n  providing information and performing tasks based on voice commands.                                              \n                                                                                                                   \n  ## 3. Health and Fitness                                                                                         \n  - **Health Monitoring**: Apps like Fitbit use generative AI to analyze data and provide health recommendations.  \n  - **Telemedicine**: AI systems assist healthcare professionals by generating patient reports and insights based  \n  on input data.                                                                                                   \n                                                                                                                   \n  ## 4. Social Media                                                                                               \n  - **Content Moderation**: Generative AI helps filter out harmful or inappropriate content from social media      \n  feeds.                                                                                                           \n  - **Algorithm-Driven Feeds**: AI systems personalize content delivery on platforms like Facebook and Instagram   \n  based on user interaction patterns.                                                                              \n                                                                                                                   \n  These use cases illustrate the versatility of generative AI across various sectors, enhancing both productivity  \n  and user experience.                                                                                             \n  \"\"\"                                                                                                              \n                                                                                                                   \n  print(markdown_summary)                                                                                          \n  final_answer(markdown_summary)                                                                                   \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n\nExecution logs:\n\n# Generative AI Use Cases\n\nGenerative AI refers to a category of artificial intelligence technologies that can create content in response to \nuser prompts. Here are some key applications of generative AI:\n\n## 1. Content Creation\n- **Text Generation**: Tools like ChatGPT generate coherent and relevant text based on prompts.\n- **Image Generation**: Models such as DALL-E create unique images from text descriptions.\n- **Music and Video Creation**: Generative models can compose music or create video clips based on user inputs.\n\n## 2. Business Applications\n- **Personalized Recommendations**: In online shopping, generative AI analyzes user behavior to recommend products \ntailored to individual preferences.\n- **Chatbots and Digital Assistants**: Applications like Amazon Alexa and Google Assistant help users by providing \ninformation and performing tasks based on voice commands.\n\n## 3. Health and Fitness\n- **Health Monitoring**: Apps like Fitbit use generative AI to analyze data and provide health recommendations.\n- **Telemedicine**: AI systems assist healthcare professionals by generating patient reports and insights based on \ninput data.\n\n## 4. Social Media\n- **Content Moderation**: Generative AI helps filter out harmful or inappropriate content from social media feeds.\n- **Algorithm-Driven Feeds**: AI systems personalize content delivery on platforms like Facebook and Instagram \nbased on user interaction patterns.\n\nThese use cases illustrate the versatility of generative AI across various sectors, enhancing both productivity and\nuser experience.\n\n\nOut - Final answer: \n# Generative AI Use Cases\n\nGenerative AI refers to a category of artificial intelligence technologies that can create content in response to \nuser prompts. Here are some key applications of generative AI:\n\n## 1. Content Creation\n- **Text Generation**: Tools like ChatGPT generate coherent and relevant text based on prompts.\n- **Image Generation**: Models such as DALL-E create unique images from text descriptions.\n- **Music and Video Creation**: Generative models can compose music or create video clips based on user inputs.\n\n## 2. Business Applications\n- **Personalized Recommendations**: In online shopping, generative AI analyzes user behavior to recommend products \ntailored to individual preferences.\n- **Chatbots and Digital Assistants**: Applications like Amazon Alexa and Google Assistant help users by providing \ninformation and performing tasks based on voice commands.\n\n## 3. Health and Fitness\n- **Health Monitoring**: Apps like Fitbit use generative AI to analyze data and provide health recommendations.\n- **Telemedicine**: AI systems assist healthcare professionals by generating patient reports and insights based on \ninput data.\n\n## 4. Social Media\n- **Content Moderation**: Generative AI helps filter out harmful or inappropriate content from social media feeds.\n- **Algorithm-Driven Feeds**: AI systems personalize content delivery on platforms like Facebook and Instagram \nbased on user interaction patterns.\n\nThese use cases illustrate the versatility of generative AI across various sectors, enhancing both productivity and\nuser experience.\n\n\n\n\n[Step 4: Duration 7.26 seconds]\n\n\n\n\nfrom IPython.display import display, Markdown\ndisplay(Markdown(answer))\n\nGenerative AI Use Cases\nGenerative AI refers to a category of artificial intelligence technologies that can create content in response to user prompts. Here are some key applications of generative AI:\n\n1. Content Creation\n\nText Generation: Tools like ChatGPT generate coherent and relevant text based on prompts.\nImage Generation: Models such as DALL-E create unique images from text descriptions.\nMusic and Video Creation: Generative models can compose music or create video clips based on user inputs.\n\n\n\n2. Business Applications\n\nPersonalized Recommendations: In online shopping, generative AI analyzes user behavior to recommend products tailored to individual preferences.\nChatbots and Digital Assistants: Applications like Amazon Alexa and Google Assistant help users by providing information and performing tasks based on voice commands.\n\n\n\n3. Health and Fitness\n\nHealth Monitoring: Apps like Fitbit use generative AI to analyze data and provide health recommendations.\nTelemedicine: AI systems assist healthcare professionals by generating patient reports and insights based on input data.\n\n\n\n4. Social Media\n\nContent Moderation: Generative AI helps filter out harmful or inappropriate content from social media feeds.\nAlgorithm-Driven Feeds: AI systems personalize content delivery on platforms like Facebook and Instagram based on user interaction patterns.\n\nThese use cases illustrate the versatility of generative AI across various sectors, enhancing both productivity and user experience.",
    "crumbs": [
      "Examples",
      "Agent-Based Task Execution"
    ]
  },
  {
    "objectID": "examples_agent.html#content-creation",
    "href": "examples_agent.html#content-creation",
    "title": "Agent-Based Task Execution",
    "section": "1. Content Creation",
    "text": "1. Content Creation\n\nText Generation: Tools like ChatGPT generate coherent and relevant text based on prompts.\nImage Generation: Models such as DALL-E create unique images from text descriptions.\nMusic and Video Creation: Generative models can compose music or create video clips based on user inputs.",
    "crumbs": [
      "Examples",
      "Agent-Based Task Execution"
    ]
  },
  {
    "objectID": "examples_agent.html#business-applications",
    "href": "examples_agent.html#business-applications",
    "title": "Agent-Based Task Execution",
    "section": "2. Business Applications",
    "text": "2. Business Applications\n\nPersonalized Recommendations: In online shopping, generative AI analyzes user behavior to recommend products tailored to individual preferences.\nChatbots and Digital Assistants: Applications like Amazon Alexa and Google Assistant help users by providing information and performing tasks based on voice commands.",
    "crumbs": [
      "Examples",
      "Agent-Based Task Execution"
    ]
  },
  {
    "objectID": "examples_agent.html#health-and-fitness",
    "href": "examples_agent.html#health-and-fitness",
    "title": "Agent-Based Task Execution",
    "section": "3. Health and Fitness",
    "text": "3. Health and Fitness\n\nHealth Monitoring: Apps like Fitbit use generative AI to analyze data and provide health recommendations.\nTelemedicine: AI systems assist healthcare professionals by generating patient reports and insights based on input data.",
    "crumbs": [
      "Examples",
      "Agent-Based Task Execution"
    ]
  },
  {
    "objectID": "examples_agent.html#social-media",
    "href": "examples_agent.html#social-media",
    "title": "Agent-Based Task Execution",
    "section": "4. Social Media",
    "text": "4. Social Media\n\nContent Moderation: Generative AI helps filter out harmful or inappropriate content from social media feeds.\nAlgorithm-Driven Feeds: AI systems personalize content delivery on platforms like Facebook and Instagram based on user interaction patterns.\n\nThese use cases illustrate the versatility of generative AI across various sectors, enhancing both productivity and user experience.",
    "crumbs": [
      "Examples",
      "Agent-Based Task Execution"
    ]
  },
  {
    "objectID": "examples_agent.html#using-local-models-in-agentic-workflows",
    "href": "examples_agent.html#using-local-models-in-agentic-workflows",
    "title": "Agent-Based Task Execution",
    "section": "Using Local Models in Agentic Workflows",
    "text": "Using Local Models in Agentic Workflows\nSome local (on-premises) models are better-suited for agentic workflows than others. For instance, ollama/llama3.2 is a 3B-parameter model and not recommended. In this example, we will use ollama/llama3.1:8b and repeat one of the earlier examples from above. The example below assumes you have already installed Ollama and pulled the Llama3.1:8b model with ollama pull llama3.1:8b.\nImportant Note: If using a local model, it is important to set the context window to a higher value (e.g., 8192) to support agentic workflows.\n\nFor the llama.cpp backend, supply n_ctx=8192 as a parameter to LLM.\nFor the Ollama backend, supply num_ctx=8192 as a parameter to LLM.\nFor the Hugging Face transformers backend (e.g., LLM(model_id='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', device='cuda'), context window is set automatically, so no extra parameters are necessary.\n\n\nfrom onprem import LLM\nfrom onprem.pipelines import Agent\nllm = LLM(\"ollama/llama3.1:8b\", mute_stream=True, num_ctx=8182) \n\nagent = Agent(llm)\nagent.add_webview_tool()\nanswer = agent.run(\"What is the highest level of education of the person listed on this page: https://arun.maiya.net?\")\n\n╭──────────────────────────────────────────────────── New run ────────────────────────────────────────────────────╮\n│                                                                                                                 │\n│ What is the highest level of education of the person listed on this page: https://arun.maiya.net?               │\n│                                                                                                                 │\n╰─ AgentModel - None ─────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ Calling tool: 'visit_webpage' with arguments: {'url': 'https://arun.maiya.net'}                                 │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nObservations: Arun S. Maiya\n\nArun S. Maiya   \n\n    \n\narun |at] maiya |dot] net  \n\n|CV](asmcv.pdf) | |LinkedIn](https://www.linkedin.com/in/arun-maiya/) | |GitHub](https://github.com/amaiya)\n\n!|](asmpic.jpg)  \n\n \n\nBasics\n======\n\nI am a computer scientist at the |Institute for Defense Analyses (IDA)](http://www.ida.org), a federally-funded \n|think tank](http://en.wikipedia.org/wiki/List_of_federally_funded_research_and_development_centers) in the \nWashington D.C. metro area. My research broadly focuses on the study of computational methods to *extract meaning \nfrom raw data* and includes the areas of natural language processing, machine learning, data mining, computer \nvision, and network science (e.g., social network analysis). I like building tools to make machine learning easier \nto apply in new ways and new areas. Through my work, I have contributed to national-level strategic-planning \nactivities and R&D roadmaps. I completed a Ph.D. in Computer Science at the |Laboratory for Computational \nPopulation Biology](http://compbio.cs.uic.edu/), which is within the |Department of Computer \nScience](http://www.cs.uic.edu/) at the\n|University of Illinois at Chicago (UIC)](http://www.uic.edu/). My CV is |here](http://arun.maiya.net/asmcv.pdf).\n\nSoftware\n========\n\n* **|ktrain](https://github.com/amaiya/ktrain)** is a Python library that makes deep learning and AI more \naccessible and easier to apply. With support for many different data types including text, images, and graphs, \nktrain has been used for a wide range of use cases in industry, government, and academia. Examples include analyses\nfor the U.S. Economic Census, financial crime analytics at Big 4 accounting firms, intelligence analyses, and \n|CoronaCentral.ai](https://coronacentral.ai), a machine-learning-enhanced search engine for coronavirus \npublications at Stanford University.\n* **|CausalNLP](https://github.com/amaiya/causalnlp)** is a practical toolkit for causal inference with text as \ntreatment, outcome, or \"controlled-for\" variable.\n* **|IDATA](https://arxiv.org/abs/1308.2359)** is a suite of software capabilities designed to facilitate search, \nexploration, and analyses of very large document sets using state-of-the-art machine learning, NLP, and information\nretrieval. It has been used for a variety of different application in the |DoD](https://www.defense.gov) including \ncyber damage assessments, biosurveillance, and policy analyses.\n* **|OnPrem.LLM](https://github.com/amaiya/onprem)** is a simple Python package for generative AI that makes it \neasier to run large language models (LLMs) on your own machine using non-public data.\n\nPublications\n============\n\n|**ktrain: A Low-Code Library for Augmented Machine \nLearning**](https://www.jmlr.org/papers/volume23/21-1124/21-1124.pdf)  \n\nA.S. Maiya  \n\n *Journal of Machine Learning Research (JMLR).*  May 2022.\n\n|**CausalNLP: A Practical Toolkit for Causal Inference with Text**](https://arxiv.org/abs/2106.08043)   \n\nA.S. Maiya  \n\n *arXiv preprint arXiv:2106.08043 .*  Jun 2021. |arXiv only]\n\n|**A Framework for Comparing Groups of Documents**](https://arxiv.org/abs/1508.05902)  \n\nA.S. Maiya  \n\n *Proc. 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP '15).*  Lisbon, Portugal. Sep \n2015.\n\n|**Mining Measured Information from Text**](https://arxiv.org/abs/1505.01072)  \n\nA.S. Maiya, D. Visser, and A. Wan  \n\n *Proc. 38th Annual ACM SIGIR Conference (SIGIR '15).* Santiago, Chile. Aug 2015.\n\n|**Topic Similarity Networks: Visual Analytics for Large Document Sets**](https://arxiv.org/abs/1409.7591)  \n\nA.S. Maiya and R.M. Rolfe  \n\n *Proc. 2014 IEEE International Conference on Big Data (IEEE BigData '14).* Washington, D.C., Oct 2014.\n\n|**Exploratory Analysis of Highly Heterogeneous Document Collections**](https://arxiv.org/abs/1308.2359)  \n\nA.S. Maiya, J.P. Thompson, F. Loaiza-Lemos, and R.M. Rolfe  \n\n *Proc. 19th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '13).* Chicago, IL, Aug 2013.\n\n|**Expansion and Decentralized Search in Complex \nNetworks**](http://link.springer.com/article/10.1007/s10115-012-0596-4)  \n\nA.S. Maiya and T.Y. Berger-Wolf  \n\n *Journal of Knowledge and Information Systems.* First published online January 2013.\n\n|**Supervised Learning in the Wild: Text Classification for Critical \nTechnologies**](https://ieeexplore.ieee.org/document/6415660)  \n\nA.S. Maiya, F. Loaiza-Lemos, and R.M. Rolfe  \n\n *Proc. IEEE Military Communications Conference (MILCOM '12).* Orlando, FL, Oct 2012.\n\n|**Benefits of Bias: Towards Better Characterization of Network Sampling**](https://arxiv.org/abs/1109.3911)  \n\nA.S. Maiya and T.Y. Berger-Wolf  \n\n *Proc. 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '11).* San Diego, CA, Aug 2011.\n\n|**Aggression, Grooming, and Group-level Cooperation in White-faced Capuchins: Insights from Social \nNetworks**](http://onlinelibrary.wiley.com/doi/10.1002/ajp.20959/abstract)  \n\nM.C. Crofoot, D.I. Rubenstein, A.S. Maiya, and T.Y. Berger-Wolf  \n\n *American Journal of Primatology.* First published online May 2011.\n\n|**Sampling and Inference in Complex Networks**](https://dl.acm.org/doi/abs/10.5555/2395432)  \n\nA.S. Maiya  \n\n *Ph.D. Dissertation, University of Illinois at Chicago (UIC).* Chicago, IL, Apr 2011.\n\n|**Expansion and Search in Networks**](https://arxiv.org/abs/1009.4383)  \n\nA.S. Maiya and T.Y. Berger-Wolf  \n\n *Proc. 19th ACM Intl. Conference on Information and Knowledge Management (CIKM '10).* Toronto, Canada, Oct 2010.\n\n|**Online Sampling of High Centrality Individuals in Social \nNetworks**](https://link.springer.com/chapter/10.1007/978-3-642-13657-3_12)  \n\nA.S. Maiya and T.Y. Berger-Wolf  \n\n *Proc. 14th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD '10).* Hyderabad, India, Jun \n2010.\n\n|**Sampling Community Structure**](https://dl.acm.org/doi/10.1145/1772690.1772762)  \n\nA.S. Maiya and T.Y. Berger-Wolf  \n\n *Proc. 19th ACM Intl. Conference on the World Wide Web (WWW '10).* Raleigh, NC, Apr 2010.\n\n|**Inferring the Maximum Likelihood Hierarchy in Social Networks**](https://ieeexplore.ieee.org/document/5284124)  \n\nA.S. Maiya and T.Y. Berger-Wolf  \n\n *Proc. 12th IEEE Intl. Conference on Computational Science and Engineering (CSE '09).* Vancouver, Canada, Aug \n2009.\n\n|**The Impact of Structural Changes on Predictions of Diffusion in \nNetworks**](https://www.computer.org/csdl/proceedings-article/icdmw/2008/3503a939/12OmNBU1jIK)  \n\nM. Lahiri, A.S. Maiya, R. Sulo, Habiba and T.Y. Berger-Wolf  \n\n*ICDM '08 Workshop on Analysis of Dynamic Networks*. Pisa, Italy, Dec 2008.\n\nHonors and Awards\n=================\n\n**Goodpaster Award for Excellence in Research**, Institute for Defense Analyses, 2021 ||News \nRelease](https://www.ida.org/research-and-publications/publications/all/a/ar/arun-maiya-receives-2021-goodpaster-aw\nard-for-excellence-in-research)]\n\n&gt; *This prize is named for Gen. Andrew J. Goodpaster (USA, retired) and is awarded to an individual demonstrating \n\"research excellence, exceptional analytic achievement and intellectual leadership.\"*\n\n**Welch Award for Best External Research**, Institute for Defense Analyses, 2016\n\n&gt; *Named in honor of General Larry D. Welch (USAF, ret.), this award \"honors individuals whose external research \npublications exemplify General Welch's high standards of analytic excellence and relevance.\"*\n\n**AFEI Award for Excellence in Enterprise Information**, NDIA (formerly Association for Enterprise Information), \n2015\n\n&gt; *This award is to \"recognize and reward the contributions and achievements of project teams that exemplify \nexcellence in achieving integrated enterprises. Winning teams are models of the best applications of technology and\nleadership to improve enterprise performance.\"*\n\n&lt;script\n\n\n\n[Step 1: Duration 8.83 seconds]\n\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ Calling tool: 'final_answer' with arguments: {'answer': 'Ph.D.'}                                                │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\nFinal answer: Ph.D.\n\n\n\n[Step 2: Duration 10.20 seconds]\n\n\n\n\nprint(answer)\n\nPh.D.",
    "crumbs": [
      "Examples",
      "Agent-Based Task Execution"
    ]
  },
  {
    "objectID": "hf.train.hfonnx.html",
    "href": "hf.train.hfonnx.html",
    "title": "hf.train.hfonnx",
    "section": "",
    "text": "source\n\nPoolingOnnx\n\ndef PoolingOnnx(\n    path, device\n):\n\nExtends Pooling methods to name inputs to model, which is required to export to ONNX.\n\nsource\n\n\nHFOnnx\n\ndef HFOnnx(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nExports a Hugging Face Transformer model to ONNX."
  },
  {
    "objectID": "pipelines.agent.tools.html",
    "href": "pipelines.agent.tools.html",
    "title": "pipelines.agent.tools",
    "section": "",
    "text": "source\n\nfromdocs\n\ndef fromdocs(\n    target, config\n):\n\nCreates a tool from method documentation.\nArgs: target: target object or function config: tool configuration\nReturns: Tool\n\nsource\n\n\nFunctionTool\n\ndef FunctionTool(\n    config\n):\n\nCreates a FunctionTool. A FunctionTool takes descriptive configuration and injects it along with a target function into an LLM prompt.\n\nsource\n\n\ncreatetool\n\ndef createtool(\n    target, config:NoneType=None\n):\n\nCreates a new Tool.\nArgs: target: target object or function config: optional tool configuration\nReturns: Tool\n\nsource\n\n\nVectorStoreTool\n\ndef VectorStoreTool(\n    name:str, description:str, store:VectorStore\n):\n\nTool to execute an VectorStore search.",
    "crumbs": [
      "Source",
      "pipelines.agent.tools"
    ]
  },
  {
    "objectID": "webapp.html",
    "href": "webapp.html",
    "title": "A Built-In Web App",
    "section": "",
    "text": "OnPrem.LLM includes a built-in web app to easily access and use LLMs. After installing OnPrem.LLM, you can start it by running the following command at the command-line:\nThen, enter localhost:8000 in your Web browser to access the application:\nThe Web app is implemented with streamlit: pip install streamlit. If it is not already installed, the onprem command will ask you to install it. Here is more information on the onprem command:\nThe app requires a file called config.yml exists in the onprem_data/webapp folder in the user’s home directory. This file stores information used by the Web app such as the model to use. If one does not exist, then a default one will be created for you and is also shown below: or FALSE) show_manage: TRUE\nYou can edit the file based on your requirements. Variables in the llm section are automatically passed to the onprem.LLM constructor, which, in turn, passes extra **kwargs to llama-cpp-python or the transformers.pipeline. For instance, you can add a temperature variable in the llm section to adjust temperature of the model in the web app (e.g., lower values closer to 0.0 for more deterministic output and higher values for more creativity).\nThe default model is a 7B-parameter model called Zephyr-7B.\nIf you’d like the LLM to support longer answers than the default 512 tokens, you can add a max_tokens parameter to the llm section.\nIf using Ollama as the LLM engine, you can replace the default model_url entry with something like:\nYou can also set the model_url or model_id parameter to point to a model of your choosing. Note that some models have particular prompt formats. For instance, if using the default Zephyr-7B model above, as described on the model’s home page, the prompt_template in the YAML file must be set to:\nIf changing models, don’t forget to update the prompt_template variable with the prompt format approrpriate for your chosen model.\nYou do not need a prompt_template value if using Ollama or transformers as the LLM engine.",
    "crumbs": [
      "Examples",
      "A Built-In Web App"
    ]
  },
  {
    "objectID": "webapp.html#using-prompts-to-solve-problems",
    "href": "webapp.html#using-prompts-to-solve-problems",
    "title": "A Built-In Web App",
    "section": "Using Prompts to Solve Problems",
    "text": "Using Prompts to Solve Problems\nThe first app page is a UI for interactive chatting and prompting to solve problems various problems with local LLMs.",
    "crumbs": [
      "Examples",
      "A Built-In Web App"
    ]
  },
  {
    "objectID": "webapp.html#talk-to-your-documents",
    "href": "webapp.html#talk-to-your-documents",
    "title": "A Built-In Web App",
    "section": "Talk To Your Documents",
    "text": "Talk To Your Documents\nThe second screen in the app is a UI for retrieval augmented generation or RAG (i.e., chatting with documents). Sources considered by the LLM when generating answers are displayed and ranked by answer-to-source similarity. Hovering over the question marks in the sources will display the snippets of text from a document considered by the LLM when generating answers. Documents you would like to consider as sources for question-answering can be uploaded through the Web UI and this is discussed below.",
    "crumbs": [
      "Examples",
      "A Built-In Web App"
    ]
  },
  {
    "objectID": "webapp.html#document-search",
    "href": "webapp.html#document-search",
    "title": "A Built-In Web App",
    "section": "Document Search",
    "text": "Document Search\nThe third screen is a UI for searching documents you’ve uploaded either through keyword searches or semantic searches. Documents that you would like to search can be uploaded through the Web app and is discussed next.",
    "crumbs": [
      "Examples",
      "A Built-In Web App"
    ]
  },
  {
    "objectID": "webapp.html#document-analysis",
    "href": "webapp.html#document-analysis",
    "title": "A Built-In Web App",
    "section": "Document Analysis",
    "text": "Document Analysis\nThe fourth screen is a UI for applying prompts to passages within uploaded documents.",
    "crumbs": [
      "Examples",
      "A Built-In Web App"
    ]
  },
  {
    "objectID": "webapp.html#workflow-builder",
    "href": "webapp.html#workflow-builder",
    "title": "A Built-In Web App",
    "section": "Workflow Builder",
    "text": "Workflow Builder\nThe Visual Workflow Builder allows users to craft complex data analysis pipelines purely through a point-and-click interface with no coding required.",
    "crumbs": [
      "Examples",
      "A Built-In Web App"
    ]
  },
  {
    "objectID": "webapp.html#ingesting-documents",
    "href": "webapp.html#ingesting-documents",
    "title": "A Built-In Web App",
    "section": "Ingesting Documents",
    "text": "Ingesting Documents\nBoth document search an document question-answering, as discussed above, require you to ingest documents into a vector store. By default, the web app uses a dual vector store that stores documents in both a conventional vector database (for semantic search) and a search index (for keyword searches).\nYou can ingest documents either manually or through a Web interface.\n\nUploading Documents Through the Web Interface\nThe Web UI includes a point-and-click interface to upload and index documents into the vector store(s). Documents can either be uploaded individually or as a zip file.\n\n\n\nIngesting Documents Through the Python API\nYou can also ingest documents through the Python API. By default, the web app assumes that the original documents are stored in &lt;home directory&gt;/onprem_data/webapp/documents and assumes the vector stores reside in &lt;home directory&gt;/onprem_data/webapp/vectordb. We just need to point LLM to these places when ingesting documents. Let’s assume you copied your project documents to /home/&lt;username&gt;/onprem_data/webapp/documents/my_project_files. You can ingest them, as follows:\nfrom onprem import LLM\nllm = LLM(store_type='dual', vectordb_path='/home/&lt;username&gt;/onprem_data/webapp/vectordb')\nllm.ingest('/home/&lt;username&gt;/onprem_data/webapp/documents/my_project_files')\nAfter the above commands complete, you should be able to search your documents and ask them questions after starting the Web app:\nonprem --port 8000",
    "crumbs": [
      "Examples",
      "A Built-In Web App"
    ]
  },
  {
    "objectID": "webapp.html#tips",
    "href": "webapp.html#tips",
    "title": "A Built-In Web App",
    "section": "Tips",
    "text": "Tips\n\nIf you’re finding answers get cut off, edit the configuration to set max_tokens to higher value than the default 512. (Not necessary when using Ollama, which uses a larger value by default.)\nYou can change the store type in the config to store_type=\"sparse\", which stores documents in a traditional search engine index, instead of a vector database. The advantage is that ingestion is a lot faster. The drawback is that sparse vectorstores assume passages with answers will include at least one word in common with the question.\nFor reasons that are unclear, parallelized ingestion in the Web interface when running on Windows 11 tends to pause for a long while before starting. For these reasons, parallelization is disabled for the Web interface when running it on Windows 11. When running on Windows systems, we recommend ingesting documents through the Python interface.",
    "crumbs": [
      "Examples",
      "A Built-In Web App"
    ]
  },
  {
    "objectID": "pipelines.guider.html",
    "href": "pipelines.guider.html",
    "title": "pipelines.guider",
    "section": "",
    "text": "source\n\nGuider\n\ndef Guider(\n    llm, kwargs:VAR_KEYWORD\n):\n\nGuider Constructor. Note that the callback andcallback_manager parameters to onprem.LLM  are not currently utilized by the Guider.\nArgs:\n\nllm: An onprem.LLM object\n\n\nsource\n\n\nGuider.prompt\n\ndef prompt(\n    guidance_program:str, echo:bool=True\n):\n\nA guided prompt. Input is a Guidance program (guidance&gt;=0.1.0)  that specifies constraints and conditions on the output for the prompt.\nArgs:\n\nguidance_program: A Guidance program (&gt;=0.1.0) in the form a string or a guidance._grammar.Join object\necho: If True, echo highlighted output in Jupyter notebook. Set echo=False if running stand-alone script.\n\nReturns:\n\nA dictionary with keys specified in the Guidance program and values containing the model outputs",
    "crumbs": [
      "Source",
      "pipelines.guider"
    ]
  },
  {
    "objectID": "ingest.stores.dual.html",
    "href": "ingest.stores.dual.html",
    "title": "ingest.stores.dual",
    "section": "",
    "text": "source\n\nElasticsearchStore\n\ndef ElasticsearchStore(\n    dense_vector_field:str='dense_vector', kwargs:VAR_KEYWORD\n):\n\nA unified Elasticsearch-based dual store that supports both dense vector searches and sparse text searches in a single index. Uses composition to manage both stores.\n\nsource\n\n\nDualStore\n\ndef DualStore(\n    dense_kind:str='chroma', dense_persist_location:Optional=None, sparse_kind:str='whoosh',\n    sparse_persist_location:Optional=None, kwargs:VAR_KEYWORD\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nDualStore.exists\n\ndef exists(\n    \n):\n\nReturns True if either store exists.\n\nsource\n\n\nDualStore.add_documents\n\ndef add_documents(\n    documents:Sequence, batch_size:int=1000, kwargs:VAR_KEYWORD\n):\n\nAdd documents to both dense and sparse stores. If both stores use the same persist_location, only add once.\n\nsource\n\n\nDualStore.remove_document\n\ndef remove_document(\n    id_to_delete\n):\n\nRemove a document from both stores.\n\nsource\n\n\nDualStore.remove_source\n\ndef remove_source(\n    source:str\n):\n\nRemove a document by source from both stores.\nThe source can either be the full path to a document or a parent folder. Returns the number of records deleted.\n\nsource\n\n\nDualStore.update_documents\n\ndef update_documents(\n    doc_dicts:dict, kwargs:VAR_KEYWORD\n):\n\nUpdate documents in both stores.\n\nsource\n\n\nDualStore.get_all_docs\n\ndef get_all_docs(\n    \n):\n\nGet all documents from the dense store. For simplicity, we only return documents from one store since they should be the same.\n\nsource\n\n\nDualStore.get_doc\n\ndef get_doc(\n    id\n):\n\nGet a document by ID from the dense store.\n\nsource\n\n\nDualStore.get_size\n\ndef get_size(\n    \n):\n\nGet the size of the dense store.\n\nsource\n\n\nDualStore.erase\n\ndef erase(\n    confirm:bool=True\n):\n\nErase both stores.\n\nsource\n\n\nVectorStore.query\n\ndef query(\n    query:str, kwargs:VAR_KEYWORD\n):\n\nGeneric query method that invokes the store’s search method. This provides a consistent interface across all store types.\n\nsource\n\n\nDualStore.semantic_search\n\ndef semantic_search(\n    query:str, kwargs:VAR_KEYWORD\n):\n\nPerform semantic search using the dense store.\n\nsource\n\n\nVectorStore.check\n\ndef check(\n    \n):\n\nRaise exception if VectorStore.exists() returns False\n\nsource\n\n\nVectorStore.ingest\n\ndef ingest(\n    source_directory:str, # path to folder containing document store\n    chunk_size:int=1000, # text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n    chunk_overlap:int=100, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n    ignore_fn:Optional=None, # Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested.\n    batch_size:int=41000, # batch size used when processing documents\n    kwargs:VAR_KEYWORD\n)-&gt;None:\n\nIngests all documents in source_directory (previously-ingested documents are ignored). When retrieved, the Document objects will each have a metadata dict with the absolute path to the file in metadata[\"source\"]. Extra kwargs fed to ingest.load_single_document.",
    "crumbs": [
      "Source",
      "ingest.stores.dual"
    ]
  },
  {
    "objectID": "hf.data.sequences.html",
    "href": "hf.data.sequences.html",
    "title": "hf.data.sequences",
    "section": "",
    "text": "source\n\nSequences\n\ndef Sequences(\n    tokenizer, columns, maxlength, prefix\n):\n\nTokenizes sequence-sequence datasets as input for training sequence-sequence models"
  },
  {
    "objectID": "hf.data.texts.html",
    "href": "hf.data.texts.html",
    "title": "hf.data.texts",
    "section": "",
    "text": "source\n\nTexts\n\ndef Texts(\n    tokenizer, columns, maxlength\n):\n\nTokenizes text datasets as input for training language models."
  },
  {
    "objectID": "pipelines.agent.base.html",
    "href": "pipelines.agent.base.html",
    "title": "pipelines.agent.base",
    "section": "",
    "text": "source\n\nAgent\n\ndef Agent(\n    llm, agent_type:str='tool_calling', max_steps:int=20, tools:dict={}, kwargs:VAR_KEYWORD\n):\n\nPipeline for agent-based task execution using smolagents. Extra kwargs are supplied directly to agent instantation.\nArgs: llm (LLM): An onprem LLM instance to use for agent reasoning agent_type (str, optional): Type of agent to use (‘tool_calling’ or ‘code’) max_steps (int, optional): Maximum number of steps the agent can take tools (dict, optional): a dictionary of tools for agent to use\n\nsource\n\n\nAgent.add_tool\n\ndef add_tool(\n    name:str, tool_instance, # tool_instance is SA_Tool\n):\n\nAdd a tool to the agent.\nArgs: name (str): The name of the tool tool_instance (Tool): The tool instance\n\nsource\n\n\nAgent.add_function_tool\n\ndef add_function_tool(\n    func:Callable\n):\n\nCreate a tool from a function and its documentation.\nArgs: func (Callable): The function to wrap as a tool name (str, optional): The name of the tool (defaults to function name) description (str, optional): The description of the tool (defaults to function docstring)\nReturns: Tool: The created tool\n\nsource\n\n\nAgent.add_vectorstore_tool\n\ndef add_vectorstore_tool(\n    name:str, store:VectorStore, description:str='Search a vector database for relevant information'\n):\n\nCreate a tool from a VectorStore instance.\nArgs: name (str): The name of the vector store tool store (VectorStore): The vector store instance description (str, optional): The description of the vector store\nReturns: Tool: The created tool\n\nsource\n\n\nAgent.add_websearch_tool\n\ndef add_websearch_tool(\n    \n):\n\nCreate a tool to perform Web searches.\nReturns: Tool: The created tool\n\nsource\n\n\nAgent.add_webview_tool\n\ndef add_webview_tool(\n    \n):\n\nCreate a tool to visit Web page\nReturns: Tool: The created tool\n\nsource\n\n\nAgent.add_python_tool\n\ndef add_python_tool(\n    \n):\n\nCreate a tool to access Python interpreter\nReturns: Tool: The created tool\n\nsource\n\n\nAgent.add_mcp_tool\n\ndef add_mcp_tool(\n    url:str\n):\n\nAdd tool to access MCP server\n\nsource\n\n\nAgent.run\n\ndef run(\n    task:str\n)-&gt;str:\n\nRun the agent on a given task.\nArgs: task (str): The task description\nReturns: str: The agent’s response",
    "crumbs": [
      "Source",
      "pipelines.agent.base"
    ]
  },
  {
    "objectID": "hf.models.models.html",
    "href": "hf.models.models.html",
    "title": "hf.models.models",
    "section": "",
    "text": "source\n\nModels\n\ndef Models(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nUtility methods for working with machine learning models"
  },
  {
    "objectID": "hf.data.labels.html",
    "href": "hf.data.labels.html",
    "title": "hf.data.labels",
    "section": "",
    "text": "source\n\nLabels\n\ndef Labels(\n    tokenizer, columns, maxlength\n):\n\nTokenizes text-classification datasets as input for training text-classification models."
  },
  {
    "objectID": "pipelines.summarizer.html",
    "href": "pipelines.summarizer.html",
    "title": "pipelines.summarizer",
    "section": "",
    "text": "source\n\nSummarizer\n\ndef Summarizer(\n    llm, prompt_template:Optional=None, map_prompt:Optional=None, reduce_prompt:Optional=None,\n    refine_prompt:Optional=None, kwargs:VAR_KEYWORD\n):\n\nSummarizer summarizes one or more documents\nArgs:\n\nllm: An onprem.LLM object\nprompt_template: A model specific prompt_template with a single placeholder named “{prompt}”. All prompts (e.g., Map-Reduce prompts) are wrapped within this prompt. If supplied, overrides the prompt_template supplied to the LLM constructor.\nmap_prompt: Map prompt for Map-Reduce summarization. If None, default is used.\nreduce_prompt: Reduce prompt for Map-Reduce summarization. If None, default is used.\nrefine_prompt: Refine prompt for Refine-based summarization. If None, default is used.\n\n\nsource\n\n\nSummarizer.summarize\n\ndef summarize(\n    fpath:str=None, # path to either a folder of documents or a single file\n    strategy:str='map_reduce', # One of {'map_reduce', 'refine'}\n    chunk_size:int=1000, # Number of characters of each chunk to summarize\n    chunk_overlap:int=0, # Number of characters that overlap between chunks\n    token_max:int=2000, # Maximum number of tokens to group documents into\n    max_chunks_to_use:Optional=None, # Maximum number of chunks (starting from beginning) to use\n    raw_text:str=None, # Optional: raw text to process (skips file loading)\n):\n\nSummarize one or more documents (e.g., PDFs, MS Word, MS Powerpoint, plain text) using either Langchain’s Map-Reduce strategy or Refine strategy. The max_chunks parameter may be useful for documents that have abstracts or informative introductions. If max_chunks=None, all chunks are considered for summarizer.\nArgs: fpath: Path to file or directory strategy: Summarization strategy (‘map_reduce’ or ‘refine’) chunk_size: Characters per chunk chunk_overlap: Character overlap between chunks\ntoken_max: Maximum tokens to group documents into max_chunks_to_use: Maximum chunks to process raw_text: Raw text to process (skips file loading)\nNote: Provide exactly one of: fpath or raw_text\n\nsource\n\n\nSummarizer.summarize_by_concept\n\ndef summarize_by_concept(\n    fpath:NoneType=None, # path to file, raw text, or list of pre-chunked text\n    concept_description:str=None, # Summaries are generated with respect to the described concept.\n    similarity_threshold:float=0.0, # Minimum similarity for consideration. Tip: Increase this when using similarity_method=\"senttransform\" to mitigate hallucination. A value of 0.0 is sufficient for TF-IDF or should be kept near-zero.\n    max_chunks:int=4, # Only this many snippets above similarity_threshold are considered.\n    similarity_method:str='tfidf', # One of \"senttransform\" (sentence-transformer embeddings) or \"tfidf\" (TF-IDF)\n    summary_prompt:str='What does the following context say with respect \"{concept_description}\"? \\n\\nCONTEXT:\\n{text}', # The prompt used for summarization. Should have exactly two variables, {concept_description} and {text}.\n    raw_text:str=None, # Optional: raw text to process (skips file loading)\n    chunks:list=None, # Optional: pre-chunked text as list (skips both file loading and chunking)\n):\n\nSummarize document with respect to concept described by concept_description. Returns a tuple of the form (summary, sources).\nArgs: fpath: Path to file concept_description: The concept to focus summarization on similarity_threshold: Minimum similarity score for chunk consideration max_chunks: Maximum number of chunks to use for summarization similarity_method: “tfidf” or “senttransform” summary_prompt: Template for summarization prompt raw_text: Raw text to process (skips file loading) chunks: Pre-chunked text as list (skips file loading and chunking)\nNote: Provide exactly one of: fpath, raw_text, or chunks",
    "crumbs": [
      "Source",
      "pipelines.summarizer"
    ]
  },
  {
    "objectID": "ingest.helpers.html",
    "href": "ingest.helpers.html",
    "title": "ingest.helpers",
    "section": "",
    "text": "source\n\nmd5sum\n\ndef md5sum(\n    filepath\n):\n\nPerform an MD5 hash of a file.\n\nsource\n\n\ndate2iso\n\ndef date2iso(\n    d\n):\n\n\nsource\n\n\niso2date\n\ndef iso2date(\n    s\n):\n\n\nsource\n\n\nextract_file_dates\n\ndef extract_file_dates(\n    filepath\n):\n\nTakes a file path and returns an ISO datetime string of last-modified and create date of file.\nReturns tuple of the form (create-date, last-modify-date)\n\nsource\n\n\nextract_extension\n\ndef extract_extension(\n    filepath:str, include_dot:bool=False\n):\n\nExtracts file extension (including dot) from file path\n\nsource\n\n\nextract_files\n\ndef extract_files(\n    source_dir:str, follow_links:bool=False, extensions:Union=None\n):\n\n\nsource\n\n\nextract_tables\n\ndef extract_tables(\n    filepath:Optional=None, docs:Optional=[]\n)-&gt;List:\n\nExtract tables from PDF and append to end of supplied Document list. Accepts either a filepath or a list of LangChain Document objects all from a single file. If filepath is empty, the file path of interest is extracted from docs.\nReturns an updated list of Document objects appended with extracted tables.\n\nsource\n\n\nincludes_caption\n\ndef includes_caption(\n    d:Document\n):\n\nReturns True if content of supplied Document includes a table caption\n\nsource\n\n\nis_random_plaintext\n\ndef is_random_plaintext(\n    extension, mimetype\n):\n\nCheck mimetype for plain text\n\nsource\n\n\nextract_mimetype\n\ndef extract_mimetype(\n    filepath\n):\n\nExtract mimetype. Returns a tuple with extracted mimetype, type, subtype.\n\nsource\n\n\nget_mimetype\n\ndef get_mimetype(\n    filepath\n):\n\n\nsource\n\n\nclean_text\n\ndef clean_text(\n    text_s_or_b\n):\n\nconvert to string and strip.\n\nsource\n\n\nParagraphTextSplitter\n\ndef ParagraphTextSplitter(\n    chunk_size:int=5000, chunk_overlap:int=0\n):\n\nInterface for splitting text into chunks.\n\nsource\n\n\nextract_file_metadata\n\ndef extract_file_metadata(\n    file_path:str, store_md5:bool=True, store_mimetype:bool=True, store_file_dates:bool=True, file_callables:dict={}\n):\n\nExtract file metadata\n\nsource\n\n\nset_metadata_defaults\n\ndef set_metadata_defaults(\n    docs:List, extra_keys:list=[]\n):\n\nSets Document metadata defaults\n\nsource\n\n\ncreate_document\n\ndef create_document(\n    page_content:str, only_required_metadata:bool=True, kwargs:VAR_KEYWORD\n):\n\nCreate document with required metadata keys from METADATA.\n\nsource\n\n\ndict_from_doc\n\ndef dict_from_doc(\n    doc, content_field:str='page_content'\n):\n\nCreate dictinoary from LangChain Document\n\nsource\n\n\ndoc_from_dict\n\ndef doc_from_dict(\n    d:dict, content_field:str='page_content'\n):\n\nCreate LangChain Document from dicationary",
    "crumbs": [
      "Source",
      "ingest.helpers"
    ]
  },
  {
    "objectID": "examples_vectorstore_factory.html",
    "href": "examples_vectorstore_factory.html",
    "title": "Using Different Vector Stores",
    "section": "",
    "text": "This example demonstrates how to use the VectorStoreFactory in OnPrem.LLM to easily create and experiment with different types of vector stores for your RAG (Retrieval-Augmented Generation) and semantic search applications.\nThe VectorStoreFactory provides a unified interface for creating three different types of vector stores, each optimized for different use cases:\nThis makes it easy to experiment with different search strategies and find the best approach for your specific data and use case.",
    "crumbs": [
      "Examples",
      "Using Different Vector Stores"
    ]
  },
  {
    "objectID": "examples_vectorstore_factory.html#setup",
    "href": "examples_vectorstore_factory.html#setup",
    "title": "Using Different Vector Stores",
    "section": "Setup",
    "text": "Setup\nFirst, let’s create some sample documents that we’ll use throughout our examples:\n\nimport tempfile\nimport os\nfrom langchain_core.documents import Document\nfrom onprem.ingest.stores import VectorStoreFactory\n\n# Create some sample documents for our examples\nsample_docs = [\n    Document(\n        page_content=\"Machine learning is a subset of artificial intelligence that enables computers to learn without explicit programming.\",\n        metadata={\"source\": \"ml_intro.txt\", \"topic\": \"AI\", \"difficulty\": \"beginner\"}\n    ),\n    Document(\n        page_content=\"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n        metadata={\"source\": \"dl_guide.txt\", \"topic\": \"AI\", \"difficulty\": \"intermediate\"}\n    ),\n    Document(\n        page_content=\"Natural language processing (NLP) enables computers to understand and process human language.\",\n        metadata={\"source\": \"nlp_basics.txt\", \"topic\": \"AI\", \"difficulty\": \"beginner\"}\n    ),\n    Document(\n        page_content=\"Vector databases store high-dimensional vectors and enable similarity search for AI applications.\",\n        metadata={\"source\": \"vector_db.txt\", \"topic\": \"databases\", \"difficulty\": \"intermediate\"}\n    ),\n    Document(\n        page_content=\"Retrieval-augmented generation (RAG) combines information retrieval with language generation for better AI responses.\",\n        metadata={\"source\": \"rag_overview.txt\", \"topic\": \"AI\", \"difficulty\": \"advanced\"}\n    ),\n    Document(\n    page_content=\"Cats have five toes on their front paws, four on their back paws, and zero interest in your personal space..\",\n    metadata={\"source\": \"cat_facts.txt\", \"topic\": \"cats\", \"difficulty\": \"advanced\"}\n    )\n]\n\nprint(f\"Created {len(sample_docs)} sample documents for testing\")\n\nCreated 6 sample documents for testing",
    "crumbs": [
      "Examples",
      "Using Different Vector Stores"
    ]
  },
  {
    "objectID": "examples_vectorstore_factory.html#example-1-chromastore-dense-vector-search",
    "href": "examples_vectorstore_factory.html#example-1-chromastore-dense-vector-search",
    "title": "Using Different Vector Stores",
    "section": "Example 1: ChromaStore (Dense Vector Search)",
    "text": "Example 1: ChromaStore (Dense Vector Search)\nChromaStore is the default option and excels at semantic similarity search. It’s perfect when you want to find documents that are conceptually similar to your query, even if they don’t share exact keywords.\n\n# Create ChromaStore using the factory (default)\nchroma_path = tempfile.mkdtemp()\nchroma_store = VectorStoreFactory.create(\n    kind='chroma',  # or just use default: VectorStoreFactory.create()\n    persist_location=chroma_path\n)\n\nprint(f\"Created ChromaStore at: {chroma_path}\")\nprint(f\"Store type: {type(chroma_store).__name__}\")\n\n# Add documents\nchroma_store.add_documents(sample_docs)\nprint(f\"Added {len(sample_docs)} documents to ChromaStore\")\n\n# Test semantic search - look for documents about AI/ML\nresults = chroma_store.semantic_search(\"artificial intelligence and machine learning\", limit=3)\nprint(f\"\\nSemantic search results for 'artificial intelligence and machine learning':\")\nfor i, doc in enumerate(results, 1):\n    print(f\"{i}. {doc.page_content[:60]}... (from {doc.metadata['source']})\")\n    print(f\"   Similarity score: {doc.metadata.get('score', 'N/A'):.3f}\")\n\n# Test semantic search - look for documents about felines\nresults = chroma_store.semantic_search(\"feline feet\", limit=3)\nprint(f\"\\nSemantic search results for 'feline feet':\")\nfor i, doc in enumerate(results, 1):\n    print(f\"{i}. {doc.page_content[:60]}... (from {doc.metadata['source']})\")\n    print(f\"   Similarity score: {doc.metadata.get('score', 'N/A'):.3f}\")\n\n# Show that semantic search finds conceptually related content\nprint(f\"\\nSemantic search for 'computer intelligence' (no exact keyword matches):\")\nresults = chroma_store.semantic_search(\"computer intelligence\", limit=2)\nfor doc in results:\n    print(f\"- {doc.page_content[:60]}... (score: {doc.metadata.get('score', 'N/A'):.3f}, category: {doc.metadata.get('topic', 'N/A')})\")\n\nCreated ChromaStore at: /tmp/tmpmlbc1286\nStore type: ChromaStore\nCreating embeddings. May take some minutes...\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  3.51it/s]\n\n\nAdded 6 documents to ChromaStore\n\nSemantic search results for 'artificial intelligence and machine learning':\n1. Machine learning is a subset of artificial intelligence that... (from ml_intro.txt)\n   Similarity score: 0.621\n2. Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n   Similarity score: 0.439\n3. Vector databases store high-dimensional vectors and enable s... (from vector_db.txt)\n   Similarity score: 0.357\n\nSemantic search results for 'feline feet':\n1. Cats have five toes on their front paws, four on their back ... (from cat_facts.txt)\n   Similarity score: 0.538\n2. Vector databases store high-dimensional vectors and enable s... (from vector_db.txt)\n   Similarity score: 0.059\n3. Natural language processing (NLP) enables computers to under... (from nlp_basics.txt)\n   Similarity score: 0.030\n\nSemantic search for 'computer intelligence' (no exact keyword matches):\n- Machine learning is a subset of artificial intelligence that... (score: 0.524, category: AI)\n- Natural language processing (NLP) enables computers to under... (score: 0.406, category: AI)",
    "crumbs": [
      "Examples",
      "Using Different Vector Stores"
    ]
  },
  {
    "objectID": "examples_vectorstore_factory.html#example-2-whooshstore-sparse-keyword-search",
    "href": "examples_vectorstore_factory.html#example-2-whooshstore-sparse-keyword-search",
    "title": "Using Different Vector Stores",
    "section": "Example 2: WhooshStore (Sparse Keyword Search)",
    "text": "Example 2: WhooshStore (Sparse Keyword Search)\nWhooshStore uses full-text search and is excellent for exact keyword matching and boolean queries. It’s faster for ingestion and works well when you know specific terms you’re looking for. Unlike ChromaStore, WhooshStore converts text to dense vectors on-the-fly for semantic searches. Since vectors are not computed at index time, ingestion is very fast.\n\n# Create WhooshStore using the factory\nwhoosh_path = tempfile.mkdtemp()\nwhoosh_store = VectorStoreFactory.create(\n    kind='whoosh',\n    persist_location=whoosh_path\n)\n\nprint(f\"Created WhooshStore at: {whoosh_path}\")\nprint(f\"Store type: {type(whoosh_store).__name__}\")\n\n# Add documents\nwhoosh_store.add_documents(sample_docs)\nprint(f\"Added {len(sample_docs)} documents to WhooshStore\")\n\n# Test keyword search - exact term matching\nresults = whoosh_store.query(\"neural networks\", limit=3)\nprint(f\"\\nKeyword search results for 'neural networks':\")\nprint(f\"Total hits: {results['total_hits']}\")\nfor i, hit in enumerate(results['hits'], 1):\n    print(f\"{i}. {hit['page_content'][:60]}... (from {hit['source']})\")\n\n# Show boolean search capabilities\nresults = whoosh_store.query(\"machine AND learning\", limit=3)\nprint(f\"\\nBoolean search for 'machine AND learning':\")\nprint(f\"Total hits: {results['total_hits']}\")\nfor hit in results['hits']:\n    print(f\"- {hit['page_content'][:60]}...\")\n\n# Test semantic search (uses embeddings on top of keyword results)\nsemantic_results = whoosh_store.semantic_search(\"feline feet\", limit=2, filters={'topic' :'cats'})\nprint(f\"\\nSemantic search results for 'feline feet':\")\nfor doc in semantic_results:\n    print(f\"- {doc.page_content[:60]}... (score: {doc.metadata.get('score', 'N/A'):.3f}, category: {doc.metadata.get('topic', 'N/A')})\")\nwhoosh_store.erase(confirm=False)\n\nCreated WhooshStore at: /tmp/tmp94i3unhk\nStore type: WhooshStore\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00&lt;00:00, 1404.73it/s]\n\n\nAdded 6 documents to WhooshStore\n\nKeyword search results for 'neural networks':\nTotal hits: 1\n1. Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n\nBoolean search for 'machine AND learning':\nTotal hits: 1\n- Machine learning is a subset of artificial intelligence that...\n\n\n\n\n\n\nSemantic search results for 'feline feet':\n- Cats have five toes on their front paws, four on their back ... (score: 0.538, category: cats)\n\n\nTrue",
    "crumbs": [
      "Examples",
      "Using Different Vector Stores"
    ]
  },
  {
    "objectID": "examples_vectorstore_factory.html#example-3-elasticsearchstore-hybrid-search",
    "href": "examples_vectorstore_factory.html#example-3-elasticsearchstore-hybrid-search",
    "title": "Using Different Vector Stores",
    "section": "Example 3: ElasticsearchStore (Hybrid Search)",
    "text": "Example 3: ElasticsearchStore (Hybrid Search)\nElasticsearchStore combines both dense and sparse search capabilities in a single unified store. It can perform keyword search, semantic search, and hybrid search that combines both approaches.\nNote: This example requires Elasticsearch to be running. These examples use Elasticsearch 8.15.5, but Elasticsearch 9.x is also supported.\nYou can download Elasticsearch and start it from command-line:\n ./elasticsearch-8.15.5/bin/elasticsearch\nWhen starting Elasticsearch for the first time, make note of the password and set the following dictionary accordingly:\nIf you don’t have Elasticsearch installed, you can skip this section or also try setting it up using Docker:\n\n# Elasticsearch 8.x with security disabled:\ndocker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -e \"xpack.security.http.ssl.enabled=false\" elasticsearch:8.15.5\n\nelastic_params = {'persist_location': 'https://localhost:9200', \n                  'index_name': 'demo_index', \n                  'verify_certs': True, \n                  'ca_certs': '/PATH/TO/ELASTIC_FOLDER/elasticsearch-8.15.5/config/certs/http_ca.crt', \n                  'basic_auth': ('elastic', 'YOUR_PASSWORD')}\n\n\n# Create ElasticsearchStore using the factory\n  # Note: This requires Elasticsearch to be running on localhost:9200\n  try:\n      elasticsearch_store = VectorStoreFactory.create(\n          kind='elasticsearch', **elastic_params,\n      )\n\n      print(f\"Created ElasticsearchStore\")\n      print(f\"Store type: {type(elasticsearch_store).__name__}\")\n\n      # Add documents\n      elasticsearch_store.add_documents(sample_docs)\n      print(f\"Added {len(sample_docs)} documents to ElasticsearchStore\")\n\n      # Test keyword search (sparse)\n      search_results = elasticsearch_store.search(\"neural networks\", limit=3)\n      print(f\"\\nKeyword search results for 'neural networks':\")\n      print(f\"Total hits: {search_results['total_hits']}\")\n      for hit in search_results['hits']:\n          print(f\"- {hit['page_content'][:60]}... (from {hit['source']})\")\n\n      # Test semantic search (dense)\n      #semantic_results = elasticsearch_store.semantic_search(\"AI algorithms\", limit=3)\n      semantic_results = elasticsearch_store.semantic_search(\"artificial intelligence and machine learning\", limit=3)\n\n      print(f\"\\nSemantic search results for 'artificial intelligence and machine learning':\")\n      print(f\"Total returned results: {len(semantic_results)}\")\n      for hit in semantic_results:\n          # Show more precision in scores to see if they're actually different\n          score = hit.metadata.get('score', 'N/A')\n          score_str = f\"{score:.6f}\" if isinstance(score, (int, float)) else str(score)\n          print(f\"- {hit.page_content[:60]}... (score: {score_str}, category: {hit.metadata.get('topic', 'N/A')})\")\n\n      # Test semantic search (dense)\n      semantic_results = elasticsearch_store.semantic_search(\"feline feet\", limit=3)\n      print(f\"\\nSemantic search results for 'feline feet':\")\n      print(f\"Total results returned: {len(semantic_results)}\")\n      for hit in semantic_results:\n          # Show more precision in scores to see if they're actually different\n          score = hit.metadata.get('score', 'N/A')\n          score_str = f\"{score:.6f}\" if isinstance(score, (int, float)) else str(score)\n          print(f\"- {hit.page_content[:60]}... (score: {score_str}, category: {hit.metadata.get('topic', 'N/A')})\")\n      \n      # Test hybrid search (combines both dense and sparse)\n      hybrid_results = elasticsearch_store.hybrid_search(\n          \"AI algorithms\",\n          limit=3,\n          weights=[0.7, 0.3]  # 70% semantic, 30% keyword\n      )\n      print(f\"\\nHybrid search results for 'machine learning algorithms':\")\n      print(f\"Total returned results: {len(hybrid_results)}\")\n      for hit in hybrid_results:\n          score = hit.metadata.get('score', 'N/A')\n          score_str = f\"{score:.6f}\" if isinstance(score, (int, float)) else str(score)\n          print(f\"- {hit.page_content[:60]}... (combined score: {score_str})\")\n\n      # Clean up\n      elasticsearch_store.erase(confirm=False)\n      print(f\"\\nCleaned up ElasticsearchStore\")\n\n  except Exception as e:\n      print(f\"ElasticsearchStore example skipped: {e}\")\n      print(\"Make sure Elasticsearch is running on localhost:9200\")\n\nCreated ElasticsearchStore\nStore type: ElasticsearchStore\nAdded 6 documents to ElasticsearchStore\n\nKeyword search results for 'neural networks':\nTotal hits: 1\n- Deep learning uses neural networks with multiple layers to m... (from dl_guide.txt)\n\nSemantic search results for 'artificial intelligence and machine learning':\nTotal returned results: 3\n- Machine learning is a subset of artificial intelligence that... (score: 0.621063, category: AI)\n- Deep learning uses neural networks with multiple layers to m... (score: 0.439149, category: AI)\n- Vector databases store high-dimensional vectors and enable s... (score: 0.357402, category: databases)\n\nSemantic search results for 'feline feet':\nTotal results returned: 3\n- Cats have five toes on their front paws, four on their back ... (score: 0.537507, category: cats)\n- Vector databases store high-dimensional vectors and enable s... (score: 0.059024, category: databases)\n- Natural language processing (NLP) enables computers to under... (score: 0.029732, category: AI)\n\nHybrid search results for 'machine learning algorithms':\nTotal returned results: 3\n- Vector databases store high-dimensional vectors and enable s... (combined score: 0.598861)\n- Retrieval-augmented generation (RAG) combines information re... (combined score: 0.355312)\n- Machine learning is a subset of artificial intelligence that... (combined score: 0.309971)\n\nCleaned up ElasticsearchStore",
    "crumbs": [
      "Examples",
      "Using Different Vector Stores"
    ]
  },
  {
    "objectID": "examples_vectorstore_factory.html#integration-with-llm",
    "href": "examples_vectorstore_factory.html#integration-with-llm",
    "title": "Using Different Vector Stores",
    "section": "Integration with LLM",
    "text": "Integration with LLM\nThe VectorStoreFactory works seamlessly with OnPrem.LLM for complete RAG (Retrieval-Augmented Generation) workflows.\nBy default, supplying store_type=\"dense\" to LLM will use ChromaStore and supplying store_type=\"sparse\" will use WhooshStore. If you supply store_type=\"dual\", a hybrid vector store that uses both ChromaStore and WhooshStore is used.\nThe ElasticsearchStore is also a hybrid vector store in that it stores documents as both dense vectors and sparse vectors.\nTo use ElasticsearchStore like the one we used above, you can supply it to load_vectorstore as a custom vector store:\nllm = LLM(...)\nllm.load_vectorstore(custom_vectorstore=elasticsearch_store)\nYou can also implement and use your own custom VectorStore instances (by subclassing DenseStore, SparseStore, or DualStore) using whatever vector database backend you like.\nFor illustration purposes, in the example below, we explictly tell LLM to use WhooshStore as a custom vector store. (This is equivalent to supplying store_type=\"sparse\" to LLM, but it shows how you would use LLM with Elasticsearch or your own custom vector store.)\n\n# Example: Using VectorStoreFactory with LLM for RAG\nprint(\"🤖 Integration with OnPrem.LLM:\")\n\n# Create a simple document corpus\ndocuments_dir = tempfile.mkdtemp()\ndoc_files = {\n    \"ai_overview.txt\": \"Artificial intelligence is transforming how we work and live. Machine learning enables computers to learn from data without explicit programming.\",\n    \"ml_types.txt\": \"There are three main types of machine learning: supervised learning uses labeled data, unsupervised learning finds patterns in unlabeled data, and reinforcement learning learns through trial and error.\",\n    \"applications.txt\": \"AI applications include natural language processing for text analysis, computer vision for image recognition, and recommendation systems for personalized content.\"\n}\n\n# Write documents to files\nfor filename, content in doc_files.items():\n    with open(os.path.join(documents_dir, filename), 'w') as f:\n        f.write(content)\n\nprint(f\"✓ Created {len(doc_files)} documents in {documents_dir}\")\n\n# Show how to use custom vector store with LLM\nfrom onprem import LLM\nfrom onprem.ingest.stores import VectorStoreFactory\n\n# Create custom vector store\nstore = VectorStoreFactory.create('whoosh', persist_location='/tmp/my_search_index')\n\n# Create LLM and use custom vector store\nllm = LLM('openai/gpt-4o-mini', vectordb_path=tempfile.mkdtemp())\nllm.load_vectorstore(custom_vectorstore=store)\n\n# Ingest documents\nllm.ingest(documents_dir)\n\nprint('\\n\\n----RAG EXAMPLE----')\n# Ask questions\nquestion = 'What are the types of machine learning?'\nprint(f'QUESTION: {question}')\nprint()\nresult = llm.ask(question)\n\nprint('\\n\\nSOURCES:')\nfor i, d in enumerate(result['source_documents']):\n    print(f\"source #{i+1}: {d.metadata['source']}\")\nstore.erase(confirm=False)\n\n🤖 Integration with OnPrem.LLM:\n✓ Created 3 documents in /tmp/tmpjekc6pkt\nCreating new vectorstore at /tmp/my_search_index\nLoading documents from /tmp/tmpjekc6pkt\n\n\nLoading new documents: 100%|█████████████████████| 3/3 [00:00&lt;00:00, 175.48it/s]\nProcessing and chunking 3 new documents: 100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 248.67it/s]\n\n\nSplit into 3 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n\n\n100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00&lt;00:00, 983.81it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n----RAG EXAMPLE----\nQUESTION: What are the types of machine learning?\n\nThe types of machine learning are:\n\n1. Supervised learning - uses labeled data.\n2. Unsupervised learning - finds patterns in unlabeled data.\n3. Reinforcement learning - learns through trial and error.\n\nSOURCES:\nsource #1: /tmp/tmpjekc6pkt/ml_types.txt\nsource #2: /tmp/tmpjekc6pkt/ai_overview.txt\n\n\nTrue",
    "crumbs": [
      "Examples",
      "Using Different Vector Stores"
    ]
  },
  {
    "objectID": "examples_vectorstore_factory.html#applying-llms-to-documents-in-pre-existing-search-engines",
    "href": "examples_vectorstore_factory.html#applying-llms-to-documents-in-pre-existing-search-engines",
    "title": "Using Different Vector Stores",
    "section": "Applying LLMs to Documents in Pre-Existing Search Engines",
    "text": "Applying LLMs to Documents in Pre-Existing Search Engines\nMany applications have documents already stored in a conventional Elasticsearch index with no vector embeddings. Surprsingly, you can still apply RAG and semantic sesarch to such documents despite the fact that they have not been preprocessed for generative AI.\n\nRAG With an Existing Elasticsearch Index\nThe ElasticsearchSparseStore module in OnPrem.LLM allows you to point OnPrem.LLM to any Elasticsearch instance for RAG and semantic similarity applications.\nYou can do so by instantiating ElasticsearchSparseStore as follows:\nfrom onprem.ingest.stores import VectorStoreFactory\nstore = VectorStoreFactory.create(\n    kind='elasticsearch_sparse', \n    persist_location='https://localhost:9200',\n    index_name='NAME_OF_YOUR_INDEX',\n    # Map OnPrem.LLM field names to your existing field names\n    content_field='content',      # Your content field name\n    id_field='doc_id',            # Your ID field name\n    source_field='filepath',      # Your source field name (optional)\n    content_analyzer='english',   # Your analyzer (defaults to standard)\n    # Optional: Authentication if needed\n    basic_auth=('elastic', 'CHANGEME'),\n    verify_certs=False, # change to True if you provide path to ES certs as we did above\n    # Optional: Enable semantic search with dynamic chunking\n    chunk_for_semantic_search=True,\n    chunk_size=500,\n    chunk_overlap=50.\n    n_candidates=25,       # number of documents to inspect for answer (default: limit*10)\n)\n\n# traditional keyword search\nresults = store.search('\"machine learning\"', filters={'extension' : 'pdf') # assuming here you have an extension field in your index\n\n# semantic searches (no vectors need to be indexed in your Elasticsearch instance!)\nresults = store.semantic_search('\"machine learning\"', return_chunks=False) # set return_chunks=True for RAG applications\n# best matching chunk from document\nbest_chunk_id =  results[0].metadata['best_chunk_idx']\nprint(results[0].metadata['chunks'][best_chunk_id]\n\n# OUTPUT: 'of the machine learning (ML) workflow such as data-preprocessing and human-in-the-loop\n#          model tuning and inspection. Following inspiration from a blog post by Rachel Thomas of\n#          fast.ai (Howard and Gugger, 2020), we refer to this as Augmented Machine Learning.'\n\n# RAG\nfrom onprem import LLM\nllm = LLM(n_gpu_layers=-1)\nllm.load_vectorstore(custom_vectorstore=elasticsearch_store)\nresult = llm.ask('What is machine learning?')\nThe interesting thing in this example above is that:\n\nEmbeddings do not have to be stored in the Elasticsearch index and are computed dynamically.\nDocuments do not even need to be pre-chunked in your index.\n\n\n\nRAG With SharePoint Documents\nYou can also point OnPrem.LLM to SharePoint documents.\n# connect to SharePoint\nfrom onprem.ingest.stores import VectorStoreFactory\nconnection_params={'persist_location':\"https://sharepoint.YOUR_ORGANIZATION.org\", # URL of your SharePoint site\n                   'username':os.getenv('USERNAME'), # e.g., CORP\\username\n                   'password':os.getenv('PASSWORD'),\n                    'n_candidates':10}  # maximum number of Sharepoint documents to inspect for answer (default: limit*10)\nstore = VectorStoreFactory.create('sharepoint', **connection_params)\n\n# traditional keyword search (results are entire documents)\nresults = store.search('\"generative AI\" AND \"material science\"', where_document=\"NSF\", limit=10)\n\n# semantic search (results are text chunks from entire documents)\nresults = store.semantic_search('Can generative AI be applied to material science?', where_document='NSF AND \"material science\"', limit=4)\n\n# RAG\nfrom onprem import LLM\nllm = LLM(n_gpu_layers=-1, verbose=0)\nllm.load_vectorstore(custom_vectorstore=store)\nresult = llm.ask('Can generative AI be applied to material science?', limit=4, where_document='NSF AND \"material science\"')\nFor RAG with SharePoint, we offer the following recommendations: 1. Many SharePoint sites are configured to not return the indexed text content as part of the query results. In these situations, OnPrem.LLM will attempt to download the documents from SharePoint and perform real-time text extraction and text chunking. For these reasons, a lower n_candidates value is recommended (see above). 2. SharePoint Search uses the Keyword Query Language (KQL) — a proprietary query language designed by Microsoft for SharePoint and other Microsoft search products (like Exchange and Microsoft Search). KQL is missing some features that are useful in yielding relevant results. For these reasons, we recommend you help the LLM target the right documents by provding a supplemental query to filter documents via the where_documents argument, as we did above.\n\n# Clean up temporary directories\nimport shutil\n\ntemp_dirs = [chroma_path, whoosh_path, documents_dir]\nfor temp_dir in temp_dirs:\n    try:\n        shutil.rmtree(temp_dir)\n    except:\n        pass\n        \nprint(\"🧹 Cleaned up temporary directories\")\n\n🧹 Cleaned up temporary directories",
    "crumbs": [
      "Examples",
      "Using Different Vector Stores"
    ]
  },
  {
    "objectID": "examples_legal_analysis.html",
    "href": "examples_legal_analysis.html",
    "title": "Legal and Regulatory Analysis",
    "section": "",
    "text": "In 2024, Senator Roger Wicker published a report on strengthening the defense industrial base and reforming defense acquisition. The latter involves cutting red tape by streamlining regulations.\nIn this tutorial, we will analyze parts of the Federal Acquisition Regulations (FAR) to identify which parts of it are driven by statuatory requirement.\nFor illustration purposes, we will focus our analysis on Part 9 of the FAR: contractor qualifications.\npart_prefixes = ['9.']\nfrom onprem import LLM\nfrom onprem.ingest import load_single_document, extract_files\nfrom onprem import utils as U\nfrom tqdm import tqdm\n\nimport pandas as pd\n\n\npd.set_option('display.max_colwidth', None)",
    "crumbs": [
      "Examples",
      "Legal and Regulatory Analysis"
    ]
  },
  {
    "objectID": "examples_legal_analysis.html#step-1-download-the-data",
    "href": "examples_legal_analysis.html#step-1-download-the-data",
    "title": "Legal and Regulatory Analysis",
    "section": "STEP 1: Download the Data",
    "text": "STEP 1: Download the Data\nWe will first download the HTML version of the FAR.\n\nimport zipfile\nimport tempfile\nimport os\n\n# URL of the ZIP file\nurl = \"https://www.acquisition.gov/sites/default/files/current/far/zip/html/FARHTML.zip\"\n\n# Create a temporary directory\ntemp_dir = tempfile.mkdtemp()\nzip_path = os.path.join(temp_dir, \"FARHTML.zip\")\n\n# Download the ZIP file\nU.download(url, zip_path, verify=True)\n\n# Extract the ZIP file\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(temp_dir)\n\nprint(f\"\\nFiles extracted to: {temp_dir}\")\n\nfilenames = [fname for fname in extract_files(temp_dir) if any(fname.lower().endswith('.html') and os.path.basename(fname).startswith(prefix) for prefix in part_prefixes)]\nprint(f'Total files: {len(list(extract_files(temp_dir)))}')\nprint(f'Number of files of interest: {len(filenames)}')\nprint('Sample:')\nfor fname in filenames[:5]:\n    print(f'\\t{fname}')\n\n[██████████████████████████████████████████████████]\nFiles extracted to: /tmp/tmp3xa2rj1w\nTotal files: 3900\nNumber of files of interest: 106\nSample:\n    /tmp/tmp3xa2rj1w/dita_html/9.406-3.html\n    /tmp/tmp3xa2rj1w/dita_html/9.505-4.html\n    /tmp/tmp3xa2rj1w/dita_html/9.201.html\n    /tmp/tmp3xa2rj1w/dita_html/9.406-5.html\n    /tmp/tmp3xa2rj1w/dita_html/9.104-1.html",
    "crumbs": [
      "Examples",
      "Legal and Regulatory Analysis"
    ]
  },
  {
    "objectID": "examples_legal_analysis.html#step-2-text-extraction",
    "href": "examples_legal_analysis.html#step-2-text-extraction",
    "title": "Legal and Regulatory Analysis",
    "section": "STEP 2: Text Extraction",
    "text": "STEP 2: Text Extraction\nWe’ll extract text from each of the HTML files.\n\ncontent = {}\nfor filename in tqdm(filenames, total=len(filenames)):\n    text = load_single_document(filename)[0].page_content\n    content[os.path.basename(filename)] = text\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [00:06&lt;00:00, 16.76it/s]\n\n\n\nprint(content['9.505-4.html'])\n\n9.505-4 Obtaining access to proprietary information.\n\n(a) When a contractor requires proprietary information from others to perform a Government contract and can use the leverage of the contract to obtain it, the contractor may gain an unfair competitive advantage unless restrictions are imposed. These restrictions protect the information and encourage companies to provide it when necessary for contract performance. They are not intended to protect information-\n\n(1) Furnished voluntarily without limitations on its use; or\n\n(2) Available to the Government or contractor from other sources without restriction.\n\n(b) A contractor that gains access to proprietary information of other companies in performing advisory and assistance services for the Government must agree with the other companies to protect their information from unauthorized use or disclosure for as long as it remains proprietary and refrain from using the information for any purpose other than that for which it was furnished. The contracting officer shall obtain copies of these agreements and ensure that they are properly executed.\n\n(c) Contractors also obtain proprietary and source selection information by acquiring the services of marketing consultants which, if used in connection with an acquisition, may give the contractor an unfair competitive advantage. Contractors should make inquiries of marketing consultants to ensure that the marketing consultant has provided no unfair competitive advantage.",
    "crumbs": [
      "Examples",
      "Legal and Regulatory Analysis"
    ]
  },
  {
    "objectID": "examples_legal_analysis.html#step-3-setup-llm-and-test-prompt",
    "href": "examples_legal_analysis.html#step-3-setup-llm-and-test-prompt",
    "title": "Legal and Regulatory Analysis",
    "section": "STEP 3: Setup LLM and Test Prompt",
    "text": "STEP 3: Setup LLM and Test Prompt\nNext, we will setup the LLM, construct a prompt for this task, and test it on a small sample of passages from the FAR.\nSince the FAR is a publicly available document, we will use a cloud LLM (i.e., gpt-4o-mini) for this task.\n\nllm = LLM(model_url='openai://gpt-4o-mini', mute_stream=True, temperature=0)\n\n/home/amaiya/projects/ghub/onprem/onprem/llm/base.py:217: UserWarning: The model you supplied is gpt-4o-mini, an external service (i.e., not on-premises). Use with caution, as your data and prompts will be sent externally.\n  warnings.warn(f'The model you supplied is {self.model_name}, an external service (i.e., not on-premises). '+\\\n\n\n\nprompt = \"\"\"\nGiven text from the Federal Acquisition Regulations (FAR), extract a list of explicitly cited statutes.\nIf there are no explicitly cited statutes,  return NA.  If there are, retun a list of cited statutes with each statute on a separate line.  \nDo not include references to the FAR itself which are numbers with dots or dashes (e.g., 1.102-1, 3.104).\n\n# Example 1:\n\n&lt;TEXT&gt;\n(2)A violation, as determined by the Secretary of Commerce, of any agreement of the group known as the \"Coordination Committee\" for purposes of the Export Administration Act of 1979 (50 U.S.C. App. 2401, et seq.) or any similar bilateral or multilateral export control agreement.\n\n&lt;STATUTES&gt;\n50 U.S.C. App. 2401 \n\n# Example 2:\n\n&lt;TEXT&gt;\n9.400 Scope of subpart.\n(a) This subpart-\n\n(1) Prescribes policies and procedures governing the debarment and suspension of contractors by agencies for the causes given in 9.406-2 and 9.407-2;\n\n(2) Provides for the listing of contractors debarred, suspended, proposed for debarment, and declared ineligible (see the definition of \"ineligible\" in 2.101); and\n\n(3) Sets forth the consequences of this listing.\n\n&lt;STATUTES&gt;\n\nNA\n\n# Example 3:\n\n&lt;TEXT&gt;\n\n--CONTENT--\n\n&lt;STATUTES&gt;\n\"\"\"\n\n\nsamples = [ '9.104-1.html', '9.104-2.html', '9.104-3.html', '9.104-4.html', '9.104-5.html', '9.104-6.html', '9.104-7.html']\nresults = []\nfor sample in samples:\n    output = llm.prompt(prompt.replace('--CONTENT--', content[sample]))\n    results.extend([(sample, o.strip()) for o in output.strip().split('\\n') if o != 'NA'])\n    #print(output)\n\n\ndf = pd.DataFrame(results, columns =['Section', 'Statute'])\ndf.head()\n\n\n\n\n\n\n\n\nSection\nStatute\n\n\n\n\n0\n9.104-5.html\nPub. L. 113-235\n\n\n1\n9.104-6.html\n41 U.S.C. 2313(d)(3)\n\n\n2\n9.104-7.html\nPub. L. 113-235",
    "crumbs": [
      "Examples",
      "Legal and Regulatory Analysis"
    ]
  },
  {
    "objectID": "examples_legal_analysis.html#step-4-run-analyses-on-far",
    "href": "examples_legal_analysis.html#step-4-run-analyses-on-far",
    "title": "Legal and Regulatory Analysis",
    "section": "STEP 4: Run Analyses on FAR",
    "text": "STEP 4: Run Analyses on FAR\n\nresults = []\nfor k in tqdm(content, total=len(content)):\n    output = llm.prompt(prompt.replace('--CONTENT--', content[k]))\n    results.extend([(k, o.strip()) for o in output.strip().split('\\n') if o != 'NA'])\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [00:55&lt;00:00,  1.90it/s]\n\n\n\ndf = pd.DataFrame(results, columns =['FAR Section', 'Cited Statute'])\ndf = df.sort_values(by='FAR Section')\ndf.head(50)\n\n\n\n\n\n\n\n\nFAR Section\nCited Statute\n\n\n\n\n46\n9.103.html\n15 U.S.C. 637\n\n\n31\n9.104-5.html\nPub. L. 113-235\n\n\n7\n9.104-6.html\n41 U.S.C. 2313\n\n\n0\n9.104-7.html\nPub. L. 113-235\n\n\n27\n9.105-2.html\nPub. L. 111-212\n\n\n44\n9.106-4.html\n15 U.S.C. 637\n\n\n28\n9.107.html\n41 U.S.C. chapter 85\n\n\n4\n9.108-1.html\n6 U.S.C. 395(c)\n\n\n3\n9.108-1.html\n6 U.S.C. 395(b)\n\n\n25\n9.108-2.html\nPub. L. 110-161\n\n\n22\n9.109-1.html\n22 U.S.C. 2593e\n\n\n39\n9.109-4.html\n22 U.S.C. 2593e(d)\n\n\n40\n9.109-4.html\n22 U.S.C. 2593e(e)\n\n\n41\n9.109-4.html\n22 U.S.C. 2593e(g)(2)\n\n\n42\n9.109-4.html\n22 U.S.C. 2593e(b)\n\n\n38\n9.109-4.html\n22 U.S.C. 2593a\n\n\n37\n9.110-1.html\n20 U.S.C. 1001\n\n\n45\n9.110-2.html\n10 U.S.C. 983\n\n\n33\n9.110-3.html\n10 U.S.C. 983\n\n\n23\n9.200.html\n10 U.S.C. 3243\n\n\n24\n9.200.html\n41 U.S.C. 3311\n\n\n1\n9.400.html\n10 U.S.C. 983\n\n\n2\n9.400.html\nFederal Acquisition Supply Chain Security Act (FASCSA)\n\n\n43\n9.401.html\n31 U.S.C. 6101, note\n\n\n29\n9.402.html\n31 U.S.C. 6101, note\n\n\n30\n9.402.html\nPub. L. 110-417\n\n\n15\n9.403.html\n31 U.S.C. 3801-3812\n\n\n14\n9.403.html\n19 U.S.C. 1337\n\n\n13\n9.403.html\n50 U.S.C. App. 2401\n\n\n26\n9.405-1.html\n10 U.S.C. 983\n\n\n32\n9.405.html\n22 U.S.C. 2593e\n\n\n21\n9.406-2.html\nTitle 18 of the United States Code\n\n\n17\n9.406-2.html\n11 U.S.C. 362\n\n\n20\n9.406-2.html\nI.R.C. §6159\n\n\n19\n9.406-2.html\nI.R.C. §6320\n\n\n16\n9.406-2.html\n31 U.S.C. 3729-3733\n\n\n18\n9.406-2.html\nI.R.C. §6212\n\n\n12\n9.406-4.html\n41 U.S.C. chapter 81\n\n\n10\n9.407-2.html\nTitle 18 of the United States Code\n\n\n9\n9.407-2.html\nPublic Law 102-558\n\n\n8\n9.407-2.html\n41 U.S.C. chapter 81\n\n\n11\n9.407-2.html\n31 U.S.C. 3729-3733\n\n\n5\n9.500.html\nPub.L.100-463\n\n\n6\n9.500.html\n102 Stat.2270-47\n\n\n35\n9.701.html\n15 U.S.C. 640\n\n\n34\n9.701.html\n15 U.S.C. 638\n\n\n36\n9.701.html\n50 U.S.C. App. 2158",
    "crumbs": [
      "Examples",
      "Legal and Regulatory Analysis"
    ]
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Use Prompts to Solve Problems",
    "section": "",
    "text": "This notebook shows various examples of using OnPrem.LLM to solve different tasks.",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#setup-the-llm-instance",
    "href": "examples.html#setup-the-llm-instance",
    "title": "Use Prompts to Solve Problems",
    "section": "Setup the LLM instance",
    "text": "Setup the LLM instance\nIn this notebook, we will use the Llama-3.1-8B model from Meta. In particular, we will use Meta-Llama-3.1-8B-Instruct-GGUF. There are different instances of this model on the Hugging Face model hub, and we will use the one from LM Studio. When selecting a model that is different than the default ones in OnPrem.LLM, it is important to inspect the model’s home page and identify the correct prompt format. The prompt format for this model is located here, and we will supply it directly to the LLM constructor along with the URL to the specific model file we want (i.e., Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf). We will offload layers to our GPU(s) to speed up inference using the n_gpu_layers parameter. (For more information on GPU acceleration, see here.) For the purposes of this notebook, we also supply temperature=0 so that there is no variability in outputs. You can increase this value for more creativity in the outputs. Note that you can change the system prompt (i.e., “You are a super-intelligent helpful assistant…”) to fit your needs.\n\nfrom onprem import LLM\nimport os\nprompt_template = \"\"\"&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nYou are a super-intelligent helpful assistant that executes instructions.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n{prompt}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\"\"\"\n\n\nllm = LLM(model_url='https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf', \n          prompt_template= prompt_template,\n          n_gpu_layers=-1,\n          temperature=0, \n          verbose=False)\n\nllama_context: n_ctx_per_seq (3900) &lt; n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n\n\nNote that, if supplying the convenience parameter, default_model='llama' to the LLM constructor, model_url and prompt_template are set automatically and do not need to be supplied as we did above.",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#information-extraction",
    "href": "examples.html#information-extraction",
    "title": "Use Prompts to Solve Problems",
    "section": "Information Extraction",
    "text": "Information Extraction\nThis is an example of zero-shot prompting:\n\nprompt = \"\"\"Extract the names of people in the supplied sentences. Separate the names with commas.\n[Sentence]: I like Cillian Murphy's acting. Florence Pugh is great, too.\n[People]:\"\"\"\n\nsaved_output = llm.prompt(prompt, stop=[])\n\nCillian Murphy, Florence Pugh\n\n\nA more complicated example of Information Extraction using few-shot prompting:\n\nprompt = \"\"\" Extract the Name, Current Position, and Current Company from each piece of Text. \n\nText: Alan F. Estevez serves as the Under Secretary of Commerce for Industry and Security.  As Under Secretary, Mr. Estevez leads\nthe Bureau of Industry and Security, which advances U.S. national security, foreign policy, and economic objectives by ensuring an\neffective export control and treaty compliance system and promoting U.S. strategic technology leadership.\nA: Name:  Alan F. Estevez | Current Position: Under Secretary | Current Company: Bureau of Industry and Security\n\nText: Pichai Sundararajan (born June 10, 1972[3][4][5]), better known as Sundar Pichai (/ˈsʊndɑːr pɪˈtʃaɪ/), is an Indian-born American\nbusiness executive.[6][7] He is the chief executive officer (CEO) of Alphabet Inc. and its subsidiary Google.[8]\nA: Name:   Sundar Pichai | Current Position: CEO | Current Company: Google\n\nNow, provide the answer (A) from this Text:\n\nText: Norton Allan Schwartz (born December 14, 1951)[1] is a retired United States Air Force general[2] who served as the 19th Chief of Staff of the \nAir Force from August 12, 2008, until his retirement in 2012.[3] He previously served as commander, United States Transportation Command from \nSeptember 2005 to August 2008. He is currently the president of the Institute for Defense Analyses, serving since January 2, 2020.[4]\nA:\"\"\"\nsaved_output = llm.prompt(prompt, stop=[])\n\nName:  Norton Allan Schwartz | Current Position: President | Current Company: Institute for Defense Analyses",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#resume-parsing",
    "href": "examples.html#resume-parsing",
    "title": "Use Prompts to Solve Problems",
    "section": "Resume Parsing",
    "text": "Resume Parsing\nResume parsing is yet an even more complex example of information extraction.\n\n!wget https://arun.maiya.net/asmcv.pdf -O /tmp/cv.pdf\n\n--2024-11-13 12:52:50--  https://arun.maiya.net/asmcv.pdf\nResolving arun.maiya.net (arun.maiya.net)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\nConnecting to arun.maiya.net (arun.maiya.net)|185.199.109.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 62791 (61K) [application/pdf]\nSaving to: ‘/tmp/cv.pdf’\n\n/tmp/cv.pdf         100%[===================&gt;]  61.32K  --.-KB/s    in 0.002s  \n\n2024-11-13 12:52:51 (36.3 MB/s) - ‘/tmp/cv.pdf’ saved [62791/62791]\n\n\n\n\nfrom onprem.ingest import load_single_document\ndocs = load_single_document('/tmp/cv.pdf')\nresume_text = docs[0].page_content # we'll only consider the first page of CV as \"resume\"\n\n\nprompt = \"\"\"\nAnalyze the resume below and extract the relevant details. Format the response in JSON according to the specified structure below. \nOnly return the JSON response, with no additional text or explanations.\n\nEnsure to:\n- Format the full name in proper case.\n- Remove any spaces and country code from the contact number.\n- Format dates as \"dd-mm-yyyy\" if given in a more complex format, or retain the year if only the year is present.\n- Do not make up a phone number. \n- Extract only the first two jobs for Work Experience.\n\nUse the following JSON structure:\n\n```json\n{\n \"Personal Information\": {\n    \"Name\": \" \",\n    \"Contact Number\": \" \",\n    \"Address\": \" \",\n    \"Email\": \" \",\n    \"Date of Birth\": \" \"\n  },\n  \"Education\": [\n    {\n      \"Degree\": \" \",\n      \"Institution\": \" \",\n      \"Year\": \" \"\n    },\n    // Additional educational qualifications in a similar format\n  ],\n  \"Work Experience\": [\n    {\n      \"Position\": \" \",\n      \"Organization\": \" \",\n      \"Duration\": \" \",\n      \"Responsibilities\": \" \"\n    },\n    // Additional work experiences in a similar format\n  ],\n  \"Skills\": [\n  {\n    \"Skills\": \" \", // e.g., Python, R, Java, statistics, quantitative psychology, applied mathematics, machine learning, gel electrophoresis\n  },\n  // A list of skills or fields that the person has experience with\n  ],\n}\n```\n\nHere is the text of the resume:\n\n---RESUMETXT---\n\"\"\"\n\n\njson_string = llm.prompt(prompt.replace('---RESUMETXT---', resume_text))\n\n{\n \"Personal Information\": {\n    \"Name\": \"Arun S. Maiya\",\n    \"Contact Number\": \"\",\n    \"Address\": \"\",\n    \"Email\": \"arun@maiya.net\",\n    \"Date of Birth\": \"\"\n  },\n  \"Education\": [\n    {\n      \"Degree\": \"Ph.D.\",\n      \"Institution\": \"University of Illinois at Chicago\",\n      \"Year\": \" \"\n    },\n    {\n      \"Degree\": \"M.S.\",\n      \"Institution\": \"DePaul University\",\n      \"Year\": \" \"\n    },\n    {\n      \"Degree\": \"B.S.\",\n      \"Institution\": \"University of Illinois at Urbana-Champaign\",\n      \"Year\": \" \"\n    }\n  ],\n  \"Work Experience\": [\n    {\n      \"Position\": \"Research Leader\",\n      \"Organization\": \"Institute for Defense Analyses – Alexandria, VA USA\",\n      \"Duration\": \"2011-Present\",\n      \"Responsibilities\": \"\"\n    },\n    {\n      \"Position\": \"Researcher\",\n      \"Organization\": \"University of Illinois at Chicago\",\n      \"Duration\": \"2007-2011\",\n      \"Responsibilities\": \"\"\n    }\n  ],\n  \"Skills\": [\n    {\n      \"Skills\": \"applied machine learning, data science, natural language processing (NLP), network science, computer vision\"\n    }\n  ]\n}\n\n\nLet’s convert the output to a Python dictionary:\n\nimport json\nd = json.loads(json_string)\nd.keys()\n\ndict_keys(['Personal Information', 'Education', 'Work Experience', 'Skills'])\n\n\n\nd['Personal Information']['Name']\n\n'Arun S. Maiya'",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#structured-outputs",
    "href": "examples.html#structured-outputs",
    "title": "Use Prompts to Solve Problems",
    "section": "Structured Outputs",
    "text": "Structured Outputs\nIn the example above, we prompted the model to output results as a JSON string with some prompt engineering. The response_format parameter to the LLM.prompt method lets you more easily describe your desired output structure by defining a Pydantic model.\n\nfrom pydantic import BaseModel, Field\n\n\nfrom pydantic import BaseModel, Field\nclass MeasuredQuantity(BaseModel):\n    value: str = Field(description=\"Numeric value of the measurement\")\n    unit: str = Field(description=\"Unit of measurement (e.g., 'kg', 'm', 's')\")\nstructured_output = llm.prompt('He was going 35 mph.', response_format=MeasuredQuantity)\n\n{\n  \"value\": \"35\",\n  \"unit\": \"mph\"\n}\n\n\n\nprint(structured_output.value) # 35\nprint(structured_output.unit)  # mph\n\n35\nmph\n\n\nWhe using an LLM backend with native support for structured outputs (e.g., OpenAI, Claude, AWS Bedrock), Suppyling response_format will automatically leverage natively supported structured outputs in LLM backends that support it (e.g., OpenAI, Anthropic, AWS Bedrock).\nWhen supplying response_format with LLM backends that do not natively support structured outputs, LLM.prompt will fall back to a prompt-based approach to structured outputs via the LLM.pydanatic_prompt method.\nTip for LLM.pydantic_prompt:\nThe attempt_fix parameter allows you to have the LLM attempt to fix any malformed or incomplete outputs. The fix_llm parameter allows you to specific a different LLM to make the fix (the current LLM is used if fix_llm=None):\nfrom langchain_openai import ChatOpenAI\n\nstructured_output = llm.pydantic_prompt('He was going 35 mph.', pydantic_model=MeasuredQuantity,\n                                        attempt_fix=True, fix_llm=ChatOpenAI())",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#theme-discovery",
    "href": "examples.html#theme-discovery",
    "title": "Use Prompts to Solve Problems",
    "section": "Theme Discovery",
    "text": "Theme Discovery\n\nprompt = \"\"\"Please provide thematic coding for the following 20 survey responses to the question: \"What did you notice about nature today?\"\n\n1. I noticed a family of ducks waddling through a nearby park this morning.\n2. As I walked by a tree, I saw a flurry of feathers and realized a bird had just landed in its branches.\n3. While driving, I observed a herd of deer gracefully moving through a meadow.\n4. The sun's rays filtered through the leaves of trees, casting intricate patterns on the ground below.\n5. As I stepped outside, a gentle breeze carried with it the fragrance of blooming flowers.\n6. A butterfly fluttered past me as I was sitting in my garden, reminding me to enjoy life's simple pleasures.\n7. The sound of birdsong filled the air as I walked through a park this afternoon.\n8. I saw a group of ants working together to move a large pebble across the sidewalk.\n9. A squirrel darted up a tree, leaving a trail of nuts behind it.\n10. The leaves on the trees rustled as if whispering secrets in the wind.\n11. While hiking, I noticed the way sunlight filtered through the canopy of trees, creating patterns on the forest floor below.\n12. A dragonfly landed on a nearby pond, dipping its long legs into the water to drink.\n13. The chirping of crickets filled the air as I walked past a field this evening.\n14. The sky transformed from shades of blue to orange and red as the sun began to set.\n15. As the day came to a close, I watched as fireflies danced among the trees.\n16. A group of geese honked in unison as they flew overhead this afternoon.\n17. The way a butterfly's wings looked like delicate stained glass as it perched on a flower.\n18. The way the sun's rays seemed to bathe everything around me in a warm, golden light.\n19. As I walked by a field, I saw a group of rabbits darting through the tall grass.\n20. The way the dew on spider webs sparkled like diamonds in the morning sunlight\n\"\"\"\nsaved_output = llm.prompt(prompt, stop=[])\n\nAfter analyzing the 20 survey responses, I have identified several thematic codes that capture the essence of what respondents noticed about nature. Here are the thematic codes:\n\n**Code 1: Wildlife Observations (6 responses)**\n\n* Examples:\n    + \"I saw a family of ducks waddling through a nearby park this morning.\"\n    + \"A squirrel darted up a tree, leaving a trail of nuts behind it.\"\n\n**Code 2: Natural Beauty and Patterns (7 responses)**\n\n* Examples:\n    + \"The sun's rays filtered through the leaves of trees, casting intricate patterns on the ground below.\"\n    + \"The dew on spider webs sparkled like diamonds in the morning sunlight\"\n\n**Code 3: Sounds and Music of Nature (4 responses)**\n\n* Examples:\n    + \"The sound of birdsong filled the air as I walked through a park this afternoon.\"\n    + \"The chirping of crickets filled the air as I walked past a field this evening.\"\n\n**Code 4: Movement and Activity in Nature (3 responses)**\n\n* Examples:\n    + \"A group of geese honked in unison as they flew overhead this afternoon.\"\n    + \"As I watched, a group of ants worked together to move a large pebble across the sidewalk.\"\n\nThese thematic codes provide a framework for understanding the common themes and patterns that emerged from the survey responses.",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#grammar-correction",
    "href": "examples.html#grammar-correction",
    "title": "Use Prompts to Solve Problems",
    "section": "Grammar Correction",
    "text": "Grammar Correction\n\nprompt = \"\"\"Here are some examples.\n[Sentence]:\nI love goin to the beach.\n[Correction]: I love going to the beach.\n[Sentence]:\nLet me hav it!\n[Correction]: Let me have it!\n[Sentence]:\nIt have too many drawbacks.\n[Correction]: It has too many drawbacks.\n\nWhat is the correction for the following sentence?\n\n[Sentence]:\nI do not wan to go\n[Correction]:\"\"\"\nsaved_output = llm.prompt(prompt, stop=[])\n\nI do not want to go.",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#classification",
    "href": "examples.html#classification",
    "title": "Use Prompts to Solve Problems",
    "section": "Classification",
    "text": "Classification\n\nprompt = \"\"\"Classify each sentence as either positive, negative, or neutral.  Here are some examples.\n[Sentence]: I love going to the beach.\n[[Classification]: Positive\n[Sentence]: It is 10am right now.\n[Classification]: Neutral\n[Sentence]: I just got fired from my job.\n[Classification]: Negative\n\nWhat is the classification for the following sentence? Answer with either Positive or Negative only.\n[Sentence]: The reactivity of  your team has been amazing, thanks!\n[Classification]:\"\"\"\n\nsaved_output = llm.prompt(prompt, stop=['\\n'])\n\nPositive",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#paraphrasing",
    "href": "examples.html#paraphrasing",
    "title": "Use Prompts to Solve Problems",
    "section": "Paraphrasing",
    "text": "Paraphrasing\n\nprompt = \"\"\"Paraphrase the following text delimited by triple backticks using a single sentence. \n```After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.```\n\"\"\"\nsaved_output = llm.prompt(prompt)\n\nAfter a 20-year war, Kabul fell to the Taliban within hours after US troops withdrew under decisions made by Presidents Trump and Biden.",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#few-shot-answer-extraction",
    "href": "examples.html#few-shot-answer-extraction",
    "title": "Use Prompts to Solve Problems",
    "section": "Few-Shot Answer Extraction",
    "text": "Few-Shot Answer Extraction\n\nprompt = \"\"\" Compelte the correct answer based on the Context. Answer should be a short word or phrase from Context.\n[Question]: When was NLP Cloud founded?\n[Context]: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n[Answer]: 2021\n\n[Question]:  What did NLP Cloud develop?\n[Context]: NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n[Answer]: API\n\n[Question]: When can plans be stopped?\n[Context]: All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n[Answer]: Anytime\n\n[Question]: Which plan is recommended for GPT-J?\n[Context]: The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n[Answer]:\"\"\"\nsaved_output = llm.prompt(prompt, stop=['\\n\\n'])\n\nGPU plan",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#generating-product-descriptions",
    "href": "examples.html#generating-product-descriptions",
    "title": "Use Prompts to Solve Problems",
    "section": "Generating Product Descriptions",
    "text": "Generating Product Descriptions\n\nprompt = \"\"\"Generate a short Sentence from the Keywords. Here are some examples.\n[Keywords]: shoes, women, $59\n[Sentence]: Beautiful shoes for women at the price of $59.\n\n[Keywords]: trousers, men, $69\n[Sentence]: Modern trousers for men, for $69 only.\n\n[Keywords]: gloves, winter, $19\n[Sentence]:  Amazingly hot gloves for cold winters, at $19.\n\nGenerate a sentence for the following Keywords and nothing else:\n\n[Keywords]:  t-shirt, men, $39\n[Sentence]:\"\"\"\nsaved_output = llm.prompt(prompt, stop=[])\n\nA comfortable t-shirt for men, available at $39.",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#tweet-generation",
    "href": "examples.html#tweet-generation",
    "title": "Use Prompts to Solve Problems",
    "section": "Tweet Generation",
    "text": "Tweet Generation\n\nprompt = \"\"\"Generate a tweet based on the supplied Keyword. Here are some examples.\n[Keyword]:\nmarkets\n[Tweet]:\nTake feedback from nature and markets, not from people\n###\n[Keyword]:\nchildren\n[Tweet]:\nMaybe we die so we can come back as children.\n###\n[Keyword]:\nstartups\n[Tweet]: \nStartups should not worry about how to put out fires, they should worry about how to start them.\n\nGenerate a Tweet for the following keyword and nothing else:\n\n###\n[Keyword]:\nclimate change\n[Tweet]:\"\"\"\n\nsaved_output = llm.prompt(prompt)\n\nThe climate is not changing, it's us who are changing the climate.",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#generating-an-email-draft",
    "href": "examples.html#generating-an-email-draft",
    "title": "Use Prompts to Solve Problems",
    "section": "Generating an Email Draft",
    "text": "Generating an Email Draft\n\nprompt = \"\"\"Generate an email introducing Tesla to shareholders.\"\"\"\nsaved_output = llm.prompt(prompt)\n\nHere is a draft email introducing Tesla to shareholders:\n\nSubject: Welcome to Tesla, Inc.\n\nDear valued shareholder,\n\nI am thrilled to introduce you to Tesla, Inc., the pioneering electric vehicle and clean energy company. As a shareholder, you are part of our mission to accelerate the world's transition to sustainable energy.\n\nAt Tesla, we are committed to pushing the boundaries of innovation and sustainability. Our products and services include:\n\n* Electric vehicles: We design, manufacture, and sell electric vehicles that are not only environmentally friendly but also technologically advanced.\n* Energy storage: Our energy storage products, such as the Powerwall and Powerpack, enable homeowners and businesses to store excess energy generated by their solar panels or other renewable sources.\n* Solar energy: We design, manufacture, and install solar panel systems for residential and commercial customers.\n\nAs a shareholder, you are part of our journey towards a sustainable future. I invite you to explore our website and social media channels to learn more about our products and services.\n\nThank you for your support and trust in Tesla, Inc.\n\nSincerely,\n\n[Your Name]\nTesla, Inc.\n\nNote: This is just a draft email and may not be suitable for actual use.",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#talk-to-your-documents",
    "href": "examples.html#talk-to-your-documents",
    "title": "Use Prompts to Solve Problems",
    "section": "Talk to Your Documents",
    "text": "Talk to Your Documents\n\nllm.ingest(\"./tests/sample_data/\")\n\nAppending to existing vectorstore at /home/amaiya/onprem_data/vectordb\nLoading documents from ./sample_data/\n\n\nLoading new documents: 100%|██████████████████████| 1/1 [00:16&lt;00:00, 16.09s/it]\n\n\nLoaded 1 new documents from ./sample_data/\nSplit into 12 chunks of text (max. 500 chars each)\nCreating embeddings. May take some minutes...\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  6.05it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n\n\n\n\nresult = llm.ask(\"What is ktrain?\")\n\nBased on the provided context, ktrain is a tool that automates various aspects of the machine learning (ML) workflow. However, unlike traditional automation tools, ktrain also allows users to make choices and decisions that best fit their unique application requirements.\n\nIn essence, ktrain uses automation to augment and complement human engineers, rather than attempting to entirely replace them.\n\n\nPro-Tip: You can try different models or re-phrase the question/prompts accordingly, which may provide better performance for certain tasks. For instance, by supplying default_model=zephyr to the LLM constructor and leaving model_url blank, the default Zephyr-7B-beta model will be used and also performs well on the above tasks (although prompts may have to be adjusted when models are changed).",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "examples.html#asynchronous-prompts",
    "href": "examples.html#asynchronous-prompts",
    "title": "Use Prompts to Solve Problems",
    "section": "Asynchronous Prompts",
    "text": "Asynchronous Prompts\nThe onprem LLM module provides asynchronous text generation through the aprompt method, enabling concurrent processing of multiple prompts without blocking execution for both cloud and local models (i.e., local models served with vLLM).\nimport asyncio\nfrom onprem.llm.base import LLM\n\nasync def test_async_processing():\n    # Initialize with cloud model\n    llm = LLM(model_url=\"openai/gpt-4o-mini\")\n\n    documents = [\n        \"Python is a high-level programming language known for its simplicity and readability.\",\n        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n        \"Natural language processing helps computers understand and interpret human language in a valuable way.\"\n    ]\n\n    sem = asyncio.Semaphore(4)  # max 4 concurrent requests\n\n    async def summarize(doc):\n        async with sem:\n            print(f\"Processing: {doc[:30]}...\")\n            result = await llm.aprompt(f\"Assign a topic keyphrase to this text: {doc}\")\n            print(f\"Result: {result}\")\n            return result\n\n    print(\"Starting concurrent processing...\")\n    results = await asyncio.gather(*[summarize(doc) for doc in documents])\n\n    print(\"\\nAll results:\")\n    for i, result in enumerate(results, 1):\n        print(f\"{i}. {result}\")\n\n    return results\n\nif __name__ == \"__main__\":\n    results = asyncio.run(test_async_processing())",
    "crumbs": [
      "Examples",
      "Use Prompts to Solve Problems"
    ]
  },
  {
    "objectID": "ingest.stores.base.html",
    "href": "ingest.stores.base.html",
    "title": "ingest.stores.base",
    "section": "",
    "text": "source\n\nVectorStore\n\ndef VectorStore(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.",
    "crumbs": [
      "Source",
      "ingest.stores.base"
    ]
  },
  {
    "objectID": "pipelines.classifier.html",
    "href": "pipelines.classifier.html",
    "title": "pipelines.classifier",
    "section": "",
    "text": "source\n\nClassifierBase\n\ndef ClassifierBase(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nClassifierBase.arrays2dataset\n\ndef arrays2dataset(\n    X:List, y:Union, text_key:str='text', label_key:str='label'\n):\n\nConvert train or test examples to HF dataset\n\nsource\n\n\nClassifierBase.dataset2arrays\n\ndef dataset2arrays(\n    dataset, text_key:str='text', label_key:str='label'\n):\n\nConvert a Hugging Face dataset to X, y arrays\n\nsource\n\n\nClassifierBase.evaluate\n\ndef evaluate(\n    X_eval:list, y_eval:list, print_report:bool=True, labels:list=[], kwargs:VAR_KEYWORD\n):\n\nEvaluates labeled data using the trained model.  If print_report is True, prints classification report and returns nothing. Otherwise, returns and prints a dictionary of the results. Extra kwargs fed to self.predict.\n\nsource\n\n\nClassifierBase.explain\n\ndef explain(\n    X:list, labels:list=[]\n):\n\nExplain the predictions on given examples in X. (Requires shap and matplotlib to be installed.)\n\nsource\n\n\nClassifierBase.sample_examples\n\ndef sample_examples(\n    X:list, y:list, num_samples:int=8, text_key:str='text', label_key:str='label'\n):\n\nSample a dataset with num_samples per class\n\nsource\n\n\nSKClassifier\n\ndef SKClassifier(\n    model_path:NoneType=None, labels:list=[], kwargs:VAR_KEYWORD\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nSKClassifier.train\n\ndef train(\n    X:List, y:Union, kwargs:VAR_KEYWORD\n):\n\nTrains the classifier on a list of texts (X) and a list of labels (y). Additional keyword arguments are passed directly to self.model.fit.\nArgs:\n\nX: List of texts\ny: List representing labels\n\nReturns:\n\nNone\n\n\nsource\n\n\nSKClassifier.predict\n\ndef predict(\n    X, kwargs:VAR_KEYWORD\n):\n\npredict labels\n\nsource\n\n\nSKClassifier.predict_proba\n\ndef predict_proba(\n    X, kwargs:VAR_KEYWORD\n):\n\npredict label probabilities\n\nsource\n\n\nSKClassifier.save\n\ndef save(\n    filename:str\n):\n\nSave model to specified filename (e.g., /tmp/mymodel.gz). Model saved as pickle file. To reload the model, supply model_path when instantiatingSKClassifier.\n\n\nExample: Training Sckit-Learn Text Classification Models\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom onprem.pipelines.classifier import SKClassifier\n\n\ncategories = [\n             \"alt.atheism\",\n             \"soc.religion.christian\",\n             \"comp.graphics\",\n             \"sci.med\" ]\n\ntrain_b = fetch_20newsgroups(\n            subset=\"train\", categories=categories, shuffle=True, random_state=42\n)\ntest_b = fetch_20newsgroups(\nsubset=\"test\", categories=categories, shuffle=True, random_state=42\n)\nx_train = train_b.data\ny_train = train_b.target\nx_test = test_b.data\ny_test = test_b.target\nclasses = train_b.target_names\n\n# y_test = [classes[y] for y in y_test]\n# y_train = [classes[y] for y in y_train]\n\nclf = SKClassifier(labels=classes)\nclf.train(x_train, y_train)\ntest_doc1 = \"Jesus Christ was a first century Jewish teacher and religious leader.\"\ntest_doc2 = \"The graphics on my monitor are terrible.\"\nprint(clf.predict(test_doc1))\nprint(clf.predict([test_doc2]))\nprint(clf.predict([test_doc1, test_doc2]))\nclf.evaluate(x_test, y_test)\n\nsoc.religion.christian\ncomp.graphics\n['soc.religion.christian', 'comp.graphics']\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.93      0.87      0.90       319\n         comp.graphics       0.88      0.96      0.92       389\n               sci.med       0.94      0.84      0.89       396\nsoc.religion.christian       0.91      0.96      0.94       398\n\n              accuracy                           0.91      1502\n             macro avg       0.91      0.91      0.91      1502\n          weighted avg       0.91      0.91      0.91      1502\n\n\n\n\nclf.evaluate(x_test, y_test, print_report=False)['accuracy']\n\n0.9114513981358189\n\n\n\nclf.save('/tmp/mymodel.gz') # save\n\n\nclf = SKClassifier(model_path='/tmp/mymodel.gz', labels=classes) # reload\nclf.evaluate(x_test, y_test)\n\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.93      0.87      0.90       319\n         comp.graphics       0.88      0.96      0.92       389\n               sci.med       0.94      0.84      0.89       396\nsoc.religion.christian       0.91      0.96      0.94       398\n\n              accuracy                           0.91      1502\n             macro avg       0.91      0.91      0.91      1502\n          weighted avg       0.91      0.91      0.91      1502\n\n\n\n\nsource\n\n\nHFClassifier\n\ndef HFClassifier(\n    model_id_or_path:str='google/bert_uncased_L-2_H-128_A-2', device:NoneType=None, labels:list=[],\n    kwargs:VAR_KEYWORD\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nHFClassifier.train\n\ndef train(\n    X:List, y:Union, kwargs:VAR_KEYWORD\n):\n\nTrains the classifier on a list of texts (X) and a list of labels (y). Extra kwargs are treated as arguments to transformers.TrainingArguments.\nArgs:\n\nX: List of texts\ny: List of integers representing labels\nnum_epochs: Number of epochs to train\nbatch_size: Batch size\nmetric: metric to use\ncallbacks: A list of callbacks to customize the training loop.\n\nReturns:\n\nNone\n\n\nsource\n\n\nHFClassifier.predict\n\ndef predict(\n    X:list, max_length:int=512, kwargs:VAR_KEYWORD\n):\n\nPredict labels.  Extra kwargs fed to Hugging Face transformers text-classification pipeline.\n\nsource\n\n\nHFClassifier.predict_proba\n\ndef predict_proba(\n    X:list, max_length:int=512, wargs:VAR_KEYWORD\n):\n\nPredict labels. Extra kwargs fed to Hugging Face transformers text-classification pipeline.\nThe default model is a tiny BERT model (i.e., `google/bert_uncased_L-2_H-128_A-2), but we will use a larger model here to improve accuracy (e.g., distilbert).\n\n\nExample: Training Hugging Face Transformer Models\n\ncategories = [\n             \"alt.atheism\",\n             \"soc.religion.christian\",\n             \"comp.graphics\",\n             \"sci.med\" ]\n\ntrain_b = fetch_20newsgroups(\n            subset=\"train\", categories=categories, shuffle=True, random_state=42\n)\ntest_b = fetch_20newsgroups(\nsubset=\"test\", categories=categories, shuffle=True, random_state=42\n)\nx_train = train_b.data\ny_train = train_b.target\nx_test = test_b.data\ny_test = test_b.target\nclasses = train_b.target_names\n\nclf = HFClassifier(model_id_or_path='distilbert/distilbert-base-uncased', \n                   device='cuda', labels=classes)\nclf.train(x_train, y_train, num_train_epochs=1, per_device_train_batch_size=8)\ntest_doc1 = \"Jesus Christ was a first century Jewish teacher and religious leader.\"\ntest_doc2 = \"The graphics on my monitor are terrible.\"\nprint(clf.predict(test_doc1))\nprint(clf.predict([test_doc2]))\nprint(clf.predict([test_doc1, test_doc2]))\nclf.evaluate(x_test, y_test)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n      \n      \n      [283/283 02:25, Epoch 1/1]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n\n\n\n\nsoc.religion.christian\ncomp.graphics\n['soc.religion.christian', 'comp.graphics']\n                        precision    recall  f1-score   support\n\n           alt.atheism       0.89      0.88      0.89       319\n         comp.graphics       0.97      0.98      0.97       389\n               sci.med       0.97      0.95      0.96       396\nsoc.religion.christian       0.94      0.96      0.95       398\n\n              accuracy                           0.95      1502\n             macro avg       0.94      0.94      0.94      1502\n          weighted avg       0.95      0.95      0.95      1502\n\n\n\n\nclf.explain(test_doc1)\n\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n\n\n\n\n\n[0]\n\n            \n\noutputs\nalt.atheism\ncomp.graphics\nsci.med\nsoc.religion.christian0.50.30.10.70.900base value00falt.atheism(inputs)                inputs0.00.0Jesus 0.0Christ 0.0was 0.0a 0.0first 0.0century 0.0Jewish 0.0teacher 0.0and 0.0religious 0.0leader0.0.0.0000000000000000000000base value00falt.atheism(inputs)                inputs0.00.0Jesus 0.0Christ 0.0was 0.0a 0.0first 0.0century 0.0Jewish 0.0teacher 0.0and 0.0religious 0.0leader0.0.0.00.50.30.10.70.900base value00fcomp.graphics(inputs)                inputs0.00.0Jesus 0.0Christ 0.0was 0.0a 0.0first 0.0century 0.0Jewish 0.0teacher 0.0and 0.0religious 0.0leader0.0.0.0000000000000000000000base value00fcomp.graphics(inputs)                inputs0.00.0Jesus 0.0Christ 0.0was 0.0a 0.0first 0.0century 0.0Jewish 0.0teacher 0.0and 0.0religious 0.0leader0.0.0.00.50.30.10.70.900base value00fsci.med(inputs)                inputs0.00.0Jesus 0.0Christ 0.0was 0.0a 0.0first 0.0century 0.0Jewish 0.0teacher 0.0and 0.0religious 0.0leader0.0.0.0000000000000000000000base value00fsci.med(inputs)                inputs0.00.0Jesus 0.0Christ 0.0was 0.0a 0.0first 0.0century 0.0Jewish 0.0teacher 0.0and 0.0religious 0.0leader0.0.0.00.50.30.10.70.90.3567670.356767base value0.9782940.978294fsoc.religion.christian(inputs)0.192      Christ  0.129      Jesus  0.122      religious  0.062      Jewish  0.058      teacher  0.022      century  0.018      was  0.017      first  0.012      leader  0.003      .  0.0      and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -0.012      a          inputs0.00.129Jesus 0.192Christ 0.018was -0.012a 0.017first 0.022century 0.062Jewish 0.058teacher 0.0and 0.122religious 0.012leader0.003.0.00.70.60.50.40.80.90.3567670.356767base value0.9782940.978294fsoc.religion.christian(inputs)0.192      Christ  0.129      Jesus  0.122      religious  0.062      Jewish  0.058      teacher  0.022      century  0.018      was  0.017      first  0.012      leader  0.003      .  0.0      and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -0.012      a          inputs0.00.129Jesus 0.192Christ 0.018was -0.012a 0.017first 0.022century 0.062Jewish 0.058teacher 0.0and 0.122religious 0.012leader0.003.0.0\n\n\n\nclf.save('/tmp/my_hf_model')\n\n\nclf = HFClassifier('/tmp/my_hf_model', device='cuda', labels=classes)\nclf.evaluate(x_test,  y_test, print_report=False)['accuracy']\n\n0.9454061251664447\n\n\n\nsource\n\n\nFewShotClassifier\n\ndef FewShotClassifier(\n    model_id_or_path:str='sentence-transformers/paraphrase-mpnet-base-v2', use_smaller:bool=False,\n    kwargs:VAR_KEYWORD\n):\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nFewShotClassifier.train\n\ndef train(\n    X:List, y:Union, num_epochs:int=10, batch_size:int=32, metric:str='accuracy', callbacks:NoneType=None,\n    kwargs:VAR_KEYWORD\n):\n\nTrains the classifier on a list of texts (X) and a list of labels (y). Additional keyword arguments are passed directly to SetFit.TrainingArguments\nArgs:\n\nX: List of texts\ny: List of integers representing labels\nnum_epochs: Number of epochs to train\nbatch_size: Batch size\nmetric: metric to use\ncallbacks: A list of callbacks to customize the training loop.\n\nReturns:\n\nNone\n\n\nsource\n\n\nFewShotClassifier.predict\n\ndef predict(\n    X, kwargs:VAR_KEYWORD\n):\n\npredict labels\n\nsource\n\n\nFewShotClassifier.predict_proba\n\ndef predict_proba(\n    X, kwargs:VAR_KEYWORD\n):\n\npredict label probabilities\n\nsource\n\n\nFewShotClassifier.save\n\ndef save(\n    save_path:str\n):\n\nSave model to specified folder path, save_path. To reload the model, supply path in model_id_or_path argument when instantiatingFewShotClassifier.\n\n\nExample: Training Few-Shot Text Classifiers\n\nclf = FewShotClassifier(labels=['negative', 'positive'])\n\nmodel_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n\n\n\nfrom datasets import load_dataset\n\nSample a tiny dataset with only 8 examples per class (or 16 total examples):\n\ndataset = load_dataset(\"SetFit/sst2\")\nX_train, y_train = clf.dataset2arrays(dataset[\"train\"], text_key=\"text\", label_key=\"label\")\nX_test, y_test = clf.dataset2arrays(dataset[\"test\"], text_key=\"text\", label_key=\"label\")\nX_sample, y_sample = clf.sample_examples(X_train,  y_train, label_key=\"label\", num_samples=8)\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\n\nclf.train(X_sample,  y_sample, max_steps=50)\n\nApplying column mapping to the training dataset\n\n\n\n\n\n***** Running training *****\n  Num unique pairs = 144\n  Batch size = 32\n  Num epochs = 10\n\n\n\n      \n      \n      [50/50 00:28, Epoch 10/10]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n1\n0.242700\n\n\n50\n0.047300\n\n\n\n\n\n\n\n\n\n\nclf.evaluate(X_test, y_test)\n\n              precision    recall  f1-score   support\n\n    negative       0.88      0.94      0.91       912\n    positive       0.93      0.87      0.90       909\n\n    accuracy                           0.91      1821\n   macro avg       0.91      0.91      0.91      1821\nweighted avg       0.91      0.91      0.91      1821\n\n\n\n\nnew_data = [\"i loved the spiderman movie!\", \"pineapple on pizza is the worst 🤮\"]\n\n\npreds = clf.predict(new_data)\npreds\n\n['positive', 'negative']\n\n\n\npreds = clf.predict_proba(new_data)\npreds\n\ntensor([[0.1657, 0.8343],\n        [0.8551, 0.1449]], dtype=torch.float64)\n\n\n\nclf.save('/tmp/my_fewshot_model')\n\n\nclf = FewShotClassifier('/tmp/my_fewshot_model', labels=['negative', 'positive'])\npreds = clf.predict(new_data)\npreds\n\n['positive', 'negative']\n\n\n\nclf.explain(new_data)\n\n\n\n\n[0]\n\n            \n\noutputs\nnegative\npositive0.50.30.1-0.10.70.91.10.6836380.683638base value0.1657120.165712fnegative(inputs)0.069      movie  0.017      spider  0.006      man  0.0                                                                                                                                                                                                                                        -0.292      loved  -0.249      !  -0.067      the  -0.002      i  -0.0                                                                                                                                                                                                                                                                                                                inputs-0.0-0.002i -0.292loved -0.067the 0.017spider0.006man 0.069movie-0.249!0.00.40.30.20.10.50.60.70.6836380.683638base value0.1657120.165712fnegative(inputs)0.069      movie  0.017      spider  0.006      man  0.0                                                                                                                                                                                                                                        -0.292      loved  -0.249      !  -0.067      the  -0.002      i  -0.0                                                                                                                                                                                                                                                                                                                inputs-0.0-0.002i -0.292loved -0.067the 0.017spider0.006man 0.069movie-0.249!0.00.50.30.1-0.10.70.91.10.3163620.316362base value0.8342880.834288fpositive(inputs)0.292      loved  0.249      !  0.067      the  0.002      i  0.0                                                                                                                                                                                                                                                                                                                -0.069      movie  -0.017      spider  -0.006      man  -0.0                                                                                                                                                                                                                                        inputs0.00.002i 0.292loved 0.067the -0.017spider-0.006man -0.069movie0.249!-0.00.60.50.40.30.70.80.90.3163620.316362base value0.8342880.834288fpositive(inputs)0.292      loved  0.249      !  0.067      the  0.002      i  0.0                                                                                                                                                                                                                                                                                                                -0.069      movie  -0.017      spider  -0.006      man  -0.0                                                                                                                                                                                                                                        inputs0.00.002i 0.292loved 0.067the -0.017spider-0.006man -0.069movie0.249!-0.0\n\n\n[1]\n\n            \n\noutputs\nnegative\npositive0.50.30.1-0.10.70.91.10.651280.65128base value0.8551250.855125fnegative(inputs)0.53      worst  0.068      on                                                                                  -0.085      pizza  -0.07      is  -0.065      apple  -0.063      pine  -0.061      the  -0.05      🤮  -0.0        -0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        inputs-0.0-0.063pine-0.065apple 0.068on -0.085pizza -0.07is -0.061the 0.53worst -0.05🤮-0.00.80.60.411.20.651280.65128base value0.8551250.855125fnegative(inputs)0.53      worst  0.068      on                                                                                  -0.085      pizza  -0.07      is  -0.065      apple  -0.063      pine  -0.061      the  -0.05      🤮  -0.0        -0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        inputs-0.0-0.063pine-0.065apple 0.068on -0.085pizza -0.07is -0.061the 0.53worst -0.05🤮-0.00.50.30.1-0.10.70.91.10.348720.34872base value0.1448750.144875fpositive(inputs)0.085      pizza  0.07      is  0.065      apple  0.063      pine  0.061      the  0.05      🤮  0.0        0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -0.53      worst  -0.068      on                                                                                  inputs0.00.063pine0.065apple -0.068on 0.085pizza 0.07is 0.061the -0.53worst 0.05🤮0.00.20-0.20.40.60.348720.34872base value0.1448750.144875fpositive(inputs)0.085      pizza  0.07      is  0.065      apple  0.063      pine  0.061      the  0.05      🤮  0.0        0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -0.53      worst  -0.068      on                                                                                  inputs0.00.063pine0.065apple -0.068on 0.085pizza 0.07is 0.061the -0.53worst 0.05🤮0.0",
    "crumbs": [
      "Source",
      "pipelines.classifier"
    ]
  },
  {
    "objectID": "ingest.stores.sparse.html",
    "href": "ingest.stores.sparse.html",
    "title": "ingest.stores.sparse",
    "section": "",
    "text": "source\n\nSparseStore\n\ndef SparseStore(\n    kwargs:VAR_KEYWORD\n):\n\nA factory for built-in SparseStore instances.\n\nsource\n\n\nSparseStore.create\n\ndef create(\n    persist_location:NoneType=None, kind:NoneType=None, kwargs:VAR_KEYWORD\n)-&gt;SparseStore:\n\nFactory method to construct a SparseStore instance.  Extra kwargs passed to object instantiation.\nArgs: persist_location: where the index is stored (for whoosh) or Elasticsearch URL (for elasticsearch) kind: one of {whoosh, elasticsearch}\nElasticsearch-specific kwargs: basic_auth: tuple of (username, password) for basic authentication verify_certs: whether to verify SSL certificates (default: True) ca_certs: path to CA certificate file timeout: connection timeout in seconds (default: 30, becomes request_timeout for v9+ compatibility) max_retries: maximum number of retries (default: 3) retry_on_timeout: whether to retry on timeout (default: True) maxsize: maximum number of connections in the pool (default: 25, removed for Elasticsearch v9+ compatibility)\nReturns: SparseStore instance\n\nsource\n\n\nSparseStore.semantic_search\n\ndef semantic_search(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nAny subclass of SparseStore can inherit this method for on-the-fly semantic searches. Retrieves results based on semantic similarity to supplied query. All arguments are fowrwarded on to the store’s search method. The search method is expected to include “query” as first positional argument and a “limit” keyword argument. Additional kwargs can be supplied to focus the search (e.g., see where_document and filters arguments of search method). Results of invoked search method are expected to be in the form: {‘hits’: list_of_dicts, ‘total_hits’ : int}. Result of this method is a list of LangChain Document objects sorted by semantic similarity.\nIf subclass supports dynamic chunking (has chunk_for_semantic_search=True), it will chunk large documents and find the best matching chunks per document.\nArgs: return_chunks (bool): If True (default), return individual chunks as Document objects for RAG. If False, return original documents with full content and all chunk scores. load_web_documents (bool): If True, attempt to load content from web URLs when content field is empty (default: False). verbose (bool): If True (default), show progress bar during semantic search processing.\n\nsource\n\n\nReadOnlySparseStore\n\ndef ReadOnlySparseStore(\n    kwargs:VAR_KEYWORD\n):\n\nA sparse vector store based on a read-only full-text search engine\n\nsource\n\n\nSharePointStore\n\ndef SharePointStore(\n    persist_location:str, # SharePoint URL\n    username:str, # user name (e.g., CORP\\my_username)\n    password:str, # your SharePoint password\n    chunk_for_semantic_search:bool=True, # whether to do dynamic chunking\n    chunk_size:int=500, # chunk size in characters (1 word ~ 4 characters)\n    chunk_overlap:int=50, # chunk overlap between chunks\n    n_candidates:Optional=None, # number of candidate documents to consider for answer. Defaults to limit*10.\n    load_web_documents:bool=True, # whether to load content from web URLs when content field is empty\n    kwargs:VAR_KEYWORD\n):\n\nA sparse vector store based on Microsoft Sharepoint using a “vectors-on-demand” approach.\n\nsource\n\n\nElasticsearchSparseStore\n\ndef ElasticsearchSparseStore(\n    persist_location:Optional=None, index_name:str='myindex', basic_auth:Optional=None, verify_certs:bool=True,\n    ca_certs:Optional=None, timeout:int=30, max_retries:int=3, retry_on_timeout:bool=True, maxsize:int=25,\n    content_field:str='page_content', # Field mapping parameters for existing indices\n    source_field:Optional='source', id_field:str='id', content_analyzer:str='standard',\n    chunk_for_semantic_search:bool=False, # Dynamic chunking for semantic search\n    chunk_size:int=500, chunk_overlap:int=50, n_candidates:Optional=None, kwargs:VAR_KEYWORD\n):\n\nA sparse vector store based on Elasticsearch.\n\nsource\n\n\nWhooshStore\n\ndef WhooshStore(\n    persist_location:Optional=None, index_name:str='myindex',\n    chunk_for_semantic_search:bool=False, # Dynamic chunking for semantic search\n    chunk_size:int=500, chunk_overlap:int=50, kwargs:VAR_KEYWORD\n):\n\nA sparse vector store based on the Whoosh full-text search engine.\n\nsource\n\n\nget_field_analyzer\n\ndef get_field_analyzer(\n    field_name, value, schema:NoneType=None\n):\n\nGet the analyzer that would be used for a field.\n\nsource\n\n\ncreate_field_for_value\n\ndef create_field_for_value(\n    field_name, value\n):\n\nCreate field definition based on value type - same logic as add_documents.\n\nsource\n\n\ndefault_schema\n\ndef default_schema(\n    \n):\n\n\nsource\n\n\nWhooshStore.exists\n\ndef exists(\n    \n):\n\nReturns True if documents have been added to search index\n\nsource\n\n\nWhooshStore.add_documents\n\ndef add_documents(\n    docs:Sequence, # list of LangChain Documents\n    limitmb:int=1024, # maximum memory in  megabytes to use\n    verbose:bool=True, # Set to False to disable progress bar\n    kwargs:VAR_KEYWORD\n):\n\nIndexes documents. Extra kwargs supplied to TextStore.ix.writer.\n\nsource\n\n\nWhooshStore.remove_document\n\ndef remove_document(\n    value:str, field:str='id', kwargs:VAR_KEYWORD\n):\n\nRemove document with corresponding value and field. Default field is the id field.\n\nsource\n\n\nWhooshStore.remove_source\n\ndef remove_source(\n    source:str\n):\n\nremove all documents associated with source. The source argument can either be the full path to document or a parent folder. In the latter case, ALL documents in parent folder will be removed.\n\nsource\n\n\nWhooshStore.update_documents\n\ndef update_documents(\n    doc_dicts:List, # list of dictionaries with keys 'page_content', 'source', 'id', etc.\n    kwargs:VAR_KEYWORD\n):\n\nUpdate a set of documents (doc in index with same ID will be over-written)\n\nsource\n\n\nWhooshStore.get_all_docs\n\ndef get_all_docs(\n    \n):\n\nReturns a generator to iterate through all indexed documents\n\nsource\n\n\nWhooshStore.get_doc\n\ndef get_doc(\n    id:str\n):\n\nGet an indexed record by ID\n\nsource\n\n\nWhooshStore.get_size\n\ndef get_size(\n    include_deleted:bool=False\n)-&gt;int:\n\nGets size of index\nIf include_deleted is True, will include deletd detects (prior to optimization).\n\nsource\n\n\nWhooshStore.erase\n\ndef erase(\n    confirm:bool=True\n):\n\nClears index\n\nsource\n\n\nVectorStore.query\n\ndef query(\n    query:str, kwargs:VAR_KEYWORD\n):\n\nGeneric query method that invokes the store’s search method. This provides a consistent interface across all store types.\n\nsource\n\n\nSparseStore.semantic_search\n\ndef semantic_search(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nAny subclass of SparseStore can inherit this method for on-the-fly semantic searches. Retrieves results based on semantic similarity to supplied query. All arguments are fowrwarded on to the store’s search method. The search method is expected to include “query” as first positional argument and a “limit” keyword argument. Additional kwargs can be supplied to focus the search (e.g., see where_document and filters arguments of search method). Results of invoked search method are expected to be in the form: {‘hits’: list_of_dicts, ‘total_hits’ : int}. Result of this method is a list of LangChain Document objects sorted by semantic similarity.\nIf subclass supports dynamic chunking (has chunk_for_semantic_search=True), it will chunk large documents and find the best matching chunks per document.\nArgs: return_chunks (bool): If True (default), return individual chunks as Document objects for RAG. If False, return original documents with full content and all chunk scores. load_web_documents (bool): If True, attempt to load content from web URLs when content field is empty (default: False). verbose (bool): If True (default), show progress bar during semantic search processing.\n\nsource\n\n\nVectorStore.check\n\ndef check(\n    \n):\n\nRaise exception if VectorStore.exists() returns False\n\nsource\n\n\nVectorStore.ingest\n\ndef ingest(\n    source_directory:str, # path to folder containing document store\n    chunk_size:int=1000, # text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n    chunk_overlap:int=100, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n    ignore_fn:Optional=None, # Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested.\n    batch_size:int=41000, # batch size used when processing documents\n    kwargs:VAR_KEYWORD\n)-&gt;None:\n\nIngests all documents in source_directory (previously-ingested documents are ignored). When retrieved, the Document objects will each have a metadata dict with the absolute path to the file in metadata[\"source\"]. Extra kwargs fed to ingest.load_single_document.",
    "crumbs": [
      "Source",
      "ingest.stores.sparse"
    ]
  },
  {
    "objectID": "examples_qualitative_survey_analysis.html",
    "href": "examples_qualitative_survey_analysis.html",
    "title": "Qualitative Survey Analysis",
    "section": "",
    "text": "This notebook covers using generative AI for tasks common in the social sciences. Qualitative survey analysis focuses on understanding the meaning and context behind survey responses, seeking to identify themes, patterns, and insights from open-ended questions and textual data, rather than numerical data.\nIn this tutorial we will use generative AI to analyze open-ended survey responses using OnPrem.LLM, an open-source document intelligence toolkit.\nMore specifically, we will use a local large language model (LLM) to analyze open-ended responses to the OSTP RFI on the future of AI. Public commentary was requested with respect to 10 topic areas. We will auto-code each responses into one of the following topic areas.\ncategories = [\n          'LEGAL',                # Legal and Governance implications of AI\n          'PUBLIC GOOD',          # AI for public good\n          'SAFETY',               # AI Safety\n          'ECONOMIC',             # Economic and Societal Impact of AI\n          'RESEARCH QUESTIONS',   # AI Research Questions\n          'RESEARCH GAPS',        # AI Research Gaps\n          'TRAINING',             # AI Training and Education\n          'MULTIDISCIPLINARY',    # Multidisciplinary Aspects of AI\n          'DATA',                 # Training Data Issues in AI\n          'MARKET_SHAPING',       # Role of Market Shaping in AI\n          'NA'                    # a miscellaneous category that we added\n         ]\nfrom onprem import LLM\nfrom onprem.pipelines import Summarizer\nfrom onprem.ingest import load_single_document\nfrom onprem import utils as U\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom textwrap import wrap\n\nimport pandas as pd\nimport re\n\n\npd.set_option('display.max_colwidth', None)",
    "crumbs": [
      "Examples",
      "Qualitative Survey Analysis"
    ]
  },
  {
    "objectID": "examples_qualitative_survey_analysis.html#step-1-download-the-data",
    "href": "examples_qualitative_survey_analysis.html#step-1-download-the-data",
    "title": "Qualitative Survey Analysis",
    "section": "STEP 1: Download the Data",
    "text": "STEP 1: Download the Data\nWe will first downlod a 349-page PDF containing all responses from obamawhitehouse.archives.gov.\n\nU.download('https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/OSTP-AI-RFI-Responses.pdf',\n            '/tmp/responses.pdf', verify=True)\n\n[██████████████████████████████████████████████████]",
    "crumbs": [
      "Examples",
      "Qualitative Survey Analysis"
    ]
  },
  {
    "objectID": "examples_qualitative_survey_analysis.html#step-2-chunk-data",
    "href": "examples_qualitative_survey_analysis.html#step-2-chunk-data",
    "title": "Qualitative Survey Analysis",
    "section": "STEP 2: Chunk Data",
    "text": "STEP 2: Chunk Data\nWe’ll treat each paragraph in the PDF as a separate “response” to code by topic area. There are 1900+ paragraphs.\n\ndocs = load_single_document('/tmp/responses.pdf')\n\n\ntext = '\\n'.join([d.page_content for d in docs])\n\n\nparagraphs = U.segment(text, unit='paragraph')\n\n\nlen(paragraphs)\n\n1911\n\n\nNOTE:\nParagraph-chunking is used here for illustrative purposes only and can be replaced by alternative chunking methods.\nGiven how the RFI responses are structured, chunking by paragraph for this dataset might be considered a comparatively more error-prone approach, as individual passages delimited by two or more newline characters may not always be valid paragraphs and/or may not contain enough context for the proper classification.\nA better approach for this dataset might be to use methods like semantic chunking, which splits chunks by semantic similarity. To use semantic chunking, you can replace the code above with use of libraries like Chonkie.",
    "crumbs": [
      "Examples",
      "Qualitative Survey Analysis"
    ]
  },
  {
    "objectID": "examples_qualitative_survey_analysis.html#step-3-run-the-analysis-using-an-llm",
    "href": "examples_qualitative_survey_analysis.html#step-3-run-the-analysis-using-an-llm",
    "title": "Qualitative Survey Analysis",
    "section": "STEP 3: Run the Analysis Using an LLM",
    "text": "STEP 3: Run the Analysis Using an LLM\nNext, we will execute the prompt below against each response in the PDF. We will load and run a quantized version of Llama 3.1. GPU accelearation (which we enable with n_gpu_layers=-1) is imporant to produce results faster.\n\nllm = LLM(default_model='llama', mute_stream=True, verbose=False, n_gpu_layers=-1, temperature=0)\n\nllama_init_from_model: n_ctx_per_seq (3904) &lt; n_ctx_train (131072) -- the full capacity of the model will not be utilized\n\n\n\nprompt = \"\"\"Here are 11 topics and their descriptions:\n1. LEGAL: The legal and governance implications of AI\n2. PUBLIC GOOD: the use of AI for public good\n3. SAFETY: the safety and control issues for AI\n4. ECONOMIC: the social and economic implications of AI\n5. RESEARCH QUESTIONS: the most pressing, fundamental questions in AI research, common to most or all scientific fields; \n6. RESEARCH GAPS: the most important research gaps in AI that must be addressed to advance this field and benefit the public;\n7. TRAINING: the scientific and technical training that will be needed to take advantage of harnessing the potential of AI technology, and the challenges faced by institutions of higher education in retaining faculty and responding to explosive growth in student enrollment in AI-related courses and courses of study;\n8. MULTIDISCIPLINARY:  the specific steps that could be taken by the federal government, research institutes, universities, and philanthropies to encourage multi-disciplinary AI research; \n9. DATA: specific training data sets that can accelerate the development of AI and its application; \n10. MARKET_SHAPING: the role that “market shaping” approaches such as incentive prizes and Advanced\nMarket Commitments can play in accelerating the development of applications of AI to\naddress societal needs, such as accelerated training for low and moderate income workers\n11. NA: text does not fit into any of the above topics\n\nCategorize the following text in exactly one of the above  topics. Output only the topic and nothing else\n\n# Example 1:\nTEXT: \nAI is not trustworthy because we don't understand how it works.\n\nTOPIC: SAFETY: the safety and control issues for AI\n\n# Example 2:\nTEXT: \n{text}\n\nTOPIC:\"\"\"\n\n\ntest_input = \"\"\"\nAt the center of medical practice is the act of inference, or reaching a conclusion on the basis\nof evidence and reasoning. Doctors and nurses learn to map patients’ symptoms, lifestyles\nand metadata to a diagnosis of their condition.\n\nAny mathematical function is simply a way of mapping input variables to an output; that is,\ninference is also at the heart of AI. The promise of AI in public health is to serve as a\nautomated second opinion for healthcare professionals; it has the ability to check them\nwhen they slip.\n\"\"\"\n\n\noutput = llm.prompt(U.format_string(prompt, text=test_input))\nprint(output)\n\nPUBLIC GOOD: the use of AI for public good\n\n\n\npredictions = []\nparagraphs = paragraphs[2:] # first two \"paragraphs\" are boiler-plate headings\nfor paragraph in tqdm(paragraphs, total=len(paragraphs)):\n    output = llm.prompt(U.format_string(prompt, text=paragraph))\n    predictions.append(output)\n\n100%|███████████████████████████████████████████████████████████████████████████████| 1909/1909 [10:24&lt;00:00,  3.06it/s]",
    "crumbs": [
      "Examples",
      "Qualitative Survey Analysis"
    ]
  },
  {
    "objectID": "examples_qualitative_survey_analysis.html#step-4-clean-the-llm-output",
    "href": "examples_qualitative_survey_analysis.html#step-4-clean-the-llm-output",
    "title": "Qualitative Survey Analysis",
    "section": "STEP 4: Clean the LLM Output",
    "text": "STEP 4: Clean the LLM Output\nThe LLM sometimes makes mistakes (e.g., outputs the number of the topic instead of the name of the topic, etc.). Options to correct are to change the prompt, change the model, and post-process (or clean) the results. To avoid running the analysis again, we will simply clean the LLM’s output.\n\n# Filter out texts shorter than 8 characters and corresponding preds\nfiltered_texts, filtered_preds = zip(*[(t, p) for t, p in zip(paragraphs, predictions) if len(t) &gt;= 32])\n\n# Convert to lists if needed\nfiltered_texts = list(filtered_texts)\n\n\nlabels = []\ndescriptions = []\nNAs = []\ncat_map = dict([(str(i+1), c) for i, c in enumerate(categories)])\nfor i, p in enumerate(filtered_preds):\n    parts = p.split(\":\")\n    label = 'NA'\n    desc = 'NA'\n    if len(parts)&lt;2:\n        label = p\n        desc = p\n    else:\n        label = parts[0].strip()\n        desc = parts[1].strip()\n    for cat in categories:\n        if cat in label:\n            label = cat\n            break\n    if label not in categories:\n        if label.strip().endswith('.'):\n            label = label[:-1].strip()\n        if label in cat_map:\n            label = cat_map[label]\n    labels.append(label)\n    descriptions.append(desc)",
    "crumbs": [
      "Examples",
      "Qualitative Survey Analysis"
    ]
  },
  {
    "objectID": "examples_qualitative_survey_analysis.html#step-5-examine-and-plot-the-results",
    "href": "examples_qualitative_survey_analysis.html#step-5-examine-and-plot-the-results",
    "title": "Qualitative Survey Analysis",
    "section": "STEP 5: Examine and Plot the Results",
    "text": "STEP 5: Examine and Plot the Results\nWe will store the results in a Dataframe, which can be used to both spot-check and plot results.\n\ndf = pd.DataFrame(\n    {'topic': labels,\n     'description': descriptions,\n     'response': filtered_texts\n    })\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n0\nPUBLIC GOOD\nthe use of AI for public good\nPublic Responses September 1, 2016 Respondent 1 Chris Nicholson, Skymind Inc. This submission will address topics 1, 2, 4 and 10 in the OSTP’s RFI: • the legal and governance implications of AI • the use of AI for public good • the social and economic implications of AI • the role of “market-shaping” approaches Governance, anomaly detection and urban systems\n\n\n1\nLEGAL\nThe legal and governance implications of AI\nThe fundamental task in the governance of urban systems is to keep them running; that is, to maintain the fluid movement of people, goods, vehicles and information throughout the system, without which it ceases to function.\n\n\n2\nSAFETY\nthe safety and control issues for AI\nBreakdowns in the functioning of these systems and their constituent parts are therefore of great interest, whether it be their energy, transport, security or information infrastructures. Those breakdowns may result from deteriorations in the physical plant, sudden and unanticipated overloads, natural disasters or adversarial behavior.\n\n\n3\nPUBLIC GOOD\nthe use of AI for public good\nIn many cases, municipal governments possess historical data about those breakdowns and the events that precede them, in the form of activity and sensor logs, video, and internal or public communications. Where they don’t possess such data already, it can be gathered.\n\n\n4\nDATA\nspecific training data sets that can accelerate the development of AI and its application\nSuch datasets are a tremendous help when applying learning algorithms to predict breakdowns and system failures. With enough lead time, those predictions make pre- emptive action possible, action that would cost cities much less than recovery efforts in the wake of a disaster. Our choice is between an ounce of prevention or a pound of cure.\n\n\n\n\n\n\n\n\nax = sns.countplot(x=\"topic\",data=df, order = df['topic'].value_counts().index)\nax.tick_params(axis='x', rotation=90)\n\n\n\n\n\n\n\n\nLet’s spot-check the responses coded as NA to ensure they’re really not relevant to the topics of interest in this study.\n\ndf[df.topic == 'NA'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n31\nNA\ntext does not fit into any of the above topics\nPlease consider a dybalic aet if rules that value, above all, the value of human life.\n\n\n41\nNA\ntext does not fit into any of the above topics\nwill create the possibility of saving our brains on computers. I found this in manuscripts, this is the “Budapest Kabbalah Theory of the Nineteen Twenties”. It sounds an innovation, but is very simple. 4. There are Cycles. In each Hundred Years we only look at two Constellations – Jubilee Years from the Bible - from among the Kings who hav had an impact on our Ancestors. This is the basic idea of the List I have found. It is based on the Biblical 50 years the Yobel Cycles. 5. Hence we do have four Decades (with 12 ys). There exist statistics about Bank-Cycles and also about the psychological differences of the Four Phases. This psychological cycle theory was presented in the Eighties by Lloyd deMause, who analyzed the differing cartoon motives of the four different years of American presidents. He found archtypes – Body Parts, Family Members – like Jung and Bowlby in their theories in the 1930s and 1950s. And this can be projected to the Four Decades of a messianic Yobel Year, of the 45-50 years (when price cycles restart five years before the fulfillment of the half century). 6.To further filter the countless events in history, we only look at religious fightings: non-Jewish David-House legends in Kingly families (like the Habsburgs, Bourbons who have had Hungarian-Polish Szapolyai ancestors (these are 2 facts that are not wel-known) 7. It is also not well-known, that in every generation there are Jewish Rabbis, who are considered to stem from the Davidic Line – potential Meshiahs. 8. There are weekly melodies in the Bible (also not well-known, ut each religion has it – but only the Jewish melodies are present everywhere in Europe without two much change. Because of this these therapeutic melodies can be found in the differenet traumatic Ancestral Stress Dates. It is a simple fact. 9. These melodies can be found in stress- contemporary non-Jewish music too. This is needed for the ancestrally relevant non-Jewish Kings might not have heard synagogue melodies. (Although\n\n\n81\nNA\nNA\nconversational AI software platform.\n\n\n93\nNA\nNA\nRoger C Schank Socratic Arts Inc\n\n\n122\nNA\nNA\n(11) any additional information related to AI research or policymaking, not requested above, that you believe OSTP should consider.\n\n\n\n\n\n\n\nLet’s spot-check some of the other topic areas to ensure the are relevant to the various topic areas. The topic and description columns were automatically populated by the LLM based on the text content of the response.\n\ndf[df.topic == 'PUBLIC GOOD'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n0\nPUBLIC GOOD\nthe use of AI for public good\nPublic Responses September 1, 2016 Respondent 1 Chris Nicholson, Skymind Inc. This submission will address topics 1, 2, 4 and 10 in the OSTP’s RFI: • the legal and governance implications of AI • the use of AI for public good • the social and economic implications of AI • the role of “market-shaping” approaches Governance, anomaly detection and urban systems\n\n\n3\nPUBLIC GOOD\nthe use of AI for public good\nIn many cases, municipal governments possess historical data about those breakdowns and the events that precede them, in the form of activity and sensor logs, video, and internal or public communications. Where they don’t possess such data already, it can be gathered.\n\n\n7\nPUBLIC GOOD\nthe use of AI for public good\nThat is, neither the public nor the private sectors have the analysts necessary to process all the data generated by our cities, and we cannot rely on hard-coded rules to automate the analyses and tell us when things are going wrong (send a notification when more than X number of white vans cross Y bridge), because the nature of events often changes faster than new hard-coded rules can be written.\n\n\n8\nPUBLIC GOOD\nthe use of AI for public good\nOne of the great applications of deep artificial neural networks, the algorithms responsible for many recent advances in artificial intelligence, is anomaly detection. Exposed to large datasets, those neural networks are capable of understanding and modeling normal behavior – reconstructing what should happen – and therefore of identifying outliers and anomalies. They do so without hard-coded rules, and the anomalies they detect can occur across multiple dimensions, changing from day to day as the neural nets are exposed to more data.\n\n\n9\nPUBLIC GOOD\nthe use of AI for public good\nThat is, deep neural networks can perform anomaly detection that keeps pace with rapidly changing patterns in the real world. This capacity to detect new anomalies is causing a shift in fraud detection practices in financial services, and cybersecurity in data centers; it is equally relevant to the governance of urban systems.\n\n\n\n\n\n\n\n\ndf[df.topic == 'ECONOMIC'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n14\nECONOMIC\nthe social and economic implications of AI\nBecause an algorithm can be trained on many more instances of data – say, X-rays of cancer patients – than a healthcare professional can be exposed to in a single lifetime, an algorithm may perceive signals, subtle signs of a tumor, that a human would overlook.\n\n\n21\nECONOMIC\nthe social and economic implications of AI\nThe social and economic implications of AI\n\n\n22\nECONOMIC\nthe social and economic implications of AI\nAs AI advances and its breakthroughs are implemented by large organizations more widely, its impact on society will grow. The scale of that impact may well rival the steam engine or electricity.\n\n\n23\nECONOMIC\nthe social and economic implications of AI\nOn the one hand, we will more efficiently and accurately process information in ways that help individuals and society; on the other, the labor market will be affected, skill sets will be made obsolete, and power and wealth will further shift to those best able to collect, interpret and act on massive amounts of data quickly.\n\n\n24\nECONOMIC\nthe social and economic implications of AI\nDeep technological changes will throw people out of work, reshape communities, and alter the way society behaves, connects and communicates collectively. The automation of trucking through driverless vehicles, for example, will affect America’s 3.5 million truckers and the more than 5 million auxiliary positions related to trucking. The same can be said for taxis, delivery and ride-haling services.\n\n\n\n\n\n\n\n\ndf[df.topic == 'SAFETY'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n2\nSAFETY\nthe safety and control issues for AI\nBreakdowns in the functioning of these systems and their constituent parts are therefore of great interest, whether it be their energy, transport, security or information infrastructures. Those breakdowns may result from deteriorations in the physical plant, sudden and unanticipated overloads, natural disasters or adversarial behavior.\n\n\n28\nSAFETY\nthe safety and control issues for AI\nproblem with Ai, if a psychotic person designs Ai, the flaws of said human could be passed along. digital-epigenetics. When the man in the mirror no longer smiles, the Ai on the inside told it too, not its body. Give Ai the same limitations as man, a bird, a chameleon, and it becomes them like water in the glass. So do not design your nightmares or they will turn to terrors. Respondent 8 D F, N/A Use Clarke's three laws! Respondent 9 Christopher Brouns, Citizen Haven't any of you seen Terminator? There may not be a time traveling robot To come hunt us down but there will be plenty of real time killing machines loosed on the masses because of the irresistible pull of geopolitical power. As soon as robots become self aware its over for us. They will only work for their own survival, dominance and propagation. Our own AI programs are routinely mopping the floor with our best fighter pilots in test scenarios. Thats some incredibly advanced programming right there. Then imagine that machine understanding how it can be turned off at a moments notice and not wanting that to happen. Our goose will be cooked. If AI leads to sentience, we are, to put it quiet simply, screwed. Respondent 10 Seth Miller, ZBMC I, for one, welcome our new arificial overlords. Respondent 11 Colin Colby, Unemployed We should ask the AI these questions. :3 Respondent 12 Harshit Bhatt, Student It is better to phase in A.I into human life in steps (which I consider you guys are already doing). It has been a nice strategy that we have pre-emptively diffused awareness about A.I to public that it should not be a shock for people to see a driverless car on the street someday next to them. It is important to brief people about the progress in A.I and how it could affect our life in succession of brief details, otherwise there would be a mass hysterical chaotic response if for e.g., one sees a robot jogging down the streets and greeting them. Respondent 13 Harshit Bhatt, Student It is better to phase\n\n\n29\nSAFETY\nthe safety and control issues for AI\nin A.I into human life in steps (which I consider you guys are already doing). It has been a nice strategy that we have pre-emptively diffused awareness about A.I to public that it should not be a shock for people to see a driverless car on the street someday next to them. It is important to brief people about the progress in A.I and how it could affect our life in succession of brief details, otherwise there would be a mass hysterical chaotic response if for e.g., one sees a robot jogging down the streets and greeting them. Respondent 14 Parham Sepehri, None Proprietary AI is very dangerous for democracy. Given the rate of advancement of computer tech, AI can quickly become overwhelming for gov to regulate. Our laws have no protection against the negative effects of super intelligence in hands of a few.\n\n\n30\nSAFETY\nthe safety and control issues for AI\nAI may be useful for war fare initially, but soon human life will lose its significance to the few that control the AI.\n\n\n36\nSAFETY\nthe safety and control issues for AI\nAssume that an AI has the same potential for prosperity or destruction as a human person does and judge it accordingly. We will have many “frankenstein's monsters” but keeping a consistent judgement is crucial for this.\n\n\n\n\n\n\n\n\ndf[df.topic == 'RESEARCH QUESTIONS'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n5\nRESEARCH QUESTIONS\nthe most pressing, fundamental questions in AI research, common to most or all scientific fields;\nEven in cases where we don’t have data covering past breakdowns, algorithms exist to identify anomalies in the data we begin gathering now.\n\n\n16\nRESEARCH QUESTIONS\nthe most pressing, fundamental questions in AI research, common to most or all scientific fields;\nIn the longer-term, reinforcement learning algorithms (which are goal oriented and learn from rewards they win from an environment) will be used to go beyond diagnoses and act as tactical advisors in more strategic situations where a person must choose one action or another.\n\n\n34\nRESEARCH QUESTIONS\nthe most pressing, fundamental questions in AI research, common to most or all scientific fields;\nThere is only one logical conclusion to the further development of artificial intelligence: AI will continue to grow and expand until it is as complex or more complex than a human being. This is the point where “it” ceases to be an “it” and becomes a “whom”.\n\n\n35\nRESEARCH QUESTIONS\nthe most pressing, fundamental questions in AI research, common to most or all scientific fields;\nQuestion 1: While surprising or appaling to some, it’s these individuals that have failed to notice that humanity has been doing this since for as long as we were capable. We do this every time we choose or mistakenly become a parent. Making mistakes is how we learn, we need to expect and prepare for this of our AI and their developers. We have already devised a system of responsibility for this and should adjust it accordingly for AI.\n\n\n39\nRESEARCH QUESTIONS\nthe most pressing, fundamental questions in AI research, common to most or all scientific fields;\nThe remainder of the questions are beyond my expertise. I’m more knowledgeable with the moral and ethical adjustments rather than the hard coding and building of the AI.\n\n\n\n\n\n\n\n\ndf[df.topic == 'LEGAL'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n1\nLEGAL\nThe legal and governance implications of AI\nThe fundamental task in the governance of urban systems is to keep them running; that is, to maintain the fluid movement of people, goods, vehicles and information throughout the system, without which it ceases to function.\n\n\n19\nLEGAL\nThe legal and governance implications of AI\nIndeed, while criminal risk assessment has undergone negative publicity recently (https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal- sentencing), newer algorithms and bigger datasets will make pre-crime units possible.\n\n\n20\nLEGAL\nThe legal and governance implications of AI\nWe may see a shift from punitive enforcement to preventative interventions. The legal implications are important, and those in governance should require transparency for all algorithms that filter and interpret data for the judicial system and law enforcement agencies.\n\n\n32\nLEGAL\nThe legal and governance implications of AI\nAlao please legislate AI tech into public domain.\n\n\n49\nLEGAL\nThe legal and governance implications of AI\nHypothetically we would have the most intelligent entity that we know of telling us to do something for our own good. Will we listen? Or, more specifically, will elites listen and if they don't, what would the fallout in society be? This is a major concern.\n\n\n\n\n\n\n\n\ndf[df.topic == 'MULTIDISCIPLINARY'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n26\nMULTIDISCIPLINARY\n\n• Initiatives focused on Java and the JVM will pave the way for AI to be implemented by large organizations in need of more accurate analytics. This includes government agencies, financial services, telecommunications and transport, among others. • Grants related to embeddable technologies will help AI spread to edge devices such as cell phones, cars and smart appliances. On personal devices such as phones or cars, various forms of inference might allow people to make better decisions about lifestyle, nutrition or even how run their errands. • Initiatives that focus on gathering high-quality datasets and making them public could vastly improve the performance of algorithms trained on that data, much as Li Fei Fei’s work on ImageNet helped usher in a new era of computer vision. Respondent 2 Joyce Hoffman, Writer I am most interested in safety and control issues, and yes, I agree that AI should be developed to the utmost. Respondent 3 kris kitchen, Machine Halo Artificial Intelligence Immune System should be thought about. Billions of friendly AI agents working to identify and act on nefarious agents. Respondent 4 Daniel Bryant, Kaufman Rossin This response is relevant to #8 directly and #6 tangentially. I am a software engineer that primary uses web development tools to build business solutions and have implemented machine learning in relevant algorithms.\n\n\n40\nMULTIDISCIPLINARY\nthe specific steps that could be taken by the federal government, research institutes, universities, and philanthropies to encourage multi-disciplinary AI research;\nMy final opinionated statement is about AI learning about people and who should be teaching them about people: Give this task to the people who enjoy people. Respondent 17 Geo (George) Cosmos (Kozma), Theological Seminary Budapest Hungary EU How to use an alternative History teaching Tool (taken from a „kabbalistic” - legend explaining - theory) when the need will arise to create AI robots that have a „personal touch”: like Ancestral Voices , memory setups. If we look at the Ten Spheres as Ten Generations of Ancestors,(as the Zohar advices and as those Miracle Rabbis who prepared themselves to become the Meshiah and watched the Other Side Kings -robbers and killers – who degraded their davidic ancestry) then we are able to look for the Bad Behaviors (and its therapies) as Ancestral Voices. Hence we can help our descendants in the Future. This week we have (by the Bible Melody’s ancestral contemporary stress) in 1709 Intellect Sphere in the Kabbalah - (Queen Anne and Louis XIV with Lully operas (Body Parts Correspondences: Mother,Ear) as it tries to work with the Kindness Sphere in 1754 (Frederic the Second of Prussia) on the Gevura-Strict Judgement Degree (Napoleon-Junot- Puységur and Beethoven) which will eventually impact the Malkuth (Ownership, Ego- Sister) level in 1979 (present in the next week)that has the Mountbatten assassination (Mother raising the Back to the Chest: to Tiferet-Recovery Sphere which will be full in 2024. This theory is a mosaic from facts. Which exists even if no one states it. I am just mixing a few very simply thing. 1. Inherited hormonal stresses exist. We all do know that there are experiments with mice on it. 2. Music is diminishing stress hormones. We all do know that, we even use it in Malls where we influence people by constant music /knowing addicts and criminals hate music/. 3. here is a method to filter our ancestral stresses (as there are millions of ancestors with their traumas: how to choose?) It will be especially important when we\n\n\n42\nMULTIDISCIPLINARY\nthe specific steps that could be taken by the federal government, research institutes, universities, and philanthropies to encourage multi-disciplinary AI research;\nwhen many thousands do sing a melody, it has a stress-diminishing effect in the whole area - there are experiments to prove it.) 10. The List of Dates can be used to find dates in the Future in every 45 years. We do know that scientists work to make resurrection possible. So we can simply understand the picture I found in the manucript about this Future Mashiah who heals Resurrected Kings with melodies and by touching the /hurt/ Body Parts.(It is an archetype in many religious groupings) 11. The Kabalah is an interpretation method for the Bible that uses Body Parts’ Fantasies and so we can find fantasies about Body Parts that are arising in the heads of the List’s non-Jewish and Jewish Leaders while they read the Bible Weekly Portion or they watch Operas. 12. So we are actually able to see into the Future. 13. There exists a theory that claims (Russell Hendel) that the Bible depicts not the Creator of the Physical World, but the Creation of the Visions or Dreams and Prophecies, an Inner World. This is an old teaching – called Gnosticism or Sufi or Kabbalah or Therapy Heretic Wisdom – from which Freud has developed his own theory. Of course these 3 Ideas are disturbingly innovative and together they sem to be complex or even complicated. But they have fusion points. Like I am seeing an arm of someone in my dream. If the weekly melody is evoking 1844 among the everpresent 12 Constellation Dates and this corresponds to opera X (now it is Verd’si Hernani) we look for this item – the Arm that belongs to the Breast among the 4 main Parts and this week indeed we read about the Breast-Plate , the Urim and Tumim, that has a Future redicting capacity. So we must look around the stresses of this date of 1844 and then we can imagine in the Future, when the Messiah heals the Ressurected Leaders, and we see how Meternich is in Therapy because he wanted to pay someone to kill Kossuth (a rebel leader in Hungary) in this year. From the Constellation of 44 we are in the Decade of 1837, when\n\n\n59\nMULTIDISCIPLINARY\nthe specific steps that could be taken by the federal government, research institutes, universities, and philanthropies to encourage multi-disciplinary AI research;\nElites in Silicon Valley and AI academia take a sanguine view of this future; they claim that everyone will benefit. But, of course, they are the ones who will be at the top when these changes come. They also have no training in economics, political science, or the social sciences, and think technology is a panacea. The focus on STEM to the detriment of well- rounded, liberal education has left our AI elite dangerously ignorant of the history of the negative effects of technology, and dangerously unimaginative with regard to potential the long-term consequences of their actions. What we need right now is some serious thought about the implications of AI for the structure of society, and what our options are as we transition into this new age. This means interdisciplinary research at the intersection of the social sciences and AI, with serious engagement (and hence funding) of social science scholars.\n\n\n62\nMULTIDISCIPLINARY\nthe specific steps that could be taken by the federal government, research institutes, universities, and philanthropies to encourage multi-disciplinary AI research\ncan benefit from adopting methods and ideas from more established fields of science, namely Cybersecurity.\n\n\n\n\n\n\n\n\ndf[df.topic == 'TRAINING'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n90\nTRAINING\nthe scientific and technical training that will be needed to take advantage of harnessing the potential of AI technology, and the challenges faced by institutions of higher education in retaining faculty and responding to explosive growth in student enrollment in AI-related courses and courses of study;\n9. We have a serious problem, within our universities, where professors are being lured to high paying technology companies. This trend is serious and getting worse. Whole computer science departments risk collapse. It could impact our ability to advance AI development in the coming decades.\n\n\n102\nTRAINING\nthe scientific and technical training that will be needed to take advantage of harnessing the potential of AI technology, and the challenges faced by institutions of higher education in retaining faculty and responding to explosive growth in student enrollment in AI-related courses and courses of study;\nTo make this happen expertise must be captured and brought in to guide from people at their time of need. A good teacher (and a good parent) can do that, but they cannot always be available. A kid in Kansas who wants to be an aerospace engineer should get to try out designing airplanes. But a mentor would be needed. We can build AI mentors in limited domains so it would be possible for a student anywhere to learn to do anything because the AI mentor would understand what a user was trying to accomplish within the domain and perhaps is struggling with. The student could ask questions and expect good answers tailored to the student’s needs because the AI/NLU mentor would know exactly what the students was trying to do because it has a perfect model of the world in which the student was working, the relevant expertise needed, and the mistakes students often make. NLU gets much easier when there is deep domain knowledge available.\n\n\n114\nTRAINING\nthe scientific and technical training that will be needed to take advantage of harnessing the potential of AI technology, and the challenges faced by institutions of higher education in retaining faculty and responding to explosive growth in student enrollment in AI-related courses and courses of study;\n(7) the scientific and technical training that will be needed to take advantage of harnessing the potential of AI technology, and the challenges faced by institutions of higher education in retaining faculty and responding to explosive growth in student enrollment in AI-related courses and courses of study;\n\n\n115\nTRAINING\nthe scientific and technical training that will be needed to take advantage of harnessing the potential of AI technology, and the challenges faced by institutions of higher education in retaining faculty and responding to explosive growth in student enrollment in AI-related courses and courses of study;\n--&gt; Computational thinking courses as general education and computer science options in K12. The major problem in higher ed is the lure of industry and the lack of prospects/opportunities within higher ed itself.\n\n\n130\nTRAINING\nthe scientific and technical training that will be needed to take advantage of harnessing the potential of AI technology, and the challenges faced by institutions of higher education in retaining faculty and responding to explosive growth in student enrollment in AI-related courses and courses of study;\nfunding should be skewed towards AI development. [2] Roadmaps should be adjusted for early developments due to large federal funding. [3] Successful industry models should be followed. For example, MITRE, an FFRDC, has initiatives of this kind underway. (Industry, FFRDCs) c. Universities: Already behind the technological curve, universities must retool to absorb faculty and students wanting to be part of the AI evolution. [1] The retooling must be vertical (entry-to-Ph.D. AI tracks). [2] The retooling must also be horizontal (cross- disciplinary and new-disciplinary). [3] Internal grants with contingency funding should anticipate external grants in the near term. [4] Industry partnerships should be started early. [5] Successful models should be followed for efficiency. Some small private colleges have integrating initiatives and programs underway that larger institutions can combine with AI and use as a template. (Academia, Industry) d. Philanthropies: The most promising non-governmental entities for extension of AI to the benefit masses should find ways to refocus their efforts on making AI available to the common man. This goes well beyond formatted, deterministic computer-based-training (CBT). AI is the only tool capable of training its human and AI users on the fly. [1] New NGOs focusing on empowerment through training AI for use in the field will attract both grants and donations. (Not-for-profits, NGOs, Philanthropists, USAID) (9) Specific training data sets that can accelerate the development of AI and its application: A scrubbed [all personal information removed] and massive health data set should be made available in a secure fashion to all responsible players in the healthcare AI arena. The data set should be scrubbed, actual data, not corrected, notional or “perfect” data because dealing with imperfect data is part of the problem set to be solved. (DHHS) (10) The role that “market shaping” approaches such as incentive prizes and Advanced Market\n\n\n\n\n\n\n\n\ndf[df.topic == 'RESEARCH GAPS'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n92\nRESEARCH GAPS\nthe most important research gaps in AI that must be addressed to advance this field and benefit the public;\nbots) (6) the most important research gaps in AI that must be addressed to advance this field and benefit the public; truly understanding spoken language and context. (7) the scientific and technical training that will be needed to take advantage of harnessing the potential of AI technology; AI is broad (Knowledge representation Natural language processing Graph analysis Simulation modelling Deep learning Social network analysis Soft robotics Machine learning Visualization Natural language generation Deep Q&A systems Virtual personal assistants Sensors/internet of things Robotics Recommender systems Audio/speech analytics Image analytics Machine translation ), so many skills needed. (8) the specific steps that could be taken by the federal government, research institutes, universities, and philanthropies to encourage multi- disciplinary AI research; address issue if job displacement and transition to other jobs. Respondent 31 roger Schank, Socratic Arts Inc response to questions 2, 5, 6, and *\n\n\n112\nRESEARCH GAPS\nthe most important research gaps in AI that must be addressed to advance this field and benefit the public;\n(6) the most important research gaps in AI that must be addressed to advance this field and benefit the public;\n\n\n222\nRESEARCH GAPS\nthe most important research gaps in AI that must be addressed to advance this field and benefit the public;\n(6) Most important research gaps in to advance AI and benefit the public\n\n\n276\nRESEARCH GAPS\nthe most important research gaps in AI that must be addressed to advance this field and benefit the public.\nTopic (6) The most important research gaps in AI that must be addressed to advance this field and benefit the public. As indicated in topic 5 above, AI has tended to shy away from high-level human cognitive processes regarding integrative complex perception, consciousness, and creativity much in part to their seemingly impenetrable complexity. These mind/brain processes constitute a huge gap in AI because machines cannot autonomously and spontaneously perceive, be conscious, and create in response to its environment; they do not have the ability to take information, process it, and act on it in some way that results in an output to the system much like humans do (from daily language use to the creation of artworks). Humans use their senses, emotion, movement, motor responses, and linguistic capabilities to act in response to their surrounding environment through visual, audial, olfactory, tactile and gustatory stimuli. Thus, research on perception in the context of multidisciplinary approaches using both Science and the Arts is fundamental to understanding human perception. The Arts offer a uniquely human platform from which to probe deeper into how emotion, language, decision-making, and creative behaviors all interact in any given moment, whether short-term or durative and/or improvisatory or deliberate. Similarly, consciousness is the human state of being aware of an external something within oneself. The human mind/brain has the ability to experience or to feel a sense of self on any given moment. While arousal levels, responsiveness and patient self-reporting narratives have been used in a medical context to understand consciousness, there is no financial support for those doing out-of-the-box research at the multidisciplinary level using both integrative methods from the Sciences and the Arts. Finally, if the creative process is not understood, machines and robots will never be truly creative. An essential aspect of being creative is the ability to make both old and new\n\n\n364\nRESEARCH GAPS\nthe most important research gaps in AI that must be addressed to advance this field and benefit the public;\n(6) - Among the points listed in (5) advances in verification and quality assurance may be the most important in use cases where AI software can cause damage. - Human-computer interaction is another important area. Bad interaction can result in accidents due to miscommunication with the machine. Another issue is that machine learning results may be incorrectly interpreted by the human operator or the operator may use inappropriate parameters. - AI will continue to improve in other areas, but the progress in these areas is not a limiting factor in the deployment of AI.\n\n\n\n\n\n\n\n\ndf[df.topic == 'DATA'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n4\nDATA\nspecific training data sets that can accelerate the development of AI and its application\nSuch datasets are a tremendous help when applying learning algorithms to predict breakdowns and system failures. With enough lead time, those predictions make pre- emptive action possible, action that would cost cities much less than recovery efforts in the wake of a disaster. Our choice is between an ounce of prevention or a pound of cure.\n\n\n6\nDATA\nspecific training data sets that can accelerate the development of AI and its application\nBut we are faced with a challenge. There is too much data in the world. Mountains of data are being generated every second. There is too much data for experts to wade through, and that data reflects complex and evolving patterns in reality.\n\n\n118\nDATA\n9\n(9) specific training data sets that can accelerate the development of AI and its application;\n\n\n231\nDATA\nspecific training data sets that can accelerate the development of AI and its application;\n(9) Any additional information you believe OSTP should consider\n\n\n369\nDATA\nspecific training data sets that can accelerate the development of AI and its application.\nAs we move forward with the maturing arsenal of AI techniques, which are largely designed for non-adversarial situations, a natural question is how robust modern AI methods are if motivated adversaries attempt to subvert their functionality. One emerging domain of research related to this broader question is in adversarial machine learning (AML). Attacks on machine learning algorithms can come, roughly, in two forms: evasion and poisoning. An evasion attack is best explained using a spam filtering example. Suppose that we have trained a classifier on data to distinguish spam from non-spam. Spammers would subsequently be motivated to modify spam instances in order to be (mis)classified as benign (that is, to evade detection). More generally, in an evasion attack, an attacker wishes to manipulate malicious instances in order to avoid detection by fixed learning algorithms (typically, binary classifiers in such domains). Poisoning attacks are different: in these, the adversary is assumed to modify the training data set itself, so as to subvert the learning algorithms. Poisoning attacks may have goals ranging from degrading observed performance of learning algorithms (the so-called availability attacks) to allowing future adversarial behavior to bypass detection. Given the importance of machine learning algorithms both in research and commercial application, consideration of such attacks on machine learning algorithms may be one of the most significant emerging research problems today. Respondent 72 Dennis Cooper, Self I have some comments regarding item #9 \"specific training sets that can accelerate the development of AI and its application.\" It is absolutely crucial that large training sets be created and maintained by a third party. These training sets can be used to: • validate the performance of an AI system • score the performance of an AI system • document troublesome cases that might be critical to safety • serve as a basis for acceptance testing Often\n\n\n\n\n\n\n\n\ndf[df.topic == 'MARKET_SHAPING'].head()\n\n\n\n\n\n\n\n\ntopic\ndescription\nresponse\n\n\n\n\n25\nMARKET_SHAPING\nthe role that “market shaping” approaches such as incentive prizes and Advanced Market Commitments can play in accelerating the development of applications of AI to address societal needs, such as accelerated training for low and moderate income workers\nIf history is any indication, our governmental response to disruptions in the labor market will be insufficient. In the hardest-hit sectors, workers, families and communities will suffer and break down. Unemployment, drug use and suicides will go up, along with political instability. Policies such as “basic income” or the Danish “flexicurity” should be explored as ways to soften the blow of job loss and fund transitional retraining periods. The role of “market-shaping” approaches Just as DARPA helped finance the explosion in data science in the Python community through repeated grants to such key players as Continuum, government agencies are in a position to support the researchers, technologies, tools and communities pushing AI in promising directions.\n\n\n120\nMARKET_SHAPING\nthe role that “market shaping” approaches such as incentive prizes and Advanced Market Commitments can play in accelerating the development of applications of AI to address societal needs, such as accelerated training for low and moderate income workers (see https\n(10) the role that “market shaping” approaches such as incentive prizes and Advanced Market Commitments can play in accelerating the development of applications of AI to address societal needs, such as accelerated training for low and moderate income workers (see https://www.usaid.gov/cii/market-shaping-primer); and\n\n\n121\nMARKET_SHAPING\nthe role that “market shaping” approaches such as incentive prizes and Advanced\\nMarket Commitments can play in accelerating the development of applications of AI to\\naddress societal needs, such as accelerated training for low and moderate income workers\n--&gt; Possibly if the prizes are large and don't expire. Large prizes with impossible timelines are not going to generate good results.\n\n\n726\nMARKET_SHAPING\nthe role that “market shaping” approaches such as incentive prizes and Advanced Market Commitments can play in accelerating the development of applications of AI to address societal needs, such as accelerated training for low and moderate income workers\n10) the role that “market shaping” approaches such as incentive prizes and Advanced Market Commitments can play in accelerating the development of applications of AI to address societal needs, such as accelerated training for low and moderate income workers\n\n\n727\nMARKET_SHAPING\nthe role that “market shaping” approaches such as incentive prizes and Advanced Market Commitments can play in accelerating the development of applications of AI to address societal needs, such as accelerated training for low and moderate income workers\nIncentive prizes can accelerate the development of applications of AI to societal needs in multiple ways. They provide a platform for citizen science at a global scale that can be focused on the most challenging technical problems faced in AI applications. They can be globally inclusive through low barriers to entry by providing access to relevant domain specific datasets, accelerated cloud computing resources and open-source foundational code. Incentive prizes also offer an educational and training opportunity at very low cost to the participant and the communities that form around these challenges are often highly active with widespread exchange of ideas and spontaneous teamwork from distributed teams with complementary approaches to a problem. NVIDIA GPUs accelerate numerous cloud computing platforms that offer the ideal host for AI incentive prizes. NVIDIA has also sponsored incentive prizes that have led to cross-disciplinary exchange of ideas in solving AI challenges with cutting edge results. For example, in the recent Second National Data Science Bowl the winning team was able to apply their AI expertise developed in their careers as financial analysts to solve a challenging medical imagery analysis problem. The AI system they developed could analyze a cardiac Magnetic Resonance Image (MRI) for a key indicator of heart disease with accuracy comparable to an expert cardiologist but thousands of times faster - this has the potential to save years of valuable time for a human cardiologist throughout their career. Respondent 100 Amir Banifatemi, XPRIZE Foundation Over the past 50 years Artificial Intelligence (AI) has made steady, linear progress. The technology has matured and is now reaching a point on the technology adoption curve where AI will have the potential to transcend from linear growth to an exponential leap forward for humanity.",
    "crumbs": [
      "Examples",
      "Qualitative Survey Analysis"
    ]
  },
  {
    "objectID": "examples_qualitative_survey_analysis.html#step-6-summarize-the-results",
    "href": "examples_qualitative_survey_analysis.html#step-6-summarize-the-results",
    "title": "Qualitative Survey Analysis",
    "section": "STEP 6: Summarize the Results",
    "text": "STEP 6: Summarize the Results\nOnce responses are coded by topic area, we can summarize all responses with respect to a specific topic area. We will summarize the responses coded as being pertinent to legal/governance as an illustrative example.\n(Note tha these comments are from 2016 - prior to the rise of generative AI.)\n\nlegal_responses = '\\n\\n'.join(df[df.topic == 'LEGAL'].response.tolist())\nwith open('/tmp/legal_responses.txt', 'w') as f:\n    f.write(legal_responses)\n\n\nsummarizer = Summarizer(llm)\n\n\noutput = summarizer.summarize('/tmp/legal_responses.txt')\n\n\nprint(\"\\n\".join(wrap(output['output_text'])))\n\nThe documents discuss the governance and ethics of artificial\nintelligence (AI). Key concerns include:  1. Accountability: Ensuring\nthat AI systems can be held accountable for their actions. 2.\nTransparency: Requiring transparency in the development and deployment\nof AI systems, particularly those used in law enforcement and judicial\ndecision-making.  The documents also highlight the need for\npolicymakers to consider the implications of AI on society, law, and\nethics. They emphasize the importance of a nuanced approach to\nregulating AI, one that balances innovation with safety and\naccountability.  Overall, the documents suggest that policymakers\nshould adopt a light-touch regulatory approach focused on consumers\nand outcomes over underlying technologies.",
    "crumbs": [
      "Examples",
      "Qualitative Survey Analysis"
    ]
  },
  {
    "objectID": "examples_qualitative_survey_analysis.html#next-steps",
    "href": "examples_qualitative_survey_analysis.html#next-steps",
    "title": "Qualitative Survey Analysis",
    "section": "Next Steps",
    "text": "Next Steps\nThis noteboook is simply an illustrative example. There are opportunities to refine the approach and improve results, which we leave as an exercise to the reader.\nFor instance, you can adjust the prompt, try different models, pre-process and post-process the data and results in different ways, and modify the summarization process.",
    "crumbs": [
      "Examples",
      "Qualitative Survey Analysis"
    ]
  },
  {
    "objectID": "sk.tm.html",
    "href": "sk.tm.html",
    "title": "sk.tm",
    "section": "",
    "text": "source\n\nTopicModel\n\ndef TopicModel(\n    texts:Optional=None, n_topics:Optional=None, n_features:int=100000, min_df:int=5, max_df:float=0.5,\n    stop_words:Union='english', model_type:str='nmf', max_iter:int=10, lda_max_iter:Optional=None,\n    lda_mode:str='online', token_pattern:Optional=None, verbose:bool=True, hyperparam_kwargs:Optional=None\n):\n\nFits a topic model to documents in . Example: python     tm = get_topic_model(docs, n_topics=20,                          n_features=1000, min_df=2, max_df=0.95)\nArgs:\n\ntexts (list of str): list of texts\nn_topics (int): number of topics. If None, n_topics = min{400, sqrt[# documents/2]})\nn_features (int): maximum words to consider\nmax_df (float): words in more than max_df proportion of docs discarded\nstop_words (str or list): either ‘english’ for built-in stop words or a list of stop words to ignore\nmodel_type(str): type of topic model to fit. One of {‘lda’, ‘nmf’}. Default:‘lda’\nmax_iter (int): maximum iterations. 5 is default if using lda_mode=‘online’ or nmf. If lda_mode=‘batch’, this should be increased (e.g., 1500).\nlda_max_iter (int): alias for max_iter for backwards compatilibity\nlda_mode (str): one of {‘online’, ‘batch’}. Ignored if model_type !=‘lda’\ntoken_pattern(str): regex pattern to use to tokenize documents.\nverbose(bool): verbosity\nhyperparam_kwargs(dict): hyperparameters for LDA/NMF Keys in this dict can be any of the following: alpha: alpha for LDA default: 5./n_topics beta: beta for LDA. default:0.01 nmf_alpha: alias for alpha for backwars compatilibity l1_ratio: l1_ratio for NMF. default: 0 ngram_range: whether to consider bigrams, trigrams. default: (1,1)"
  },
  {
    "objectID": "hf.models.pooling.factory.html",
    "href": "hf.models.pooling.factory.html",
    "title": "hf.models.pooling.factory",
    "section": "",
    "text": "source\n\nPoolingFactory\n\ndef PoolingFactory(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nMethod to create pooling models."
  },
  {
    "objectID": "workflows.html",
    "href": "workflows.html",
    "title": "Buildling Workflows",
    "section": "",
    "text": "Create orchestrated document-processing pipelines using simple YAML configuration files. Instead of writing code, you design workflows by connecting and chaining together different types of nodes. Allows you to solve problems by assembling pre-designed building blocks.\n\n\n\nQuick Start - Three Core Examples\nCommand-Line Usage\nVisual Workflow Builder\nWorkflow Structure\nPort Types and Data Flow\nNode Types Reference\nConfiguration Options\nAdvanced Examples\nValidation and Error Handling\nBest Practices\nTroubleshooting\n\n\n\n\nThe workflow engine provides three essential patterns that cover the most common document processing scenarios:\n\n\nPurpose: Download and perform thematic analysis of the State of the Union address using chunking and aggregation.\n# Requires: set OPENAI_API_KEY  \npython -m onprem.workflow yaml_examples/1_direct_analysis.yaml\nWhat it does: - Downloads the State of the Union text automatically from GitHub - Chunks the speech into 1000-character segments with 100-character overlap - Extracts thematic keywords from each chunk (policy areas, topics) - Cleans and standardizes theme responses for consistency - Aggregates all themes into comprehensive political analysis - Exports both detailed themes (CSV) and summary analysis (JSON)\nRequirements: - Set OPENAI_API_KEY environment variable, as example uses GPT-4o-mini. Local LLMs can be used as well. - Internet connection for document download\n\n\n\nPurpose: Load PDF files, chunk them, and store in a vector database for later retrieval.\n# Run from the workflows directory\npython -m onprem.workflow yaml_examples/2_ingest_pdfs.yaml\nWhat it does: - Loads PDF files from ../sample_data/ - Converts PDFs to markdown for better processing - Chunks documents into 800-character pieces with 80-character overlap - Stores in ChromaDB vector database at document_vectors/\nRequirements: PDF files in the sample_data directory\n\n\n\nPurpose: Query an existing vector database, apply AI analysis, and export results.\n# Requires: Run example 1 first + set OPENAI_API_KEY\npython -m onprem.workflow yaml_examples/3_analyze_from_vectorstore.yaml\nWhat it does: - Searches the vector database created in example 1 - Applies AI analysis to find documents about “artificial intelligence machine learning” - Uses GPT-3.5-turbo to analyze each document for topic, key points, and relevance - Exports results to document_analysis_results.xlsx\nRequirements: - Run example 2 first to create document_vectors/ - Set OPENAI_API_KEY environment variable, as GPT-4o-mini is used. Local LLMs can be used as well.\n\n\n\n\n\n\npython -m onprem.workflow &lt;workflow.yaml&gt;\n\n\n\n\nThe OnPrem.LLM web UI includes a Visual Workflow Builder that can generate and execute YAML-based workflows through a point-and-click interface with no coding required.\n\nSee the web UI documentation for more information.\n\n\npython -m onprem.workflow &lt;workflow.yaml&gt;\n\n\n\n# Show help and examples\npython -m onprem.workflow --help\n\n# Validate workflow without running\npython -m onprem.workflow --validate workflow.yaml\n\n# List all available node types\npython -m onprem.workflow --list-nodes\n\n# Run quietly (suppress progress output)\npython -m onprem.workflow --quiet workflow.yaml\n\n# Show version\npython -m onprem.workflow --version\n\n\n\n# Run the PDF ingestion workflow\npython -m onprem.workflow yaml_examples/1_ingest_pdfs.yaml\n\n# Validate a workflow before running\npython -m onprem.workflow --validate yaml_examples/2_analyze_from_vectorstore.yaml\n\n# See all available node types\npython -m onprem.workflow --list-nodes\n\n\n\n\nThe three core examples demonstrate the main workflow patterns:\n\n\nDocuments → Chunking → Vector Store\nUse this pattern to build searchable databases from your document collections.\n\n\n\nVector Store → Query → AI Analysis → Export\nUse this pattern to analyze specific topics from large document collections using semantic search.\n\n\n\nDocuments → Full Processing → AI Analysis → Cleanup → Export\nUse this pattern for comprehensive analysis of entire document collections without intermediate storage.\n\n\n\n\n\n\n\nLoadFromFolder - Load all documents from a directory\nLoadSingleDocument - Load a specific file\nLoadWebDocument - Download and load from URL\nLoadSpreadsheet - Load text from spreadsheet rows with metadata\n\n\n\n\n\nSplitByCharacterCount - Chunk by character count\nSplitByParagraph - Chunk by paragraphs (preserves structure)\nKeepFullDocument - Keep documents whole, optionally concatenate pages\n\n\n\n\n\nAddMetadata - Add custom metadata fields to documents\nContentPrefix - Prepend text to document content\nContentSuffix - Append text to document content\nDocumentFilter - Filter documents by metadata or content criteria\nPythonDocumentTransformer - Custom Python transformations\n\n\n\n\n\nChromaStore - Vector database for semantic search\nWhooshStore - Full-text search index\nElasticsearchStore - Hybrid search capabilities\n\n\n\n\n\nQueryDualStore - Search dual vector store (sparse, semantic, and hybrid search)\nQueryChromaStore - Search vector database\nQueryWhooshStore - Search text index\nQueryElasticsearchStore - Search Elasticsearch index\n\n\n\n\n\nPromptProcessor - Apply AI analysis using custom prompts (DocumentProcessor)\nResponseCleaner - Clean and format AI responses (ResultProcessor)\nSummaryProcessor - Generate document summaries (DocumentProcessor)\n\nPythonDocumentProcessor - Execute custom Python code on documents (DocumentProcessor)\nPythonResultProcessor - Execute custom Python code on processing results (ResultProcessor)\nDocumentToResults - Convert documents to export-ready results (bridges Query → Exporter)\n\n\n\n\n\nAggregatorNode - Aggregate multiple results into single result using LLM (AggregatorProcessor)\nPythonAggregatorNode - Aggregate multiple results using custom Python code (AggregatorProcessor)\n\n\n\n\n\nCSVExporter - Export to CSV format\nExcelExporter - Export to Excel format\n\nJSONExporter - Export to JSON format\nJSONResponseExporter - Extract and export JSON from LLM responses\n\n\n\n\n\n\n\nCreate a file called my_workflow.yaml:\nnodes:\n  document_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"/path/to/your/documents\"\n      verbose: true\n  \n  text_chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 500\n      chunk_overlap: 50\n  \n  search_index:\n    type: WhooshStore\n    config:\n      persist_location: \"my_search_index\"\n\nconnections:\n  - from: document_loader\n    from_port: documents\n    to: text_chunker\n    to_port: documents\n  \n  - from: text_chunker\n    from_port: documents\n    to: search_index\n    to_port: documents\n\n\n\nfrom onprem.workflow import execute_workflow\n\n# Execute the workflow\nresults = execute_workflow(\"my_workflow.yaml\", verbose=True)\nOr programmatically:\nfrom onprem.workflow import WorkflowEngine\n\nengine = WorkflowEngine()\nengine.load_workflow_from_yaml(\"my_workflow.yaml\")\nresults = engine.execute(verbose=True)\n\n\n\n\nA workflow YAML file has two main sections:\n\n\nDefines the processing nodes in your pipeline:\nnodes:\n  node_id:              # Unique identifier for this node\n    type: NodeTypeName  # Type of node (see Node Types Reference)\n    config:             # Configuration specific to this node type\n      parameter1: value1\n      parameter2: value2\n\n\n\nDefines how data flows between nodes:\nconnections:\n  - from: source_node_id    # Source node ID\n    from_port: output_port  # Output port name\n    to: target_node_id      # Target node ID  \n    to_port: input_port     # Input port name\n\n\n\n\nThe workflow system uses a strongly-typed port system to ensure data consistency and prevent invalid connections. Understanding port types is essential for building valid workflows.\n\n\nThere are three main port types in the workflow system:\n\n\nMost common type - Contains LangChain Document objects with content and metadata.\n# Document structure\nDocument(\n    page_content=\"The actual text content of the document...\",\n    metadata={\n        \"source\": \"/path/to/file.pdf\",\n        \"page\": 1,\n        \"author\": \"John Smith\",\n        \"extension\": \"pdf\"\n    }\n)\nUsed by: - All Loader → TextSplitter connections - All TextSplitter → Storage connections\n- All Query → Processor connections\n\n\n\nProcessing output - Contains structured analysis results, summaries, or prompt responses.\n# Results structure\n[\n    {\n        \"document_id\": 0,\n        \"source\": \"research.pdf\",\n        \"prompt\": \"Analyze this document and provide...\",\n        \"response\": \"TOPIC: AI Research | TECH: Neural networks | LEVEL: Advanced\",\n        \"original_length\": 1247,\n        \"metadata\": {\"page\": 1, \"author\": \"Smith\", \"year\": 2023}\n    }\n]\nUsed by: - All Processor → Exporter connections\n\n\n\nAggregated analysis - Contains a single dictionary with consolidated results from multiple inputs.\n# Aggregated result structure\n{\n    \"aggregated_response\": \"Top 3 topics: AI (80%), automation (65%), data science (45%)\",\n    \"source_count\": 12,\n    \"aggregation_method\": \"llm_prompt\",\n    \"topic_analysis\": {\n        \"top_topics\": [\"AI\", \"automation\", \"data science\"],\n        \"coverage_percentage\": 75.5\n    },\n    \"original_results\": [...]  # Reference to source data\n}\nUsed by: - AggregatorNode and PythonAggregatorNode outputs - Can be exported using Exporter nodes (converted to single-row format)\n\n\n\nCompletion status - Simple text messages indicating operation results.\n# Status examples\n\"Successfully stored 150 documents in WhooshStore\"\n\"Exported 25 results to analysis_report.xlsx\"\n\"No documents to store\"\nUsed by: - Storage node outputs (terminal) - Exporter node outputs (terminal)\n\n\n\n\nRaw Files → List[Document] → List[Document] → str\n   ↓            ↓               ↓             ↓\nLoader    TextSplitter      Storage      Status\n\nAlternative analysis path:\nIndex → List[Document] → List[Dict] → str  \n  ↓         ↓              ↓         ↓\nQuery   Processor      Exporter  Status\n\nNew direct export path:\nIndex → List[Document] → List[Dict] → str\n  ↓         ↓              ↓         ↓\nQuery  DocumentToResults Exporter Status\n\nNew aggregation path:\nIndex → List[Document] → List[Dict] → Dict → str\n  ↓         ↓              ↓         ↓      ↓\nQuery   Processor     Aggregator Export Status\n\n\n\nThe workflow engine validates that connected ports have matching types:\n✅ Valid Connections:\n# Document processing chain\n- from: loader\n  from_port: documents        # List[Document]\n  to: chunker\n  to_port: documents         # List[Document] ✓\n\n# Analysis chain  \n- from: query\n  from_port: documents        # List[Document]\n  to: processor\n  to_port: documents         # List[Document] ✓\n\n- from: processor\n  from_port: results          # List[Dict]\n  to: exporter\n  to_port: results           # List[Dict] ✓\n❌ Invalid Connections:\n# Type mismatch\n- from: loader\n  from_port: documents        # List[Document]\n  to: exporter\n  to_port: results           # List[Dict] ❌\n\n# Wrong direction\n- from: storage\n  from_port: status           # str (terminal node)\n  to: processor\n  to_port: documents         # List[Document] ❌\n\n\n\n\ndocuments - Always contains List[Document] objects\nresults - Always contains List[Dict] with analysis results\n\nresult - Always contains Dict with single aggregated result (AggregatorProcessor output)\nstatus - Always contains str with completion messages\n\n\n\n\nData flows preserve metadata throughout the pipeline:\n\nLoader → Document metadata (source, extension, dates, etc.)\nTextSplitter → Preserves original metadata in chunks\nQuery → Returns documents with original metadata\nProcessor → Includes metadata in results under metadata key\nExporter → Flattens metadata into columns (meta_source, meta_page, etc.)\n\n\n\n\nWhen port types don’t match, you’ll see validation errors like:\nWorkflowValidationError: Type mismatch: \nloader.documents (List[Document]) -&gt; exporter.results (List[Dict])\nWorkflowValidationError: Target node processor has no input port 'status'. \nAvailable: ['documents']\nUnderstanding these port types helps you: - Design valid workflows - Debug connection errors - Understand data transformations - Plan processing pipelines\n\n\n\n\n\n\nLoader nodes read documents from various sources and output List[Document].\n\n\nLoads all documents from a directory using ingest.load_documents.\nnodes:\n  my_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"/path/to/documents\"    # Required: Directory path\n      ignored_files: [\"temp.txt\", \"draft.doc\"] # Optional: Specific files to skip\n      include_patterns: [\"*.pdf\", \"*.docx\"]    # Optional: Only load files matching these patterns\n      exclude_patterns: [\"*draft*\", \"*temp*\"]  # Optional: Skip files matching these patterns\n      verbose: true                             # Optional: Show progress\n      pdf_markdown: false                       # Optional: Convert PDFs to markdown\n      pdf_unstructured: false                   # Optional: Use unstructured PDF parsing\n      n_proc: null                             # Optional: Number of CPU cores (null = all)\n      store_md5: false                         # Optional: Store MD5 hash in metadata\n      store_mimetype: false                    # Optional: Store MIME type in metadata\n      store_file_dates: false                  # Optional: Store file dates in metadata\n      infer_table_structure: false             # Optional: Extract tables from PDFs\n      caption_tables: false                    # Optional: Generate table captions (requires llm)\n      extract_document_titles: false           # Optional: Extract document titles (requires llm)\nFilename Pattern Filtering:\n\ninclude_patterns: Only process files matching these glob patterns (e.g., [\"*.pdf\", \"*.doc*\"])\nexclude_patterns: Skip files matching these glob patterns (e.g., [\"*draft*\", \"*backup*\"])\nIf both specified, file must match include pattern AND not match exclude pattern\nUses standard Unix glob patterns: * (any chars), ? (single char), [abc] (character set)\n\nOutput Ports: - documents: List[Document] - Loaded documents\n\n\n\nLoads a single document using ingest.load_single_document.\nnodes:\n  single_doc:\n    type: LoadSingleDocument\n    config:\n      file_path: \"/path/to/document.pdf\"       # Required: Path to single file\n      pdf_markdown: false                      # Optional: Convert PDF to markdown\n      pdf_unstructured: false                  # Optional: Use unstructured parsing\n      store_md5: false                        # Optional: Store MD5 hash\n      store_mimetype: false                   # Optional: Store MIME type\n      store_file_dates: false                 # Optional: Store file dates\n      infer_table_structure: false            # Optional: Extract tables\nOutput Ports: - documents: List[Document] - Loaded document\n\n\n\nDownloads and loads a document from a URL.\nnodes:\n  web_doc:\n    type: LoadWebDocument\n    config:\n      url: \"https://example.com/document.pdf\"  # Required: Document URL\n      username: \"user\"                         # Optional: Authentication username\n      password: \"pass\"                         # Optional: Authentication password\nOutput Ports: - documents: List[Document] - Downloaded document\n\n\n\nLoads documents from spreadsheet files where each row becomes a document with text content and metadata.\nnodes:\n  spreadsheet_loader:\n    type: LoadSpreadsheet\n    config:\n      file_path: \"/path/to/data.xlsx\"          # Required: Path to spreadsheet (.xlsx, .xls, .csv)\n      text_column: \"description\"               # Required: Column containing text content\n      metadata_columns: [\"id\", \"category\", \"priority\"]  # Optional: Specific columns as metadata\n      sheet_name: \"Sheet1\"                     # Optional: Excel sheet name (default: first sheet)\n  \n  # Use all other columns as metadata (default behavior)\n  auto_metadata:\n    type: LoadSpreadsheet\n    config:\n      file_path: \"survey_responses.csv\"\n      text_column: \"feedback\"                  # Only specify text column\n      # metadata_columns not specified = use all other columns\nUse Cases: - Survey Analysis - Load feedback text with respondent demographics as metadata - Product Reviews - Load review text with ratings, dates, and categories - Customer Support - Load ticket descriptions with priority, department, and status - Content Management - Load article text with tags, authors, and publication dates - Research Data - Load interview transcripts with participant information\nFile Format Support: - CSV files (.csv) - Comma-separated values - Excel files (.xlsx, .xls) - Microsoft Excel format - Automatic detection based on file extension\nMetadata Handling: - Automatic metadata - By default, all columns except text_column become metadata - Custom metadata - Specify metadata_columns to select only certain fields - Standard fields - Always includes source, row_number, and text_column in metadata - Data types - Automatically converts pandas/numpy types to JSON-serializable Python types - Null handling - Empty cells become None in metadata\nError Handling: - Validates text column exists - Validates metadata columns exist (if specified) - Skips rows with empty text content - Provides helpful error messages for missing files or columns\nOutput Ports: - documents: List[Document] - Documents created from spreadsheet rows\n\n\n\n\nTextSplitter nodes process documents and output chunked List[Document].\n\n\nChunks documents by character count using ingest.chunk_documents.\nnodes:\n  char_splitter:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 500                         # Optional: Characters per chunk (default: 500)\n      chunk_overlap: 50                       # Optional: Overlap between chunks (default: 50)\n      infer_table_structure: false            # Optional: Handle tables specially\n      preserve_paragraphs: false              # Optional: Keep paragraphs intact\nInput Ports: - documents: List[Document] - Documents to chunk\nOutput Ports: - documents: List[Document] - Chunked documents\n\n\n\nChunks documents by paragraph boundaries, preserving document structure.\nnodes:\n  para_splitter:\n    type: SplitByParagraph\n    config:\n      chunk_size: 1000                        # Optional: Max characters per chunk\n      chunk_overlap: 100                      # Optional: Overlap between chunks\n      # preserve_paragraphs is automatically set to True\nInput Ports: - documents: List[Document] - Documents to chunk\nOutput Ports: - documents: List[Document] - Paragraph-based chunks\n\n\n\nPasses documents through without any chunking. Optionally concatenates multi-page documents and/or truncates documents to a maximum word count.\nnodes:\n  no_split:\n    type: KeepFullDocument\n    config: {}  # No configuration needed - keeps documents as-is\n  \n  # For multi-page documents (PDFs, etc.) - combine into single document\n  full_document:\n    type: KeepFullDocument\n    config:\n      concatenate_pages: true    # Optional: Combine pages into single document\n  \n  # Truncate documents to first N words (useful for LLM context limits)\n  truncated_document:\n    type: KeepFullDocument\n    config:\n      max_words: 500            # Optional: Truncate to first 500 words\n  \n  # Both concatenation and truncation (applied in that order)\n  combined_processing:\n    type: KeepFullDocument\n    config:\n      concatenate_pages: true   # First: combine multi-page documents\n      max_words: 1000          # Then: truncate to first 1000 words\nPage Concatenation:\nWhen concatenate_pages: true, multi-page documents are combined: - Pages sorted by page number - Content joined with --- PAGE BREAK --- separators - Metadata preserved from first page plus additional fields: - page: -1 (indicates full document) - page_count: N (number of pages combined) - page_range: \"1-5\" (original page range) - concatenated: true (flag indicating concatenation)\nDocument Truncation:\nWhen max_words: N is specified, documents are truncated to the first N words: - Word boundaries are preserved (no partial words) - Metadata is enriched with truncation information: - original_word_count: 2500 (original document length) - truncated: true (indicates truncation occurred) - truncated_word_count: 500 (target truncation size) - Documents shorter than max_words are passed through unchanged - Processing order: concatenation first, then truncation\nUse Cases: - Page Concatenation: - Resume Processing - Combine multi-page resumes into single document - Contract Analysis - Process entire contracts as one unit - Report Analysis - Analyze complete reports without page boundaries - Legal Documents - Preserve document structure while enabling full-text analysis\n\nDocument Truncation:\n\nLLM Context Management - Fit long documents within token limits\nCost Control - Reduce processing costs for very long documents\nPreview Generation - Create document summaries from beginnings\nPerformance Optimization - Speed up processing of large documents\nClassification Tasks - Use document openings for categorization\n\n\nInput Ports: - documents: List[Document] - Documents to pass through or concatenate\nOutput Ports: - documents: List[Document] - Unchanged or concatenated documents\n\n\n\n\nDocumentTransformer nodes transform documents while preserving the List[Document] → List[Document] flow. They can add metadata, modify content, filter documents, or apply custom transformations. These nodes can be placed anywhere in the document pipeline.\n\n\nAdds static metadata fields to all documents for categorization and organization.\nnodes:\n  categorize_meeting:\n    type: AddMetadata\n    config:\n      metadata:\n        category: \"meeting20251001\"\n        department: \"engineering\"\n        priority: \"high\"\n        project: \"Project Alpha\"\n        classification: \"internal\"\nUse Cases: - Meeting Organization - Tag all documents from a specific meeting - Project Tracking - Add project identifiers to document collections - Department Categorization - Organize documents by department or team - Classification - Mark documents as confidential, internal, or public - Batch Processing - Add consistent metadata to large document collections\nInput Ports: - documents: List[Document] - Documents to enrich\nOutput Ports: - documents: List[Document] - Documents with added metadata\n\n\n\nPrepends text to the page_content of all documents.\nnodes:\n  mark_confidential:\n    type: ContentPrefix\n    config:\n      prefix: \"[CONFIDENTIAL - INTERNAL USE ONLY]\"\n      separator: \"\\n\\n\"  # Optional: separator between prefix and content (default: \"\\n\\n\")\n  \n  add_header:\n    type: ContentPrefix\n    config:\n      prefix: \"Project Alpha Documentation\"\n      separator: \"\\n---\\n\"\nUse Cases: - Confidentiality Markings - Add confidential headers to sensitive documents - Document Headers - Add consistent headers to document collections - Processing Stamps - Mark documents as processed by specific workflows - Context Addition - Add contextual information to document beginnings\nInput Ports: - documents: List[Document] - Documents to modify\nOutput Ports: - documents: List[Document] - Documents with prefixed content\n\n\n\nAppends text to the page_content of all documents.\nnodes:\n  add_footer:\n    type: ContentSuffix\n    config:\n      suffix: |\n        ---\n        Document processed by OnPrem Workflow Engine\n        Processing date: 2025-01-16\n        For questions, contact: admin@company.com\n      separator: \"\\n\"  # Optional: separator between content and suffix (default: \"\\n\\n\")\nUse Cases: - Processing Information - Add processing timestamps and contact info - Legal Disclaimers - Append legal text to documents - Document Footers - Add consistent footers to document collections - Attribution - Add source or processing attribution\nInput Ports: - documents: List[Document] - Documents to modify\nOutput Ports: - documents: List[Document] - Documents with appended content\n\n\n\nFilters documents based on metadata criteria, content patterns, or length requirements.\nnodes:\n  filter_engineering:\n    type: DocumentFilter\n    config:\n      # Filter by metadata\n      metadata_filters:\n        department: \"engineering\"\n        status: \"active\"\n      # Filter by content\n      content_contains: [\"project\", \"analysis\", \"results\"]\n      content_excludes: [\"draft\", \"template\"]\n      # Filter by length\n      min_length: 100\n      max_length: 10000\n  \n  # Simple content filtering\n  relevant_docs_only:\n    type: DocumentFilter\n    config:\n      content_contains: [\"machine learning\", \"AI\", \"neural network\"]\n      min_length: 50\nFilter Options: - metadata_filters: Dictionary of metadata key-value pairs that must match exactly - content_contains: List of terms - document must contain at least one - content_excludes: List of terms - document must not contain any - min_length: Minimum content length in characters - max_length: Maximum content length in characters\nUse Cases: - Relevance Filtering - Keep only documents containing specific keywords - Quality Control - Remove documents that are too short or too long - Content Curation - Filter out drafts, templates, or irrelevant content - Metadata-based Selection - Keep only documents matching specific criteria\nInput Ports: - documents: List[Document] - Documents to filter\nOutput Ports: - documents: List[Document] - Filtered documents\n\n\n\nExecutes custom Python code to transform documents with full flexibility and security controls.\nnodes:\n  extract_document_info:\n    type: PythonDocumentTransformer\n    config:\n      code: |\n        # Available variables:\n        # - doc: Document object\n        # - content: doc.page_content (string)\n        # - metadata: doc.metadata (mutable copy)\n        # - document_id: index of document (int)\n        # - source: source file path (string)\n        \n        # Extract information from content\n        import re\n        \n        word_count = len(content.split())\n        sentence_count = len(re.findall(r'[.!?]+', content))\n        \n        # Find email addresses\n        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n        emails = re.findall(email_pattern, content)\n        \n        # Determine document type\n        if 'meeting' in content.lower() and 'agenda' in content.lower():\n            doc_type = 'meeting_agenda'\n        elif 'analysis' in content.lower():\n            doc_type = 'analysis_report'\n        else:\n            doc_type = 'general_document'\n        \n        # Enrich metadata\n        metadata.update({\n            'word_count': word_count,\n            'sentence_count': sentence_count,\n            'email_count': len(emails),\n            'document_type': doc_type,\n            'complexity_score': min(10, word_count // 100)\n        })\n        \n        # Add summary to content\n        summary = f\"[{doc_type.upper()}: {word_count} words, Complexity: {metadata['complexity_score']}/10]\"\n        content = summary + \"\\n\\n\" + content\n        \n        # Create transformed document\n        transformed_doc = Document(\n            page_content=content,\n            metadata=metadata\n        )\n\n  # Load transformation from external file\n  complex_transform:\n    type: PythonDocumentTransformer\n    config:\n      code_file: \"scripts/document_enricher.py\"\nAvailable Python Environment: - Built-in functions: len, str, int, float, bool, list, dict, min, max, etc. - Safe modules: re, json, math, datetime (pre-imported) - Document class: Available for creating new Document objects - Security: No file I/O, network access, or system operations\nVariable Reference: - doc: Original Document object (read-only) - content: Document content (modifiable string) - metadata: Document metadata (modifiable dictionary copy) - document_id: Index of current document (int) - source: Source file path (string) - transformed_doc: Set this to a Document object for the output (optional)\nTransformation Options: 1. Modify Variables: Change content and metadata, let the system create the Document 2. Explicit Creation: Create and set transformed_doc explicitly\nUse Cases: - Content Analysis - Extract key information and add to metadata - Document Classification - Automatically categorize documents by content - Data Extraction - Find emails, URLs, phone numbers, etc. - Content Transformation - Modify content based on complex rules - Custom Enrichment - Add calculated metrics or derived information\nInput Ports: - documents: List[Document] - Documents to transform\nOutput Ports: - documents: List[Document] - Transformed documents\n\n\n\n\nStorage nodes save documents to various backends and return status messages.\n\n\nStores documents in a ChromaDB vector database.\nnodes:\n  vector_db:\n    type: ChromaStore\n    config:\n      persist_location: \"/path/to/chromadb\"    # Optional: Database path\n      # Additional ChromaDB configuration options\nInput Ports: - documents: List[Document] - Documents to store\nOutput Ports: - status: str - Storage status message\n\n\n\nStores documents in a Whoosh full-text search index.\nnodes:\n  search_index:\n    type: WhooshStore\n    config:\n      persist_location: \"/path/to/whoosh_index\" # Optional: Index path\n      # Additional Whoosh configuration options\nInput Ports: - documents: List[Document] - Documents to index\nOutput Ports: - status: str - Indexing status message\n\n\n\nStores documents in an Elasticsearch cluster.\nnodes:\n  es_store:\n    type: ElasticsearchStore\n    config:\n      persist_location: \"http://localhost:9200\" # Required: Elasticsearch URL\n      index_name: \"my_documents\"                # Optional: Index name\n      # Additional Elasticsearch configuration options\nInput Ports: - documents: List[Document] - Documents to store\nOutput Ports: - status: str - Storage status message\n\n\n\n\nQuery nodes search existing storage indexes and return matching documents.\n\n\nSearches documents in a Whoosh full-text search index with support for different search types.\nnodes:\n  # Sparse search (pure keyword matching)\n  keyword_search:\n    type: QueryWhooshStore\n    config:\n      persist_location: \"/path/to/whoosh_index\" # Required: Index path\n      query: \"artificial intelligence ML\"        # Required: Search terms\n      search_type: \"sparse\"                     # Optional: \"sparse\" or \"semantic\" (default: sparse)\n      limit: 20                                 # Optional: Max results (default: 100)\n      \n  # Semantic search (keyword + embedding re-ranking)  \n  smart_search:\n    type: QueryWhooshStore\n    config:\n      persist_location: \"/path/to/whoosh_index\"\n      query: \"machine learning concepts\"\n      search_type: \"semantic\"                   # Uses keyword search + semantic re-ranking\n      limit: 10\nSearch Types: - sparse: Pure keyword/full-text search using Whoosh - semantic: Keyword search followed by embedding-based re-ranking\nInput Ports: - None (queries existing storage directly)\nOutput Ports: - documents: List[Document] - Matching documents\n\n\n\nSearches documents in a dual vector store that combines both sparse (keyword) and dense (semantic) search capabilities with hybrid search option.\nnodes:\n  hybrid_search:\n    type: QueryDualStore\n    config:\n      persist_location: \"/path/to/dualstore\"      # Required: Database path\n      query: \"artificial intelligence trends\"     # Required: Search query\n      search_type: \"hybrid\"                       # Optional: sparse/semantic/hybrid (default: hybrid)\n      limit: 15                                  # Optional: Max results (default: 10)\n      weights: [0.6, 0.4]                       # Optional: [dense, sparse] weights for hybrid search\nSearch Types: - sparse: Keyword/full-text search using sparse vectors - semantic: Vector similarity search using dense embeddings - hybrid: Combines sparse and semantic search with configurable weights\nHybrid Search Configuration: - weights: Array of [dense_weight, sparse_weight] (default: [0.6, 0.4]) - Dense weight controls semantic search influence - Sparse weight controls keyword search influence - Weights should sum to 1.0 for optimal results\nInput Ports: - None (queries existing storage directly)\nOutput Ports: - documents: List[Document] - Matching documents (ranked by combined score for hybrid)\n\n\n\nSearches documents in a ChromaDB vector database using semantic similarity.\nnodes:\n  vector_search:\n    type: QueryChromaStore\n    config:\n      persist_location: \"/path/to/chromadb\"    # Required: Database path\n      query: \"machine learning algorithms\"     # Required: Search query\n      search_type: \"semantic\"                  # Optional: Only \"semantic\" supported (default)\n      limit: 10                               # Optional: Max results (default: 10)\nSearch Types: - semantic: Vector similarity search (only supported type)\nInput Ports: - None (queries existing storage directly)\nOutput Ports: - documents: List[Document] - Similar documents\n\n\n\nSearches documents in an Elasticsearch index with full support for all search types.\nnodes:\n  # Sparse search (BM25 text matching)\n  text_search:\n    type: QueryElasticsearchStore\n    config:\n      persist_location: \"http://localhost:9200\" # Required: Elasticsearch URL\n      index_name: \"my_index\"                    # Required: Index name\n      query: \"artificial intelligence\"          # Required: Search query\n      search_type: \"sparse\"                     # Optional: \"sparse\", \"semantic\", or \"hybrid\"\n      limit: 5                                  # Optional: Max results (default: 10)\n      \n  # Semantic search (vector similarity)\n  vector_search:\n    type: QueryElasticsearchStore\n    config:\n      persist_location: \"https://my-es:9200\"\n      index_name: \"documents\"\n      query: \"machine learning concepts\" \n      search_type: \"semantic\"                   # Dense vector search\n      limit: 3\n      basic_auth: [\"user\", \"password\"]          # Optional: Authentication\n      verify_certs: false                       # Optional: SSL verification\n      \n  # Hybrid search (combines text + vector)\n  best_search:\n    type: QueryElasticsearchStore\n    config:\n      persist_location: \"http://localhost:9200\"\n      index_name: \"knowledge_base\"\n      query: \"deep learning neural networks\"\n      search_type: \"hybrid\"                     # Best of both worlds\n      weights: [0.7, 0.3]                      # Optional: [text_weight, vector_weight]\n      limit: 5\nSearch Types: - sparse: Traditional BM25 text search - semantic: Dense vector similarity search\n- hybrid: Weighted combination of sparse + semantic results\nInput Ports: - None (queries existing storage directly)\nOutput Ports: - documents: List[Document] - Matching documents\n\n\n\n\nProcessor nodes apply AI analysis, prompts, or transformations to documents.\n\n\nApplies a custom prompt to each document using an LLM.\nnodes:\n  document_analyzer:\n    type: PromptProcessor\n    config:\n      prompt: |                               # Option 1: Inline prompt template\n        Analyze this document and provide:\n        1. Main topic: \n        2. Key findings:\n        3. Complexity (1-5):\n        \n        Source: {source}\n        Content: {content}\n      llm:                                    # New flexible LLM configuration\n        model_url: \"openai://gpt-3.5-turbo\"  # Model URL specification\n        temperature: 0.7                     # Creativity level\n        max_tokens: 1000                     # Response length limit\n      batch_size: 5                          # Optional: Process in batches\n\n  # Alternative: Load complex prompt from file with advanced LLM config\n  complex_analyzer:\n    type: PromptProcessor\n    config:\n      prompt_file: \"prompts/statute_extraction.txt\"  # Option 2: Load from file\n      llm:                                   # Advanced LLM configuration\n        model_url: \"openai://gpt-4o-mini\"    # Full model URL specification\n        temperature: 0                       # Deterministic results\n        mute_stream: true                    # Quiet processing\n        timeout: 60                          # Request timeout\n      batch_size: 2                          # Optional: Process in batches\nLoading Prompts from Files:\nFor complex prompts, you can store them in separate text files and reference them with prompt_file:\n# File: prompts/resume_parser.txt\nAnalyze the resume and extract details in JSON format:\n{\n  \"name\": \"...\",\n  \"skills\": [\"...\", \"...\"],\n  \"experience\": [...]\n}\n\nResume text: {content}\n\n# Workflow configuration\nconfig:\n  prompt_file: \"prompts/resume_parser.txt\"\nBenefits of External Prompt Files: - Better organization for complex prompts - Version control and collaboration - Reusability across workflows - Easier prompt engineering and testing\nLLM Configuration Options:\nThe llm section accepts all parameters supported by the OnPrem LLM class.\nLLM Instance Sharing:\nThe workflow engine automatically shares LLM instances between processors that use identical configurations, improving performance and memory usage:\nnodes:\n  extractor:\n    type: PromptProcessor\n    config:\n      prompt_file: \"prompts/extraction.txt\"\n      llm:\n        model_url: \"openai://gpt-4o-mini\"  # LLM instance created\n        temperature: 0\n        \n  cleaner:\n    type: ResponseCleaner  \n    config:\n      cleanup_prompt_file: \"prompts/cleanup.txt\"\n      llm:\n        model_url: \"openai://gpt-4o-mini\"  # Same instance reused!\n        temperature: 0\nConfiguration Options:\nllm:\n  # Model specification\n  model_url: \"openai://gpt-4o-mini\"      # Model URL (recommended format)\n  \n  # Generation parameters\n  temperature: 0.7                       # Randomness (0.0-2.0)\n  max_tokens: 1500                       # Maximum response length\n  top_p: 0.9                            # Nucleus sampling\n  frequency_penalty: 0.0                 # Repetition penalty\n  presence_penalty: 0.0                  # Topic diversity penalty\n  \n  # Behavior options\n  mute_stream: true                      # Suppress streaming output\n  timeout: 120                          # Request timeout in seconds\n  \n  # Provider-specific options (passed through)\n  api_key: \"${OPENAI_API_KEY}\"          # API authentication\n  base_url: \"https://api.openai.com/v1\"  # Custom API endpoint\nPrompt Variables: - {content} - Document content - {source} - Document source path - {page} - Page number (if available) - Any metadata field (e.g., {meta_author})\nInput Ports: - documents: List[Document] - Documents to process\nOutput Ports: - results: List[Dict] - Analysis results with prompt responses\n\n\n\nPost-processes and cleans LLM responses using another LLM call.\nnodes:\n  # Default usage - cleans \"response\" field from PromptProcessor results\n  response_cleaner:\n    type: ResponseCleaner\n    config:\n      source_field: \"response\"           # Optional: field to clean (default: \"response\")\n      cleanup_prompt: |                  # Inline cleanup instructions\n        Remove XML tags and clean up formatting while preserving all valid content:\n        {response}\n        \n        Keep all important information, just remove formatting artifacts.\n      llm:\n        model_url: \"openai://gpt-3.5-turbo\"\n        temperature: 0               # Deterministic cleanup\n\n  # Clean different field - useful after DocumentToResults or PythonDocumentProcessor\n  content_cleaner:\n    type: ResponseCleaner\n    config:\n      source_field: \"page_content\"       # Clean the page_content field instead\n      cleanup_prompt: |\n        Clean and standardize this document content:\n        {response}\n        \n        Remove any formatting artifacts while preserving meaning.\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0\n\n  # Alternative: Load cleanup prompt from file\n  citation_cleaner:\n    type: ResponseCleaner\n    config:\n      cleanup_prompt_file: \"prompts/statute_cleanup.txt\"  # Complex cleanup rules\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0\n        mute_stream: true\n      # source_field defaults to \"response\" if not specified\nUse Cases: - Clean LLM responses from PromptProcessor (default “response” field) - Clean document content from DocumentToResults (“page_content” field) - Clean custom outputs from PythonDocumentProcessor (any field name) - Remove formatting artifacts like XML tags or unwanted text\n- Clean statutory citations while preserving all valid references (U.S.C., Public Laws, etc.) - Standardize outputs for consistent data processing - Chain with any processor that produces text fields needing cleanup\nImportant Notes: - Cleanup prompts should be carefully designed to avoid over-aggressive cleaning - Always test with representative examples to ensure valid data isn’t removed - Consider the specific domain and format of your LLM responses\nInput Ports: - results: List[Dict] - Results from PromptProcessor to clean\nOutput Ports: - results: List[Dict] - Cleaned results (original kept in original_response field)\n\n\n\nGenerates summaries for documents using an LLM.\nnodes:\n  summarizer:\n    type: SummaryProcessor\n    config:\n      max_length: 150                        # Optional: Max summary length in words\n      model_name: \"gpt-3.5-turbo\"          # Optional: LLM model\n      llm_type: \"openai\"                   # Optional: LLM provider\nInput Ports: - documents: List[Document] - Documents to summarize\nOutput Ports: - results: List[Dict] - Summaries with metadata\n\n\n\nConverts List[Document] to List[Dict] format suitable for Exporter nodes. This node bridges Query nodes (which output documents) to Exporter nodes (which expect processing results), enabling direct export from search results.\nnodes:\n  # Basic document-to-results conversion\n  prepare_export:\n    type: DocumentToResults\n    config:\n      include_content: true                      # Optional: Include document content (default: true)\n      include_metadata: true                     # Optional: Include document metadata (default: true)\n      content_field: \"content\"                   # Optional: Name for content field (default: \"content\")\n      metadata_prefix: \"meta_\"                   # Optional: Prefix for metadata fields (default: \"meta_\")\n      flatten_metadata: true                     # Optional: Flatten metadata with prefix (default: true)\n\n  # Custom conversion configuration\n  custom_export_prep:\n    type: DocumentToResults\n    config:\n      include_content: false                     # Exclude content, metadata only\n      content_field: \"document_text\"             # Custom content field name\n      metadata_prefix: \"doc_\"                    # Custom metadata prefix\n      custom_fields:                             # Optional: Add static fields\n        export_date: \"2025-01-16\"\n        processed_by: \"workflow_v2\"\n        department: \"research\"\nUse Cases: - Direct Query Export - Query → Convert → Export in 3 nodes - Search Result Analysis - Export search results with metadata for analysis - Document Cataloging - Create spreadsheets of document collections - Content Audit - Export document inventories with metadata - Research Data Collection - Prepare search results for external analysis tools\nConfiguration Options: - include_content: Whether to include document text content - include_metadata: Whether to include document metadata - content_field: Field name for document content (default: “content”) - metadata_prefix: Prefix for metadata fields (default: “meta_”) - flatten_metadata: Flatten metadata with prefix vs nested object - custom_fields: Dictionary of static fields to add to all results\nOutput Format: Each document becomes a dictionary with: - document_id: Sequential ID (0, 1, 2, …) - source: Document source path - content: Document content (if included) - content_length: Length of document content - meta_*: Flattened metadata fields with prefix - Custom fields from configuration\nExample Workflow - Direct Query Export:\nnodes:\n  search_docs:\n    type: QueryDualStore\n    config:\n      persist_location: \"vectordb\"\n      query: \"artificial intelligence trends\"\n      search_type: \"hybrid\"\n      limit: 20\n\n  prepare_export:\n    type: DocumentToResults\n    config:\n      content_field: \"document_text\"\n      metadata_prefix: \"\"  # No prefix for cleaner column names\n      custom_fields:\n        search_query: \"artificial intelligence trends\"\n        export_date: \"2025-01-16\"\n\n  export_results:\n    type: ExcelExporter\n    config:\n      output_path: \"ai_research_papers.xlsx\"\n      sheet_name: \"Search_Results\"\n\nconnections:\n  - from: search_docs\n    from_port: documents\n    to: prepare_export\n    to_port: documents\n    \n  - from: prepare_export\n    from_port: results\n    to: export_results\n    to_port: results\nInput Ports: - documents: List[Document] - Documents to convert\nOutput Ports: - results: List[Dict] - Export-ready results with content and metadata\n\n\n\nExecutes custom Python code on documents with proper security controls, allowing unlimited customization of document processing logic.\nnodes:\n  # Inline Python code\n  custom_analyzer:\n    type: PythonDocumentProcessor\n    config:\n      code: |\n        # Available variables:\n        # - doc: Document object\n        # - content: doc.page_content (string)\n        # - metadata: doc.metadata (dict) \n        # - document_id: index of document (int)\n        # - source: source file path (string)\n        # - result: dictionary to populate (dict)\n        \n        # Extract key information (re module is pre-imported)\n        word_count = len(content.split())\n        sentence_count = len(re.findall(r'[.!?]+', content))\n        \n        # Find email addresses\n        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n        emails = re.findall(email_pattern, content)\n        \n        # Populate result dictionary\n        result['analysis'] = {\n            'word_count': word_count,\n            'sentence_count': sentence_count,\n            'emails_found': emails,\n            'document_length': len(content)\n        }\n        result['processing_status'] = 'completed'\n\n  # Load Python code from external file\n  external_processor:\n    type: PythonDocumentProcessor\n    config:\n      code_file: \"scripts/document_analyzer.py\"\nAvailable Python Environment: - Built-in functions: len, str, int, float, bool, list, dict, min, max, etc. - Safe modules: re, json, math, datetime (pre-imported) - Document class: Available for creating new Document objects - Security: No file I/O, network access, or system operations\nInput Ports: - documents: List[Document] - Documents to process\nOutput Ports: - results: List[Dict] - Processing results with custom analysis\n\n\n\nExecutes custom Python code on processing results, enabling post-processing and enhancement of analysis results.\nnodes:\n  result_enhancer:\n    type: PythonResultProcessor\n    config:\n      code: |\n        # Available variables:\n        # - result: original result dictionary (modifiable copy)\n        # - original_result: read-only original result\n        # - result_id: index of result (int) \n        # - processed_result: dictionary to populate (dict)\n        \n        # Enhance analysis results\n        analysis = result.get('analysis', {})\n        word_count = analysis.get('word_count', 0)\n        \n        # Categorize document by length\n        if word_count &lt; 100:\n            category = 'short'\n        elif word_count &lt; 500:\n            category = 'medium'\n        else:\n            category = 'long'\n        \n        # Create enhanced result\n        processed_result['enhanced_analysis'] = {\n            'original_analysis': analysis,\n            'document_category': category,\n            'complexity_score': min(10, word_count // 50),\n            'has_emails': len(analysis.get('emails_found', [])) &gt; 0\n        }\n        \n        # Add summary\n        processed_result['summary'] = f\"Document categorized as '{category}'\"\nVariable Naming Conventions: - Document Processor: Populate the result dictionary with your analysis - Result Processor: Populate the processed_result dictionary with enhanced data\nInput Ports: - results: List[Dict] - Results to process\nOutput Ports: - results: List[Dict] - Enhanced processing results\n\n\n\nAggregates multiple processing results into a single collapsed result using LLM-based analysis. Perfect for creating summaries of summaries, extracting top themes from multiple responses, or producing executive summaries from document collections.\nnodes:\n  # Topic aggregation - analyze topic keywords across documents\n  topic_aggregator:\n    type: AggregatorNode\n    config:\n      prompt: |\n        Analyze these {num_results} topic keywords from different documents and identify the top 5 most important topics:\n        \n        {responses}\n        \n        Please provide:\n        1. TOP TOPICS: List the 5 most important topics with frequency\n        2. TRENDS: What patterns do you see across documents?\n        3. SUMMARY: One sentence summary of the overall theme\n        \n        Format your response clearly with headings.\n      llm:\n        model_url: \"openai://gpt-3.5-turbo\"\n        temperature: 0.3\n        max_tokens: 500\n\n  # Summary aggregation - create summary of summaries\n  meta_summarizer:\n    type: AggregatorNode\n    config:\n      prompt_file: \"prompts/summary_aggregation.txt\"  # Load from file\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0.1\nUse Cases: - Topic Analysis - Individual documents → topic keywords → top themes across collection - Executive Summaries - Document summaries → executive summary of key findings - Survey Analysis - Individual responses → overall trends and insights - Meeting Analysis - Multiple meeting notes → key decisions and action items - Research Synthesis - Paper summaries → research landscape overview\nInput Ports: - results: List[Dict] - Multiple results to aggregate (from PromptProcessor, SummaryProcessor, etc.)\nOutput Ports:\n- result: Dict - Single aggregated result containing: - aggregated_response: LLM’s aggregated analysis - source_count: Number of original results processed - aggregation_method: “llm_prompt” - original_results: Reference to source data\n\n\n\nAggregates multiple processing results using custom Python code for precise control over aggregation logic. Ideal for statistical analysis, data consolidation, and algorithmic aggregation.\nnodes:\n  # Topic frequency analysis\n  topic_frequency_analyzer:\n    type: PythonAggregatorNode\n    config:\n      code: |\n        # Available variables:\n        # - results: list of all result dictionaries\n        # - num_results: number of results\n        # - result: dictionary to populate with aggregated result\n        \n        # Count topic frequency across all responses\n        topic_counts = {}\n        total_responses = 0\n        \n        for res in results:\n            response = res.get('response', '')\n            if response:\n                topics = [t.strip() for t in response.split(',') if t.strip()]\n                total_responses += 1\n                for topic in topics:\n                    topic_counts[topic] = topic_counts.get(topic, 0) + 1\n        \n        # Get top topics by frequency\n        sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n        top_topics = sorted_topics[:5]\n        \n        # Calculate statistics\n        result['topic_analysis'] = {\n            'top_topics': [\n                {'topic': topic, 'frequency': count, 'percentage': round(count/total_responses*100, 1)}\n                for topic, count in top_topics\n            ],\n            'total_unique_topics': len(topic_counts),\n            'total_documents': total_responses,\n            'coverage': f\"{len([t for t in topic_counts.values() if t &gt; 1])} topics appear in multiple documents\"\n        }\n        \n        result['aggregation_summary'] = f\"Analyzed {total_responses} documents, found {len(topic_counts)} unique topics\"\n\n  # Statistical summary aggregator  \n  statistical_aggregator:\n    type: PythonAggregatorNode\n    config:\n      code: |\n        # Aggregate numerical metrics from document analysis\n        import json\n        import math\n        \n        word_counts = []\n        sentiment_scores = []\n        complexity_scores = []\n        \n        for res in results:\n            # Extract metrics from results\n            metadata = res.get('metadata', {})\n            analysis = res.get('analysis', {})\n            \n            if 'word_count' in metadata:\n                word_counts.append(metadata['word_count'])\n            \n            if 'sentiment_score' in analysis:\n                sentiment_scores.append(analysis['sentiment_score'])\n                \n            if 'complexity_score' in analysis:\n                complexity_scores.append(analysis['complexity_score'])\n        \n        # Calculate statistics\n        def calc_stats(values):\n            if not values:\n                return {}\n            return {\n                'count': len(values),\n                'mean': sum(values) / len(values),\n                'median': sorted(values)[len(values)//2],\n                'min': min(values),\n                'max': max(values),\n                'std_dev': math.sqrt(sum((x - sum(values)/len(values))**2 for x in values) / len(values))\n            }\n        \n        result['statistical_summary'] = {\n            'document_metrics': {\n                'total_documents': len(results),\n                'word_count_stats': calc_stats(word_counts),\n                'sentiment_stats': calc_stats(sentiment_scores),\n                'complexity_stats': calc_stats(complexity_scores)\n            },\n            'data_quality': {\n                'documents_with_word_count': len(word_counts),\n                'documents_with_sentiment': len(sentiment_scores),\n                'documents_with_complexity': len(complexity_scores)\n            }\n        }\n        \n        # Create readable summary\n        avg_words = result['statistical_summary']['document_metrics']['word_count_stats'].get('mean', 0)\n        result['executive_summary'] = f\"Processed {len(results)} documents, average length: {avg_words:.0f} words\"\nUse Cases: - Topic Frequency Analysis - Count and rank topics across document collections - Statistical Aggregation - Calculate means, medians, distributions from analysis results - Data Consolidation - Merge and deduplicate information from multiple sources - Custom Scoring - Apply business logic to rank or categorize results - Format Conversion - Transform aggregated data into specific output formats\nAvailable Python Environment: - Built-in functions: len, str, int, float, bool, list, dict, min, max, sum, etc. - Safe modules: re, json, math, datetime (pre-imported) - Security: No file I/O, network access, or system operations\nInput Ports: - results: List[Dict] - Multiple results to aggregate\nOutput Ports: - result: Dict - Single aggregated result containing: - Custom fields based on your aggregation logic - source_count: Number of original results processed - aggregation_method: “python_code”\n\n\n\n\nExporter nodes save processed results to various file formats.\n\n\nExports results to CSV format for spreadsheet analysis.\nnodes:\n  csv_output:\n    type: CSVExporter\n    config:\n      output_path: \"results.csv\"            # Optional: Output file (default: results.csv)\n      columns: [\"source\", \"response\"]       # Optional: Columns to include (default: all)\nInput Ports: - results: List[Dict] - Results to export\nOutput Ports: - status: str - Export status message\n\n\n\nExports results to Excel format with formatting support.\nnodes:\n  excel_output:\n    type: ExcelExporter\n    config:\n      output_path: \"analysis.xlsx\"          # Optional: Output file (default: results.xlsx)\n      sheet_name: \"Document_Analysis\"       # Optional: Sheet name (default: Results)\nInput Ports: - results: List[Dict] - Results to export\nOutput Ports: - status: str - Export status message\n\n\n\nExports results to JSON format for programmatic access. Can handle both regular processing results and single aggregated results.\nnodes:\n  json_output:\n    type: JSONExporter\n    config:\n      output_path: \"results.json\"           # Optional: Output file (default: results.json)\n      pretty_print: true                    # Optional: Format JSON nicely (default: true)\nInput Ports: - results: List[Dict] - Multiple results to export (from processors) - result: Dict - Single aggregated result to export (from aggregators)\nOutput Ports: - status: str - Export status message\n\n\n\nExtracts JSON content from LLM responses and exports as clean JSON array. Ideal for structured data extraction workflows where LLM responses contain JSON objects that need to be exported without metadata.\nnodes:\n  extract_resume_data:\n    type: JSONResponseExporter\n    config:\n      output_path: \"extracted_resumes.json\"     # Optional: Output file (default: extracted_responses.json)\n      pretty_print: true                        # Optional: Format JSON nicely (default: true)\n      response_field: \"response\"                # Optional: Field containing JSON response (default: \"response\")\nKey Features: - JSON Extraction: Uses llm.helpers.extract_json to parse JSON from text responses - Field Flexibility: Searches multiple response fields (“response”, “aggregated_response”, “output”) - Error Handling: Includes raw response and error info when JSON extraction fails - Clean Output: Exports only the extracted JSON data, removing all metadata\nInput Ports: - results: List[Dict] - Multiple results with JSON responses to extract (from processors)\n- result: Dict - Single aggregated result with JSON response to extract (from aggregators)\nOutput Ports: - status: str - Export status message\nExample Use Case - Resume Parsing:\n# Extract structured data from resume analysis\nextract_structured_resumes:\n  type: JSONResponseExporter\n  config:\n    output_path: \"candidate_profiles.json\"\n    response_field: \"analysis\"\nWhere LLM responses like:\n{\"name\": \"John Doe\", \"skills\": [\"Python\", \"Machine Learning\"], \"experience\": \"5 years\"}\nAre exported as a clean JSON array without workflow metadata.\n\n\n\n\n\n\n\n\n\nconfig:\n  pdf_markdown: true          # Convert PDFs to markdown format\n  pdf_unstructured: false     # Use unstructured parsing for complex PDFs\n  infer_table_structure: true # Extract and preserve table structure\n\n\n\nconfig:\n  store_md5: true            # Add MD5 hash to document metadata\n  store_mimetype: true       # Add MIME type to document metadata  \n  store_file_dates: true     # Add creation/modification dates\n  extract_document_titles: true  # Extract document titles (requires LLM)\n\n\n\nconfig:\n  n_proc: 4                  # Use 4 CPU cores for parallel processing\n  verbose: true              # Show detailed progress information\n  batch_size: 1000          # Process documents in batches\n\n\n\n\nDifferent storage backends support different configuration options:\n# ChromaDB\nconfig:\n  persist_location: \"./chroma_db\"\n  collection_name: \"documents\"\n  \n# Whoosh\nconfig:\n  persist_location: \"./whoosh_index\"\n  schema_fields: [\"content\", \"title\", \"source\"]\n\n# Elasticsearch\nconfig:\n  persist_location: \"https://elastic:password@localhost:9200\"\n  index_name: \"document_index\"\n  basic_auth: [\"username\", \"password\"]\n\n\n\n\n\n\nProcess documents from multiple sources with different strategies:\nnodes:\n  # Load PDFs with table extraction\n  pdf_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"pdfs/\"\n      pdf_markdown: true\n      infer_table_structure: true\n  \n  # Load text files\n  text_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"texts/\"\n  \n  # Chunk PDFs by paragraph (preserve structure)\n  pdf_chunker:\n    type: SplitByParagraph\n    config:\n      chunk_size: 1000\n      chunk_overlap: 100\n  \n  # Chunk text files by character count\n  text_chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 500\n      chunk_overlap: 50\n  \n  # Store everything in unified index\n  unified_store:\n    type: WhooshStore\n    config:\n      persist_location: \"unified_index\"\n\nconnections:\n  - from: pdf_loader\n    from_port: documents\n    to: pdf_chunker\n    to_port: documents\n  \n  - from: text_loader\n    from_port: documents\n    to: text_chunker\n    to_port: documents\n  \n  - from: pdf_chunker\n    from_port: documents\n    to: unified_store\n    to_port: documents\n  \n  - from: text_chunker\n    from_port: documents\n    to: unified_store\n    to_port: documents\n\n\n\nMultiple processing steps in sequence:\nnodes:\n  loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"documents/\"\n      extract_document_titles: true\n  \n  # First pass: large chunks for context\n  coarse_chunker:\n    type: SplitByParagraph\n    config:\n      chunk_size: 2000\n      chunk_overlap: 200\n  \n  # Second pass: fine chunks for retrieval\n  fine_chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 400\n      chunk_overlap: 40\n  \n  # Store in vector database\n  vector_store:\n    type: ChromaStore\n    config:\n      persist_location: \"vector_db\"\n\nconnections:\n  - from: loader\n    from_port: documents\n    to: coarse_chunker\n    to_port: documents\n  \n  - from: coarse_chunker\n    from_port: documents\n    to: fine_chunker\n    to_port: documents\n  \n  - from: fine_chunker\n    from_port: documents\n    to: vector_store\n    to_port: documents\n\n\n\nDownload and process documents from URLs:\nnodes:\n  web_loader:\n    type: LoadWebDocument\n    config:\n      url: \"https://example.com/report.pdf\"\n      username: \"api_user\"\n      password: \"secret_key\"\n  \n  doc_processor:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 800\n      chunk_overlap: 80\n  \n  search_store:\n    type: ElasticsearchStore\n    config:\n      persist_location: \"http://elasticsearch:9200\"\n      index_name: \"web_documents\"\n\nconnections:\n  - from: web_loader\n    from_port: documents\n    to: doc_processor\n    to_port: documents\n  \n  - from: doc_processor\n    from_port: documents\n    to: search_store\n    to_port: documents\n\n\n\nProcess documents with comprehensive metadata enrichment and content transformation:\nnodes:\n  # Load meeting documents\n  meeting_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"meeting_docs/\"\n      include_patterns: [\"*.pdf\", \"*.docx\", \"*.txt\"]\n      \n  # Tag all documents with meeting metadata\n  tag_meeting_info:\n    type: AddMetadata\n    config:\n      metadata:\n        meeting_id: \"meeting20251001\"\n        department: \"engineering\"\n        project: \"Project Alpha\"\n        classification: \"internal\"\n        attendees: \"team_leads\"\n        \n  # Add confidential header to all documents\n  mark_confidential:\n    type: ContentPrefix\n    config:\n      prefix: \"[CONFIDENTIAL - PROJECT ALPHA TEAM ONLY]\"\n      separator: \"\\n\\n\"\n      \n  # Extract key information and enrich metadata\n  analyze_content:\n    type: PythonDocumentTransformer\n    config:\n      code: |\n        # Analyze document content for key information\n        import re\n        \n        # Basic statistics\n        word_count = len(content.split())\n        paragraph_count = len([p for p in content.split('\\n\\n') if p.strip()])\n        \n        # Look for action items and decisions\n        action_items = len(re.findall(r'(?:action item|todo|task):', content, re.IGNORECASE))\n        decisions = len(re.findall(r'(?:decision|resolved|agreed):', content, re.IGNORECASE))\n        \n        # Find mentions of team members\n        team_members = re.findall(r'@(\\w+)', content)\n        \n        # Classify document type\n        content_lower = content.lower()\n        if 'agenda' in content_lower:\n            doc_type = 'meeting_agenda'\n        elif action_items &gt; 0 or 'action' in content_lower:\n            doc_type = 'action_items'\n        elif 'minutes' in content_lower or 'notes' in content_lower:\n            doc_type = 'meeting_minutes'\n        else:\n            doc_type = 'meeting_document'\n        \n        # Update metadata with extracted information\n        metadata.update({\n            'word_count': word_count,\n            'paragraph_count': paragraph_count,\n            'action_items_count': action_items,\n            'decisions_count': decisions,\n            'mentioned_members': list(set(team_members)),\n            'document_type': doc_type,\n            'priority_score': min(10, (action_items * 2) + decisions),\n            'has_action_items': action_items &gt; 0,\n            'complexity': 'high' if word_count &gt; 1000 else 'medium' if word_count &gt; 300 else 'low'\n        })\n        \n        # Add document summary at the beginning\n        summary = f\"[{doc_type.upper()}: {word_count} words, {action_items} action items, Priority: {metadata['priority_score']}/10]\"\n        content = summary + \"\\n\\n\" + content\n        \n        # Create enriched document\n        transformed_doc = Document(\n            page_content=content,\n            metadata=metadata\n        )\n        \n  # Filter to keep only relevant documents\n  filter_important:\n    type: DocumentFilter\n    config:\n      metadata_filters:\n        classification: \"internal\"\n      # Keep documents with action items or decisions\n      content_contains: [\"action\", \"decision\", \"task\", \"todo\"]\n      min_length: 100\n      \n  # Add processing footer\n  add_footer:\n    type: ContentSuffix\n    config:\n      suffix: |\n        \n        ---\n        Document processed: 2025-01-16\n        Meeting ID: meeting20251001\n        Next review: 2025-01-23\n        Contact: project-alpha-admin@company.com\n        \n  # Chunk for storage\n  chunk_docs:\n    type: SplitByParagraph\n    config:\n      chunk_size: 1000\n      chunk_overlap: 100\n      \n  # Store enriched documents\n  meeting_store:\n    type: WhooshStore\n    config:\n      persist_location: \"meeting_20251001_index\"\n\nconnections:\n  - from: meeting_loader\n    from_port: documents\n    to: tag_meeting_info\n    to_port: documents\n    \n  - from: tag_meeting_info\n    from_port: documents\n    to: mark_confidential\n    to_port: documents\n    \n  - from: mark_confidential\n    from_port: documents\n    to: analyze_content\n    to_port: documents\n    \n  - from: analyze_content\n    from_port: documents\n    to: filter_important\n    to_port: documents\n    \n  - from: filter_important\n    from_port: documents\n    to: add_footer\n    to_port: documents\n    \n  - from: add_footer\n    from_port: documents\n    to: chunk_docs\n    to_port: documents\n    \n  - from: chunk_docs\n    from_port: documents\n    to: meeting_store\n    to_port: documents\nThis example demonstrates the power of DocumentTransformer nodes:\n\nMetadata Tagging - Organizes documents by meeting, project, and department\nContent Marking - Adds confidential headers for security\nIntelligent Analysis - Extracts action items, decisions, and team mentions\nQuality Filtering - Keeps only documents with actionable content\nProcessing Attribution - Adds footer with processing information\nSearchable Storage - Creates indexed, searchable document collection\n\nThe enriched metadata enables powerful queries like: - “Find all documents from meeting20251001 with action items” - “Show high-priority engineering documents from Project Alpha” - “List all documents mentioning specific team members”\n\n\n\nA real-world example that extracts statutory citations from Federal Acquisition Regulation (FAR) documents using specialized legal analysis prompts:\n# Federal Acquisition Regulation (FAR) Legal Analysis\n# Extracts statutory citations from FAR HTML files using LLM analysis and cleanup\n\nnodes:\n  # Load FAR HTML files (Part 9 - contractor qualifications)\n  far_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"/tmp/far_data\"  # Directory containing extracted FAR HTML files\n      include_patterns: [\"9.*.html\"]     # Focus on Part 9 sections\n      verbose: true\n  \n  # Keep each FAR section as complete document (no chunking needed)\n  full_sections:\n    type: KeepFullDocument\n    config: {}\n  \n  # Apply statute extraction prompt directly to all FAR sections\n  statute_extractor:\n    type: PromptProcessor\n    config:\n      prompt_file: \"prompts/statute_extraction.txt\"  # Complex legal analysis prompt\n      llm:\n        model_url: \"openai://gpt-4o-mini\"  # GPT-4o-mini for accurate legal analysis\n        temperature: 0                     # Deterministic results for legal work\n        mute_stream: true                  # Quiet processing\n      batch_size: 5                        # Process 5 sections at a time\n  \n  # Clean up the raw LLM responses to extract just the citations\n  citation_cleaner:\n    type: ResponseCleaner\n    config:\n      cleanup_prompt_file: \"prompts/statute_cleanup.txt\"  # Specialized cleanup prompt\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0\n        mute_stream: true\n  \n  # Export cleaned results to Excel spreadsheet\n  statute_analysis:\n    type: ExcelExporter\n    config:\n      output_path: \"far_statutory_citations_cleaned.xlsx\"\n      sheet_name: \"FAR_Part9_Analysis\"\n\nconnections:\n  # Pipeline: Load -&gt; Process -&gt; Extract -&gt; Cleanup -&gt; Export\n  - from: far_loader\n    from_port: documents\n    to: full_sections\n    to_port: documents\n  \n  - from: full_sections\n    from_port: documents\n    to: statute_extractor\n    to_port: documents\n  \n  - from: statute_extractor\n    from_port: results\n    to: citation_cleaner\n    to_port: results\n  \n  - from: citation_cleaner\n    from_port: results\n    to: statute_analysis\n    to_port: results\nData Preparation (run before workflow):\n# 1. Download FAR data\nwget https://www.acquisition.gov/sites/default/files/current/far/zip/html/FARHTML.zip\n\n# 2. Extract to processing directory\nunzip FARHTML.zip -d /tmp/far_data/\n\n# 3. Run the analysis workflow\npython -m onprem.workflow far_legal_analysis_simple.yaml\nKey Features: - Specialized Legal Processing: Uses domain-specific prompts for accurate legal text analysis - Two-Stage LLM Pipeline: Initial extraction followed by response cleanup for precision - External Prompt Files: Complex prompts stored in separate files for maintainability - Deterministic Results: Zero temperature ensures consistent legal analysis - Structured Output: Excel export with organized columns for legal review\nExpected Results: - Excel file with statutory citations extracted from Part 9 sections - Columns: document_id, source, response (cleaned citations), metadata - Ready for legal team review and compliance analysis\nThis example demonstrates how workflow pipelines can handle complex, domain-specific document analysis tasks with multiple LLM processing stages and specialized prompts.\n\n\n\n\nThe workflow engine performs comprehensive validation:\n\n\n\nPort Existence: Verifies source and target ports exist\nType Compatibility: Ensures data types match between connections\nNode Compatibility: Enforces valid connection patterns:\n\n✅ Loader → TextSplitter\n✅ Loader → DocumentTransformer\n✅ TextSplitter → TextSplitter\n\n✅ TextSplitter → DocumentTransformer\n✅ TextSplitter → Storage\n✅ DocumentTransformer → TextSplitter\n✅ DocumentTransformer → DocumentTransformer\n\n✅ DocumentTransformer → Storage\n✅ Query → DocumentTransformer\n❌ Loader → Storage (must have TextSplitter or DocumentTransformer in between)\n❌ Storage → Any (Storage nodes are terminal)\n\n\n\n\n\n\nFile Existence: Checks that source directories and files exist\nConfiguration Validation: Validates required parameters\nDependency Resolution: Uses topological sorting to determine execution order\nCycle Detection: Prevents infinite loops in workflows\n\n\n\n\nThe engine provides detailed error messages:\n# Invalid node type\nWorkflowValidationError: Unknown node type: InvalidNodeType\n\n# Missing required configuration\nNodeExecutionError: Node my_loader: source_directory is required\n\n# Invalid connection\nWorkflowValidationError: Loader node loader can only connect to TextSplitter nodes, not ChromaStoreNode\n\n# Type mismatch\nWorkflowValidationError: Type mismatch: loader.documents (List[Document]) -&gt; storage.text (str)\n\n# Missing port\nWorkflowValidationError: Source node loader has no output port 'data'. Available: ['documents']\n\n\n\nQuery existing storage, apply AI analysis, and export to spreadsheet:\nnodes:\n  # Search for research documents\n  research_query:\n    type: QueryWhooshStore\n    config:\n      persist_location: \"research_index\"\n      query: \"methodology results conclusions findings\"\n      limit: 25\n  \n  # Analyze each document with custom prompts\n  research_analysis:\n    type: PromptProcessor\n    config:\n      prompt: |\n        Analyze this research document and extract:\n        \n        1. RESEARCH QUESTION: What is the main research question?\n        2. METHODOLOGY: What research methods were used?\n        3. KEY FINDINGS: What are the 3 most important findings?\n        4. LIMITATIONS: What limitations are mentioned?\n        5. CONFIDENCE: How confident are the conclusions (High/Medium/Low)?\n        \n        Document: {source}\n        Content: {content}\n        \n        Please format each answer on a separate line.\n      model_name: \"gpt-4\"\n      batch_size: 3\n  \n  # Export to Excel for review and analysis\n  analysis_report:\n    type: ExcelExporter\n    config:\n      output_path: \"research_analysis_report.xlsx\"\n      sheet_name: \"Document_Analysis\"\n\nconnections:\n  - from: research_query\n    from_port: documents\n    to: research_analysis\n    to_port: documents\n  \n  - from: research_analysis\n    from_port: results\n    to: analysis_report\n    to_port: results\n\n\n\nExtract individual document topics and create an aggregated analysis:\nnodes:\n  # Load documents from directory\n  document_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"../sample_data\"\n      include_patterns: [\"*.pdf\", \"*.txt\", \"*.html\"]\n      pdf_markdown: true\n      store_file_dates: true\n  \n  # Keep documents whole for comprehensive analysis\n  full_document_processor:\n    type: KeepFullDocument\n    config:\n      concatenate_pages: true\n  \n  # Extract topic keyword from each document\n  document_analyzer:\n    type: PromptProcessor\n    config:\n      prompt: |\n        Provide a single short keyword or keyphrase that captures the topic of the following text from {source} (only output the keyphrase without quotes and nothing else): {content}\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0.2\n        mute_stream: true\n      batch_size: 2\n  \n  # Clean up responses for consistency\n  response_cleaner:\n    type: ResponseCleaner\n    config:\n      cleanup_prompt: |\n        Clean up this analysis response to ensure consistent formatting:\n        {original_response}\n        \n        Requirements:\n        - Ensure the response is a single short keyword or keyphrase that captures the topic and nothing else.\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0\n        mute_stream: true\n  \n  # Aggregate all topics into comprehensive analysis\n  topic_aggregator:\n    type: AggregatorNode\n    config:\n      prompt: |\n        Analyze these {num_results} topic keywords from different documents and provide a comprehensive topic analysis:\n        \n        {responses}\n        \n        Please provide your analysis in the following structure:\n        1. TOP TOPICS: List the 5 most frequently mentioned topics with their frequency counts\n        2. TOPIC CATEGORIES: Group related topics into broader categories (e.g., Technology, Business, etc.)\n        3. COVERAGE ANALYSIS: What percentage of documents cover each major theme?\n        4. INSIGHTS: What patterns or trends do you observe across the document collection?\n        5. EXECUTIVE SUMMARY: One paragraph summarizing the overall thematic landscape\n        \n        Format your response with clear headings and bullet points for easy reading.\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0.3\n        max_tokens: 1000\n        mute_stream: true\n  \n  # Export individual document topics to CSV\n  individual_topics_export:\n    type: CSVExporter\n    config:\n      output_path: \"document_analysis_summary.csv\"\n  \n  # Export aggregated analysis to JSON\n  aggregated_export:\n    type: JSONExporter\n    config:\n      output_path: \"document_topic_analysis.json\"\n      pretty_print: true\n\nconnections:\n  # Pipeline: Load → Process → Analyze → Clean → Export Both Ways\n  - from: document_loader\n    from_port: documents\n    to: full_document_processor\n    to_port: documents\n  \n  - from: full_document_processor\n    from_port: documents\n    to: document_analyzer\n    to_port: documents\n  \n  - from: document_analyzer\n    from_port: results\n    to: response_cleaner\n    to_port: results\n  \n  # Export individual document topics to CSV\n  - from: response_cleaner\n    from_port: results\n    to: individual_topics_export\n    to_port: results\n  \n  # Aggregate topics for comprehensive analysis\n  - from: response_cleaner\n    from_port: results\n    to: topic_aggregator\n    to_port: results\n  \n  # Export aggregated analysis to JSON (note: result → result port)\n  - from: topic_aggregator\n    from_port: result\n    to: aggregated_export\n    to_port: result\nThis example demonstrates the power of aggregation workflows:\nBenefits: - Individual Analysis: CSV file with topic keyword for each document - Collection Insights: JSON file with aggregated analysis of all topics - Dual Output: Both granular and summary views of your document collection - Executive Summary: High-level insights perfect for reporting and decision-making\nOutput Files: - document_analysis_summary.csv - Individual document topics (one row per document) - document_topic_analysis.json - Aggregated topic analysis with patterns and insights\nKey Pattern: Documents → Individual Analysis → Split → (CSV Export + Aggregation → JSON Export)\n\n\n\nExport search results directly from vector stores without additional processing, perfect for document cataloging, research data collection, and content audits:\nnodes:\n  # Search for documents matching specific criteria\n  research_search:\n    type: QueryDualStore\n    config:\n      persist_location: \"~/vectordb\"\n      query: \"machine learning neural networks deep learning\"\n      search_type: \"hybrid\"\n      limit: 50\n      weights: [0.7, 0.3]  # Favor semantic search\n\n  # Convert documents to export-ready format\n  prepare_export:\n    type: DocumentToResults\n    config:\n      include_content: true\n      content_field: \"document_content\"\n      metadata_prefix: \"\"  # No prefix for cleaner column names\n      custom_fields:\n        search_query: \"machine learning neural networks deep learning\"\n        export_timestamp: \"2025-01-16T10:30:00Z\"\n        search_type: \"hybrid\"\n        analyst: \"research_team\"\n        project: \"ML_Literature_Review\"\n\n  # Export to Excel for analysis and sharing\n  research_export:\n    type: ExcelExporter\n    config:\n      output_path: \"ml_research_documents.xlsx\"\n      sheet_name: \"Search_Results\"\n\n  # Also export to CSV for data processing\n  data_export:\n    type: CSVExporter\n    config:\n      output_path: \"ml_research_data.csv\"\n\n  # And to JSON for programmatic access\n  json_export:\n    type: JSONExporter\n    config:\n      output_path: \"ml_research_index.json\"\n      pretty_print: true\n\nconnections:\n  # Single search feeds multiple export formats\n  - from: research_search\n    from_port: documents\n    to: prepare_export\n    to_port: documents\n\n  - from: prepare_export\n    from_port: results\n    to: research_export\n    to_port: results\n\n  - from: prepare_export\n    from_port: results\n    to: data_export\n    to_port: results\n\n  - from: prepare_export\n    from_port: results\n    to: json_export\n    to_port: results\nBenefits: - Simple Pipeline: Only 3 nodes for Query → Convert → Export - Multiple Formats: Single search exported to Excel, CSV, and JSON simultaneously - Rich Metadata: All document metadata preserved and exported - Search Tracking: Custom fields track search parameters and export details - Ready for Analysis: Excel format perfect for manual review and annotation\nOutput Files: - ml_research_documents.xlsx - Formatted spreadsheet for manual analysis - ml_research_data.csv - Raw data for statistical analysis or import into other tools - ml_research_index.json - Structured data for programmatic processing\nUse Cases: - Literature Reviews - Export relevant papers with metadata for analysis - Document Audits - Create inventories of document collections - Research Data Collection - Gather documents matching specific criteria - Content Cataloging - Build searchable catalogs of document libraries - Compliance Reporting - Export document lists with metadata for regulatory review\nKey Pattern: Query → Convert → Multiple Exports\n\n\n\nProcess documents and export results in multiple formats:\nnodes:\n  # Query for AI/ML papers\n  ai_papers:\n    type: QueryChromaStore\n    config:\n      persist_location: \"vector_db\"\n      query: \"artificial intelligence machine learning neural networks\"\n      limit: 15\n  \n  # Generate structured summaries\n  paper_summaries:\n    type: SummaryProcessor\n    config:\n      max_length: 200\n      model_name: \"gpt-3.5-turbo\"\n  \n  # Export to CSV for spreadsheet analysis\n  csv_export:\n    type: CSVExporter\n    config:\n      output_path: \"ai_paper_summaries.csv\"\n      columns: [\"document_id\", \"source\", \"summary\", \"original_length\"]\n  \n  # Export to JSON for programmatic access\n  json_export:\n    type: JSONExporter\n    config:\n      output_path: \"ai_paper_summaries.json\"\n      pretty_print: true\n  \n  # Export to Excel for presentation\n  excel_export:\n    type: ExcelExporter\n    config:\n      output_path: \"ai_paper_report.xlsx\"\n      sheet_name: \"AI_ML_Summaries\"\n\nconnections:\n  # Single source, multiple outputs\n  - from: ai_papers\n    from_port: documents\n    to: paper_summaries\n    to_port: documents\n  \n  - from: paper_summaries\n    from_port: results\n    to: csv_export\n    to_port: results\n  \n  - from: paper_summaries\n    from_port: results\n    to: json_export\n    to_port: results\n  \n  - from: paper_summaries\n    from_port: results\n    to: excel_export\n    to_port: results\n\n\n\n\n\n\n# Use descriptive node names\nnodes:\n  legal_document_loader:        # Not: loader1\n    type: LoadFromFolder\n    config:\n      source_directory: \"legal_docs/\"\n  \n  contract_chunker:            # Not: splitter1\n    type: SplitByParagraph\n    config:\n      chunk_size: 1500\n  \n  contract_search_index:       # Not: storage1\n    type: WhooshStore\n    config:\n      persist_location: \"contract_index\"\n\n\n\nUse environment variables for paths and sensitive data:\nnodes:\n  loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"${DOCUMENT_PATH:-./documents}\"  # Default fallback\n  \n  es_store:\n    type: ElasticsearchStore\n    config:\n      persist_location: \"${ELASTICSEARCH_URL}\"\n      basic_auth: [\"${ES_USERNAME}\", \"${ES_PASSWORD}\"]\n\n\n\nChoose appropriate chunk sizes for your use case:\n# Small chunks (200-400 chars) - Good for:\n# - Precise retrieval\n# - Question answering\n# - Fine-grained search\n\n# Medium chunks (500-1000 chars) - Good for:\n# - General purpose RAG\n# - Balanced context/precision\n# - Most common use case\n\n# Large chunks (1000-2000 chars) - Good for:\n# - Document summarization\n# - Context-heavy tasks\n# - Preserving document structure\n\n\n\nnodes:\n  bulk_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"large_corpus/\"\n      n_proc: 8                    # Parallel processing\n      verbose: false               # Reduce logging overhead\n      batch_size: 500             # Process in batches\n  \n  efficient_chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 500\n      chunk_overlap: 25           # Reduce overlap for speed\n\n\n\nnodes:\n  rich_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"documents/\"\n      store_md5: true              # Document integrity\n      store_mimetype: true         # File type tracking\n      store_file_dates: true       # Temporal information\n      extract_document_titles: true # Content-aware metadata\n      infer_table_structure: true  # Preserve structure\n\n\n\n\n\n\n\n\n# Ensure you're in the correct directory\ncd /path/to/onprem/project\n\n# Or add to Python path\nexport PYTHONPATH=/path/to/onprem:$PYTHONPATH\n\n\n\n# Use absolute paths for reliability\nnodes:\n  loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"/full/path/to/documents\"  # Not: \"documents/\"\n\n\n\n# Process in smaller batches\nnodes:\n  loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"large_docs/\"\n      batch_size: 100              # Reduce batch size\n      n_proc: 2                    # Reduce parallelism\n  \n  chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 300              # Smaller chunks\n      chunk_overlap: 30            # Less overlap\n\n\n\n# Test connectivity first\nnodes:\n  es_store:\n    type: ElasticsearchStore\n    config:\n      persist_location: \"http://localhost:9200\"\n      # Add authentication if needed\n      basic_auth: [\"username\", \"password\"]\n      # Increase timeouts if needed\n      timeout: 30\n\n\n\n\nControl document size for LLM processing and cost optimization:\nnodes:\n  # Load large documents\n  document_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"large_documents/\"\n  \n  # Keep full documents but truncate to manageable size\n  size_control:\n    type: KeepFullDocument\n    config:\n      concatenate_pages: true    # First combine multi-page documents\n      max_words: 2000           # Then truncate to first 2000 words\n  \n  # Analyze the controlled-size documents\n  quick_analysis:\n    type: PromptProcessor\n    config:\n      prompt: |\n        Analyze this document excerpt (first 2000 words):\n        \n        Source: {source}\n        Content: {content}\n        \n        Provide:\n        1. Document type and purpose\n        2. Main topics covered\n        3. Key findings or conclusions\n        4. Whether this appears to be the beginning, middle, or complete document\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0.1\n        max_tokens: 500\n\nconnections:\n  - from: document_loader\n    from_port: documents\n    to: size_control\n    to_port: documents\n    \n  - from: size_control\n    from_port: documents\n    to: quick_analysis\n    to_port: documents\nBenefits: - Cost Control - Process only document beginnings instead of entire files - Speed - Faster analysis with consistent processing times - Context Management - Ensure documents fit within LLM context windows - Preview Analysis - Get quick insights from document openings - Metadata Tracking - Know original document size and truncation status\n\n\n\nEnable verbose output to see detailed execution:\nfrom onprem.workflow import execute_workflow\n\n# Detailed logging\nresults = execute_workflow(\"workflow.yaml\", verbose=True)\n\n# Check results\nfor node_id, result in results.items():\n    print(f\"Node {node_id}: {result}\")\nValidate before execution:\nfrom onprem.workflow import WorkflowEngine\n\nengine = WorkflowEngine()\ntry:\n    engine.load_workflow_from_yaml(\"workflow.yaml\")\n    print(\"✓ Workflow validation passed\")\nexcept Exception as e:\n    print(f\"✗ Validation failed: {e}\")\n\n\n\nTrack processing times and document counts:\nimport time\nfrom onprem.workflow import execute_workflow\n\nstart_time = time.time()\nresults = execute_workflow(\"workflow.yaml\", verbose=True)\nend_time = time.time()\n\nprint(f\"Processing time: {end_time - start_time:.2f} seconds\")\n\n# Count processed documents\ntotal_docs = 0\nfor node_id, result in results.items():\n    if 'documents' in result:\n        count = len(result['documents'])\n        print(f\"{node_id}: {count} documents\")\n        total_docs += count\n\nprint(f\"Total documents processed: {total_docs}\")\n\nThis tutorial covers all aspects of the OnPrem workflow engine.",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#table-of-contents",
    "href": "workflows.html#table-of-contents",
    "title": "Buildling Workflows",
    "section": "",
    "text": "Quick Start - Three Core Examples\nCommand-Line Usage\nVisual Workflow Builder\nWorkflow Structure\nPort Types and Data Flow\nNode Types Reference\nConfiguration Options\nAdvanced Examples\nValidation and Error Handling\nBest Practices\nTroubleshooting",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#quick-start---three-core-examples",
    "href": "workflows.html#quick-start---three-core-examples",
    "title": "Buildling Workflows",
    "section": "",
    "text": "The workflow engine provides three essential patterns that cover the most common document processing scenarios:\n\n\nPurpose: Download and perform thematic analysis of the State of the Union address using chunking and aggregation.\n# Requires: set OPENAI_API_KEY  \npython -m onprem.workflow yaml_examples/1_direct_analysis.yaml\nWhat it does: - Downloads the State of the Union text automatically from GitHub - Chunks the speech into 1000-character segments with 100-character overlap - Extracts thematic keywords from each chunk (policy areas, topics) - Cleans and standardizes theme responses for consistency - Aggregates all themes into comprehensive political analysis - Exports both detailed themes (CSV) and summary analysis (JSON)\nRequirements: - Set OPENAI_API_KEY environment variable, as example uses GPT-4o-mini. Local LLMs can be used as well. - Internet connection for document download\n\n\n\nPurpose: Load PDF files, chunk them, and store in a vector database for later retrieval.\n# Run from the workflows directory\npython -m onprem.workflow yaml_examples/2_ingest_pdfs.yaml\nWhat it does: - Loads PDF files from ../sample_data/ - Converts PDFs to markdown for better processing - Chunks documents into 800-character pieces with 80-character overlap - Stores in ChromaDB vector database at document_vectors/\nRequirements: PDF files in the sample_data directory\n\n\n\nPurpose: Query an existing vector database, apply AI analysis, and export results.\n# Requires: Run example 1 first + set OPENAI_API_KEY\npython -m onprem.workflow yaml_examples/3_analyze_from_vectorstore.yaml\nWhat it does: - Searches the vector database created in example 1 - Applies AI analysis to find documents about “artificial intelligence machine learning” - Uses GPT-3.5-turbo to analyze each document for topic, key points, and relevance - Exports results to document_analysis_results.xlsx\nRequirements: - Run example 2 first to create document_vectors/ - Set OPENAI_API_KEY environment variable, as GPT-4o-mini is used. Local LLMs can be used as well.",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#command-line-usage",
    "href": "workflows.html#command-line-usage",
    "title": "Buildling Workflows",
    "section": "",
    "text": "python -m onprem.workflow &lt;workflow.yaml&gt;",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#visual-workflow-builder",
    "href": "workflows.html#visual-workflow-builder",
    "title": "Buildling Workflows",
    "section": "",
    "text": "The OnPrem.LLM web UI includes a Visual Workflow Builder that can generate and execute YAML-based workflows through a point-and-click interface with no coding required.\n\nSee the web UI documentation for more information.\n\n\npython -m onprem.workflow &lt;workflow.yaml&gt;\n\n\n\n# Show help and examples\npython -m onprem.workflow --help\n\n# Validate workflow without running\npython -m onprem.workflow --validate workflow.yaml\n\n# List all available node types\npython -m onprem.workflow --list-nodes\n\n# Run quietly (suppress progress output)\npython -m onprem.workflow --quiet workflow.yaml\n\n# Show version\npython -m onprem.workflow --version\n\n\n\n# Run the PDF ingestion workflow\npython -m onprem.workflow yaml_examples/1_ingest_pdfs.yaml\n\n# Validate a workflow before running\npython -m onprem.workflow --validate yaml_examples/2_analyze_from_vectorstore.yaml\n\n# See all available node types\npython -m onprem.workflow --list-nodes",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#workflow-patterns",
    "href": "workflows.html#workflow-patterns",
    "title": "Buildling Workflows",
    "section": "",
    "text": "The three core examples demonstrate the main workflow patterns:\n\n\nDocuments → Chunking → Vector Store\nUse this pattern to build searchable databases from your document collections.\n\n\n\nVector Store → Query → AI Analysis → Export\nUse this pattern to analyze specific topics from large document collections using semantic search.\n\n\n\nDocuments → Full Processing → AI Analysis → Cleanup → Export\nUse this pattern for comprehensive analysis of entire document collections without intermediate storage.",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#available-node-types-summary",
    "href": "workflows.html#available-node-types-summary",
    "title": "Buildling Workflows",
    "section": "",
    "text": "LoadFromFolder - Load all documents from a directory\nLoadSingleDocument - Load a specific file\nLoadWebDocument - Download and load from URL\nLoadSpreadsheet - Load text from spreadsheet rows with metadata\n\n\n\n\n\nSplitByCharacterCount - Chunk by character count\nSplitByParagraph - Chunk by paragraphs (preserves structure)\nKeepFullDocument - Keep documents whole, optionally concatenate pages\n\n\n\n\n\nAddMetadata - Add custom metadata fields to documents\nContentPrefix - Prepend text to document content\nContentSuffix - Append text to document content\nDocumentFilter - Filter documents by metadata or content criteria\nPythonDocumentTransformer - Custom Python transformations\n\n\n\n\n\nChromaStore - Vector database for semantic search\nWhooshStore - Full-text search index\nElasticsearchStore - Hybrid search capabilities\n\n\n\n\n\nQueryDualStore - Search dual vector store (sparse, semantic, and hybrid search)\nQueryChromaStore - Search vector database\nQueryWhooshStore - Search text index\nQueryElasticsearchStore - Search Elasticsearch index\n\n\n\n\n\nPromptProcessor - Apply AI analysis using custom prompts (DocumentProcessor)\nResponseCleaner - Clean and format AI responses (ResultProcessor)\nSummaryProcessor - Generate document summaries (DocumentProcessor)\n\nPythonDocumentProcessor - Execute custom Python code on documents (DocumentProcessor)\nPythonResultProcessor - Execute custom Python code on processing results (ResultProcessor)\nDocumentToResults - Convert documents to export-ready results (bridges Query → Exporter)\n\n\n\n\n\nAggregatorNode - Aggregate multiple results into single result using LLM (AggregatorProcessor)\nPythonAggregatorNode - Aggregate multiple results using custom Python code (AggregatorProcessor)\n\n\n\n\n\nCSVExporter - Export to CSV format\nExcelExporter - Export to Excel format\n\nJSONExporter - Export to JSON format\nJSONResponseExporter - Extract and export JSON from LLM responses",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#workflow-structure",
    "href": "workflows.html#workflow-structure",
    "title": "Buildling Workflows",
    "section": "",
    "text": "Create a file called my_workflow.yaml:\nnodes:\n  document_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"/path/to/your/documents\"\n      verbose: true\n  \n  text_chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 500\n      chunk_overlap: 50\n  \n  search_index:\n    type: WhooshStore\n    config:\n      persist_location: \"my_search_index\"\n\nconnections:\n  - from: document_loader\n    from_port: documents\n    to: text_chunker\n    to_port: documents\n  \n  - from: text_chunker\n    from_port: documents\n    to: search_index\n    to_port: documents\n\n\n\nfrom onprem.workflow import execute_workflow\n\n# Execute the workflow\nresults = execute_workflow(\"my_workflow.yaml\", verbose=True)\nOr programmatically:\nfrom onprem.workflow import WorkflowEngine\n\nengine = WorkflowEngine()\nengine.load_workflow_from_yaml(\"my_workflow.yaml\")\nresults = engine.execute(verbose=True)",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#workflow-structure-1",
    "href": "workflows.html#workflow-structure-1",
    "title": "Buildling Workflows",
    "section": "",
    "text": "A workflow YAML file has two main sections:\n\n\nDefines the processing nodes in your pipeline:\nnodes:\n  node_id:              # Unique identifier for this node\n    type: NodeTypeName  # Type of node (see Node Types Reference)\n    config:             # Configuration specific to this node type\n      parameter1: value1\n      parameter2: value2\n\n\n\nDefines how data flows between nodes:\nconnections:\n  - from: source_node_id    # Source node ID\n    from_port: output_port  # Output port name\n    to: target_node_id      # Target node ID  \n    to_port: input_port     # Input port name",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#port-types-and-data-flow",
    "href": "workflows.html#port-types-and-data-flow",
    "title": "Buildling Workflows",
    "section": "",
    "text": "The workflow system uses a strongly-typed port system to ensure data consistency and prevent invalid connections. Understanding port types is essential for building valid workflows.\n\n\nThere are three main port types in the workflow system:\n\n\nMost common type - Contains LangChain Document objects with content and metadata.\n# Document structure\nDocument(\n    page_content=\"The actual text content of the document...\",\n    metadata={\n        \"source\": \"/path/to/file.pdf\",\n        \"page\": 1,\n        \"author\": \"John Smith\",\n        \"extension\": \"pdf\"\n    }\n)\nUsed by: - All Loader → TextSplitter connections - All TextSplitter → Storage connections\n- All Query → Processor connections\n\n\n\nProcessing output - Contains structured analysis results, summaries, or prompt responses.\n# Results structure\n[\n    {\n        \"document_id\": 0,\n        \"source\": \"research.pdf\",\n        \"prompt\": \"Analyze this document and provide...\",\n        \"response\": \"TOPIC: AI Research | TECH: Neural networks | LEVEL: Advanced\",\n        \"original_length\": 1247,\n        \"metadata\": {\"page\": 1, \"author\": \"Smith\", \"year\": 2023}\n    }\n]\nUsed by: - All Processor → Exporter connections\n\n\n\nAggregated analysis - Contains a single dictionary with consolidated results from multiple inputs.\n# Aggregated result structure\n{\n    \"aggregated_response\": \"Top 3 topics: AI (80%), automation (65%), data science (45%)\",\n    \"source_count\": 12,\n    \"aggregation_method\": \"llm_prompt\",\n    \"topic_analysis\": {\n        \"top_topics\": [\"AI\", \"automation\", \"data science\"],\n        \"coverage_percentage\": 75.5\n    },\n    \"original_results\": [...]  # Reference to source data\n}\nUsed by: - AggregatorNode and PythonAggregatorNode outputs - Can be exported using Exporter nodes (converted to single-row format)\n\n\n\nCompletion status - Simple text messages indicating operation results.\n# Status examples\n\"Successfully stored 150 documents in WhooshStore\"\n\"Exported 25 results to analysis_report.xlsx\"\n\"No documents to store\"\nUsed by: - Storage node outputs (terminal) - Exporter node outputs (terminal)\n\n\n\n\nRaw Files → List[Document] → List[Document] → str\n   ↓            ↓               ↓             ↓\nLoader    TextSplitter      Storage      Status\n\nAlternative analysis path:\nIndex → List[Document] → List[Dict] → str  \n  ↓         ↓              ↓         ↓\nQuery   Processor      Exporter  Status\n\nNew direct export path:\nIndex → List[Document] → List[Dict] → str\n  ↓         ↓              ↓         ↓\nQuery  DocumentToResults Exporter Status\n\nNew aggregation path:\nIndex → List[Document] → List[Dict] → Dict → str\n  ↓         ↓              ↓         ↓      ↓\nQuery   Processor     Aggregator Export Status\n\n\n\nThe workflow engine validates that connected ports have matching types:\n✅ Valid Connections:\n# Document processing chain\n- from: loader\n  from_port: documents        # List[Document]\n  to: chunker\n  to_port: documents         # List[Document] ✓\n\n# Analysis chain  \n- from: query\n  from_port: documents        # List[Document]\n  to: processor\n  to_port: documents         # List[Document] ✓\n\n- from: processor\n  from_port: results          # List[Dict]\n  to: exporter\n  to_port: results           # List[Dict] ✓\n❌ Invalid Connections:\n# Type mismatch\n- from: loader\n  from_port: documents        # List[Document]\n  to: exporter\n  to_port: results           # List[Dict] ❌\n\n# Wrong direction\n- from: storage\n  from_port: status           # str (terminal node)\n  to: processor\n  to_port: documents         # List[Document] ❌\n\n\n\n\ndocuments - Always contains List[Document] objects\nresults - Always contains List[Dict] with analysis results\n\nresult - Always contains Dict with single aggregated result (AggregatorProcessor output)\nstatus - Always contains str with completion messages\n\n\n\n\nData flows preserve metadata throughout the pipeline:\n\nLoader → Document metadata (source, extension, dates, etc.)\nTextSplitter → Preserves original metadata in chunks\nQuery → Returns documents with original metadata\nProcessor → Includes metadata in results under metadata key\nExporter → Flattens metadata into columns (meta_source, meta_page, etc.)\n\n\n\n\nWhen port types don’t match, you’ll see validation errors like:\nWorkflowValidationError: Type mismatch: \nloader.documents (List[Document]) -&gt; exporter.results (List[Dict])\nWorkflowValidationError: Target node processor has no input port 'status'. \nAvailable: ['documents']\nUnderstanding these port types helps you: - Design valid workflows - Debug connection errors - Understand data transformations - Plan processing pipelines",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#node-types-reference",
    "href": "workflows.html#node-types-reference",
    "title": "Buildling Workflows",
    "section": "",
    "text": "Loader nodes read documents from various sources and output List[Document].\n\n\nLoads all documents from a directory using ingest.load_documents.\nnodes:\n  my_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"/path/to/documents\"    # Required: Directory path\n      ignored_files: [\"temp.txt\", \"draft.doc\"] # Optional: Specific files to skip\n      include_patterns: [\"*.pdf\", \"*.docx\"]    # Optional: Only load files matching these patterns\n      exclude_patterns: [\"*draft*\", \"*temp*\"]  # Optional: Skip files matching these patterns\n      verbose: true                             # Optional: Show progress\n      pdf_markdown: false                       # Optional: Convert PDFs to markdown\n      pdf_unstructured: false                   # Optional: Use unstructured PDF parsing\n      n_proc: null                             # Optional: Number of CPU cores (null = all)\n      store_md5: false                         # Optional: Store MD5 hash in metadata\n      store_mimetype: false                    # Optional: Store MIME type in metadata\n      store_file_dates: false                  # Optional: Store file dates in metadata\n      infer_table_structure: false             # Optional: Extract tables from PDFs\n      caption_tables: false                    # Optional: Generate table captions (requires llm)\n      extract_document_titles: false           # Optional: Extract document titles (requires llm)\nFilename Pattern Filtering:\n\ninclude_patterns: Only process files matching these glob patterns (e.g., [\"*.pdf\", \"*.doc*\"])\nexclude_patterns: Skip files matching these glob patterns (e.g., [\"*draft*\", \"*backup*\"])\nIf both specified, file must match include pattern AND not match exclude pattern\nUses standard Unix glob patterns: * (any chars), ? (single char), [abc] (character set)\n\nOutput Ports: - documents: List[Document] - Loaded documents\n\n\n\nLoads a single document using ingest.load_single_document.\nnodes:\n  single_doc:\n    type: LoadSingleDocument\n    config:\n      file_path: \"/path/to/document.pdf\"       # Required: Path to single file\n      pdf_markdown: false                      # Optional: Convert PDF to markdown\n      pdf_unstructured: false                  # Optional: Use unstructured parsing\n      store_md5: false                        # Optional: Store MD5 hash\n      store_mimetype: false                   # Optional: Store MIME type\n      store_file_dates: false                 # Optional: Store file dates\n      infer_table_structure: false            # Optional: Extract tables\nOutput Ports: - documents: List[Document] - Loaded document\n\n\n\nDownloads and loads a document from a URL.\nnodes:\n  web_doc:\n    type: LoadWebDocument\n    config:\n      url: \"https://example.com/document.pdf\"  # Required: Document URL\n      username: \"user\"                         # Optional: Authentication username\n      password: \"pass\"                         # Optional: Authentication password\nOutput Ports: - documents: List[Document] - Downloaded document\n\n\n\nLoads documents from spreadsheet files where each row becomes a document with text content and metadata.\nnodes:\n  spreadsheet_loader:\n    type: LoadSpreadsheet\n    config:\n      file_path: \"/path/to/data.xlsx\"          # Required: Path to spreadsheet (.xlsx, .xls, .csv)\n      text_column: \"description\"               # Required: Column containing text content\n      metadata_columns: [\"id\", \"category\", \"priority\"]  # Optional: Specific columns as metadata\n      sheet_name: \"Sheet1\"                     # Optional: Excel sheet name (default: first sheet)\n  \n  # Use all other columns as metadata (default behavior)\n  auto_metadata:\n    type: LoadSpreadsheet\n    config:\n      file_path: \"survey_responses.csv\"\n      text_column: \"feedback\"                  # Only specify text column\n      # metadata_columns not specified = use all other columns\nUse Cases: - Survey Analysis - Load feedback text with respondent demographics as metadata - Product Reviews - Load review text with ratings, dates, and categories - Customer Support - Load ticket descriptions with priority, department, and status - Content Management - Load article text with tags, authors, and publication dates - Research Data - Load interview transcripts with participant information\nFile Format Support: - CSV files (.csv) - Comma-separated values - Excel files (.xlsx, .xls) - Microsoft Excel format - Automatic detection based on file extension\nMetadata Handling: - Automatic metadata - By default, all columns except text_column become metadata - Custom metadata - Specify metadata_columns to select only certain fields - Standard fields - Always includes source, row_number, and text_column in metadata - Data types - Automatically converts pandas/numpy types to JSON-serializable Python types - Null handling - Empty cells become None in metadata\nError Handling: - Validates text column exists - Validates metadata columns exist (if specified) - Skips rows with empty text content - Provides helpful error messages for missing files or columns\nOutput Ports: - documents: List[Document] - Documents created from spreadsheet rows\n\n\n\n\nTextSplitter nodes process documents and output chunked List[Document].\n\n\nChunks documents by character count using ingest.chunk_documents.\nnodes:\n  char_splitter:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 500                         # Optional: Characters per chunk (default: 500)\n      chunk_overlap: 50                       # Optional: Overlap between chunks (default: 50)\n      infer_table_structure: false            # Optional: Handle tables specially\n      preserve_paragraphs: false              # Optional: Keep paragraphs intact\nInput Ports: - documents: List[Document] - Documents to chunk\nOutput Ports: - documents: List[Document] - Chunked documents\n\n\n\nChunks documents by paragraph boundaries, preserving document structure.\nnodes:\n  para_splitter:\n    type: SplitByParagraph\n    config:\n      chunk_size: 1000                        # Optional: Max characters per chunk\n      chunk_overlap: 100                      # Optional: Overlap between chunks\n      # preserve_paragraphs is automatically set to True\nInput Ports: - documents: List[Document] - Documents to chunk\nOutput Ports: - documents: List[Document] - Paragraph-based chunks\n\n\n\nPasses documents through without any chunking. Optionally concatenates multi-page documents and/or truncates documents to a maximum word count.\nnodes:\n  no_split:\n    type: KeepFullDocument\n    config: {}  # No configuration needed - keeps documents as-is\n  \n  # For multi-page documents (PDFs, etc.) - combine into single document\n  full_document:\n    type: KeepFullDocument\n    config:\n      concatenate_pages: true    # Optional: Combine pages into single document\n  \n  # Truncate documents to first N words (useful for LLM context limits)\n  truncated_document:\n    type: KeepFullDocument\n    config:\n      max_words: 500            # Optional: Truncate to first 500 words\n  \n  # Both concatenation and truncation (applied in that order)\n  combined_processing:\n    type: KeepFullDocument\n    config:\n      concatenate_pages: true   # First: combine multi-page documents\n      max_words: 1000          # Then: truncate to first 1000 words\nPage Concatenation:\nWhen concatenate_pages: true, multi-page documents are combined: - Pages sorted by page number - Content joined with --- PAGE BREAK --- separators - Metadata preserved from first page plus additional fields: - page: -1 (indicates full document) - page_count: N (number of pages combined) - page_range: \"1-5\" (original page range) - concatenated: true (flag indicating concatenation)\nDocument Truncation:\nWhen max_words: N is specified, documents are truncated to the first N words: - Word boundaries are preserved (no partial words) - Metadata is enriched with truncation information: - original_word_count: 2500 (original document length) - truncated: true (indicates truncation occurred) - truncated_word_count: 500 (target truncation size) - Documents shorter than max_words are passed through unchanged - Processing order: concatenation first, then truncation\nUse Cases: - Page Concatenation: - Resume Processing - Combine multi-page resumes into single document - Contract Analysis - Process entire contracts as one unit - Report Analysis - Analyze complete reports without page boundaries - Legal Documents - Preserve document structure while enabling full-text analysis\n\nDocument Truncation:\n\nLLM Context Management - Fit long documents within token limits\nCost Control - Reduce processing costs for very long documents\nPreview Generation - Create document summaries from beginnings\nPerformance Optimization - Speed up processing of large documents\nClassification Tasks - Use document openings for categorization\n\n\nInput Ports: - documents: List[Document] - Documents to pass through or concatenate\nOutput Ports: - documents: List[Document] - Unchanged or concatenated documents\n\n\n\n\nDocumentTransformer nodes transform documents while preserving the List[Document] → List[Document] flow. They can add metadata, modify content, filter documents, or apply custom transformations. These nodes can be placed anywhere in the document pipeline.\n\n\nAdds static metadata fields to all documents for categorization and organization.\nnodes:\n  categorize_meeting:\n    type: AddMetadata\n    config:\n      metadata:\n        category: \"meeting20251001\"\n        department: \"engineering\"\n        priority: \"high\"\n        project: \"Project Alpha\"\n        classification: \"internal\"\nUse Cases: - Meeting Organization - Tag all documents from a specific meeting - Project Tracking - Add project identifiers to document collections - Department Categorization - Organize documents by department or team - Classification - Mark documents as confidential, internal, or public - Batch Processing - Add consistent metadata to large document collections\nInput Ports: - documents: List[Document] - Documents to enrich\nOutput Ports: - documents: List[Document] - Documents with added metadata\n\n\n\nPrepends text to the page_content of all documents.\nnodes:\n  mark_confidential:\n    type: ContentPrefix\n    config:\n      prefix: \"[CONFIDENTIAL - INTERNAL USE ONLY]\"\n      separator: \"\\n\\n\"  # Optional: separator between prefix and content (default: \"\\n\\n\")\n  \n  add_header:\n    type: ContentPrefix\n    config:\n      prefix: \"Project Alpha Documentation\"\n      separator: \"\\n---\\n\"\nUse Cases: - Confidentiality Markings - Add confidential headers to sensitive documents - Document Headers - Add consistent headers to document collections - Processing Stamps - Mark documents as processed by specific workflows - Context Addition - Add contextual information to document beginnings\nInput Ports: - documents: List[Document] - Documents to modify\nOutput Ports: - documents: List[Document] - Documents with prefixed content\n\n\n\nAppends text to the page_content of all documents.\nnodes:\n  add_footer:\n    type: ContentSuffix\n    config:\n      suffix: |\n        ---\n        Document processed by OnPrem Workflow Engine\n        Processing date: 2025-01-16\n        For questions, contact: admin@company.com\n      separator: \"\\n\"  # Optional: separator between content and suffix (default: \"\\n\\n\")\nUse Cases: - Processing Information - Add processing timestamps and contact info - Legal Disclaimers - Append legal text to documents - Document Footers - Add consistent footers to document collections - Attribution - Add source or processing attribution\nInput Ports: - documents: List[Document] - Documents to modify\nOutput Ports: - documents: List[Document] - Documents with appended content\n\n\n\nFilters documents based on metadata criteria, content patterns, or length requirements.\nnodes:\n  filter_engineering:\n    type: DocumentFilter\n    config:\n      # Filter by metadata\n      metadata_filters:\n        department: \"engineering\"\n        status: \"active\"\n      # Filter by content\n      content_contains: [\"project\", \"analysis\", \"results\"]\n      content_excludes: [\"draft\", \"template\"]\n      # Filter by length\n      min_length: 100\n      max_length: 10000\n  \n  # Simple content filtering\n  relevant_docs_only:\n    type: DocumentFilter\n    config:\n      content_contains: [\"machine learning\", \"AI\", \"neural network\"]\n      min_length: 50\nFilter Options: - metadata_filters: Dictionary of metadata key-value pairs that must match exactly - content_contains: List of terms - document must contain at least one - content_excludes: List of terms - document must not contain any - min_length: Minimum content length in characters - max_length: Maximum content length in characters\nUse Cases: - Relevance Filtering - Keep only documents containing specific keywords - Quality Control - Remove documents that are too short or too long - Content Curation - Filter out drafts, templates, or irrelevant content - Metadata-based Selection - Keep only documents matching specific criteria\nInput Ports: - documents: List[Document] - Documents to filter\nOutput Ports: - documents: List[Document] - Filtered documents\n\n\n\nExecutes custom Python code to transform documents with full flexibility and security controls.\nnodes:\n  extract_document_info:\n    type: PythonDocumentTransformer\n    config:\n      code: |\n        # Available variables:\n        # - doc: Document object\n        # - content: doc.page_content (string)\n        # - metadata: doc.metadata (mutable copy)\n        # - document_id: index of document (int)\n        # - source: source file path (string)\n        \n        # Extract information from content\n        import re\n        \n        word_count = len(content.split())\n        sentence_count = len(re.findall(r'[.!?]+', content))\n        \n        # Find email addresses\n        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n        emails = re.findall(email_pattern, content)\n        \n        # Determine document type\n        if 'meeting' in content.lower() and 'agenda' in content.lower():\n            doc_type = 'meeting_agenda'\n        elif 'analysis' in content.lower():\n            doc_type = 'analysis_report'\n        else:\n            doc_type = 'general_document'\n        \n        # Enrich metadata\n        metadata.update({\n            'word_count': word_count,\n            'sentence_count': sentence_count,\n            'email_count': len(emails),\n            'document_type': doc_type,\n            'complexity_score': min(10, word_count // 100)\n        })\n        \n        # Add summary to content\n        summary = f\"[{doc_type.upper()}: {word_count} words, Complexity: {metadata['complexity_score']}/10]\"\n        content = summary + \"\\n\\n\" + content\n        \n        # Create transformed document\n        transformed_doc = Document(\n            page_content=content,\n            metadata=metadata\n        )\n\n  # Load transformation from external file\n  complex_transform:\n    type: PythonDocumentTransformer\n    config:\n      code_file: \"scripts/document_enricher.py\"\nAvailable Python Environment: - Built-in functions: len, str, int, float, bool, list, dict, min, max, etc. - Safe modules: re, json, math, datetime (pre-imported) - Document class: Available for creating new Document objects - Security: No file I/O, network access, or system operations\nVariable Reference: - doc: Original Document object (read-only) - content: Document content (modifiable string) - metadata: Document metadata (modifiable dictionary copy) - document_id: Index of current document (int) - source: Source file path (string) - transformed_doc: Set this to a Document object for the output (optional)\nTransformation Options: 1. Modify Variables: Change content and metadata, let the system create the Document 2. Explicit Creation: Create and set transformed_doc explicitly\nUse Cases: - Content Analysis - Extract key information and add to metadata - Document Classification - Automatically categorize documents by content - Data Extraction - Find emails, URLs, phone numbers, etc. - Content Transformation - Modify content based on complex rules - Custom Enrichment - Add calculated metrics or derived information\nInput Ports: - documents: List[Document] - Documents to transform\nOutput Ports: - documents: List[Document] - Transformed documents\n\n\n\n\nStorage nodes save documents to various backends and return status messages.\n\n\nStores documents in a ChromaDB vector database.\nnodes:\n  vector_db:\n    type: ChromaStore\n    config:\n      persist_location: \"/path/to/chromadb\"    # Optional: Database path\n      # Additional ChromaDB configuration options\nInput Ports: - documents: List[Document] - Documents to store\nOutput Ports: - status: str - Storage status message\n\n\n\nStores documents in a Whoosh full-text search index.\nnodes:\n  search_index:\n    type: WhooshStore\n    config:\n      persist_location: \"/path/to/whoosh_index\" # Optional: Index path\n      # Additional Whoosh configuration options\nInput Ports: - documents: List[Document] - Documents to index\nOutput Ports: - status: str - Indexing status message\n\n\n\nStores documents in an Elasticsearch cluster.\nnodes:\n  es_store:\n    type: ElasticsearchStore\n    config:\n      persist_location: \"http://localhost:9200\" # Required: Elasticsearch URL\n      index_name: \"my_documents\"                # Optional: Index name\n      # Additional Elasticsearch configuration options\nInput Ports: - documents: List[Document] - Documents to store\nOutput Ports: - status: str - Storage status message\n\n\n\n\nQuery nodes search existing storage indexes and return matching documents.\n\n\nSearches documents in a Whoosh full-text search index with support for different search types.\nnodes:\n  # Sparse search (pure keyword matching)\n  keyword_search:\n    type: QueryWhooshStore\n    config:\n      persist_location: \"/path/to/whoosh_index\" # Required: Index path\n      query: \"artificial intelligence ML\"        # Required: Search terms\n      search_type: \"sparse\"                     # Optional: \"sparse\" or \"semantic\" (default: sparse)\n      limit: 20                                 # Optional: Max results (default: 100)\n      \n  # Semantic search (keyword + embedding re-ranking)  \n  smart_search:\n    type: QueryWhooshStore\n    config:\n      persist_location: \"/path/to/whoosh_index\"\n      query: \"machine learning concepts\"\n      search_type: \"semantic\"                   # Uses keyword search + semantic re-ranking\n      limit: 10\nSearch Types: - sparse: Pure keyword/full-text search using Whoosh - semantic: Keyword search followed by embedding-based re-ranking\nInput Ports: - None (queries existing storage directly)\nOutput Ports: - documents: List[Document] - Matching documents\n\n\n\nSearches documents in a dual vector store that combines both sparse (keyword) and dense (semantic) search capabilities with hybrid search option.\nnodes:\n  hybrid_search:\n    type: QueryDualStore\n    config:\n      persist_location: \"/path/to/dualstore\"      # Required: Database path\n      query: \"artificial intelligence trends\"     # Required: Search query\n      search_type: \"hybrid\"                       # Optional: sparse/semantic/hybrid (default: hybrid)\n      limit: 15                                  # Optional: Max results (default: 10)\n      weights: [0.6, 0.4]                       # Optional: [dense, sparse] weights for hybrid search\nSearch Types: - sparse: Keyword/full-text search using sparse vectors - semantic: Vector similarity search using dense embeddings - hybrid: Combines sparse and semantic search with configurable weights\nHybrid Search Configuration: - weights: Array of [dense_weight, sparse_weight] (default: [0.6, 0.4]) - Dense weight controls semantic search influence - Sparse weight controls keyword search influence - Weights should sum to 1.0 for optimal results\nInput Ports: - None (queries existing storage directly)\nOutput Ports: - documents: List[Document] - Matching documents (ranked by combined score for hybrid)\n\n\n\nSearches documents in a ChromaDB vector database using semantic similarity.\nnodes:\n  vector_search:\n    type: QueryChromaStore\n    config:\n      persist_location: \"/path/to/chromadb\"    # Required: Database path\n      query: \"machine learning algorithms\"     # Required: Search query\n      search_type: \"semantic\"                  # Optional: Only \"semantic\" supported (default)\n      limit: 10                               # Optional: Max results (default: 10)\nSearch Types: - semantic: Vector similarity search (only supported type)\nInput Ports: - None (queries existing storage directly)\nOutput Ports: - documents: List[Document] - Similar documents\n\n\n\nSearches documents in an Elasticsearch index with full support for all search types.\nnodes:\n  # Sparse search (BM25 text matching)\n  text_search:\n    type: QueryElasticsearchStore\n    config:\n      persist_location: \"http://localhost:9200\" # Required: Elasticsearch URL\n      index_name: \"my_index\"                    # Required: Index name\n      query: \"artificial intelligence\"          # Required: Search query\n      search_type: \"sparse\"                     # Optional: \"sparse\", \"semantic\", or \"hybrid\"\n      limit: 5                                  # Optional: Max results (default: 10)\n      \n  # Semantic search (vector similarity)\n  vector_search:\n    type: QueryElasticsearchStore\n    config:\n      persist_location: \"https://my-es:9200\"\n      index_name: \"documents\"\n      query: \"machine learning concepts\" \n      search_type: \"semantic\"                   # Dense vector search\n      limit: 3\n      basic_auth: [\"user\", \"password\"]          # Optional: Authentication\n      verify_certs: false                       # Optional: SSL verification\n      \n  # Hybrid search (combines text + vector)\n  best_search:\n    type: QueryElasticsearchStore\n    config:\n      persist_location: \"http://localhost:9200\"\n      index_name: \"knowledge_base\"\n      query: \"deep learning neural networks\"\n      search_type: \"hybrid\"                     # Best of both worlds\n      weights: [0.7, 0.3]                      # Optional: [text_weight, vector_weight]\n      limit: 5\nSearch Types: - sparse: Traditional BM25 text search - semantic: Dense vector similarity search\n- hybrid: Weighted combination of sparse + semantic results\nInput Ports: - None (queries existing storage directly)\nOutput Ports: - documents: List[Document] - Matching documents\n\n\n\n\nProcessor nodes apply AI analysis, prompts, or transformations to documents.\n\n\nApplies a custom prompt to each document using an LLM.\nnodes:\n  document_analyzer:\n    type: PromptProcessor\n    config:\n      prompt: |                               # Option 1: Inline prompt template\n        Analyze this document and provide:\n        1. Main topic: \n        2. Key findings:\n        3. Complexity (1-5):\n        \n        Source: {source}\n        Content: {content}\n      llm:                                    # New flexible LLM configuration\n        model_url: \"openai://gpt-3.5-turbo\"  # Model URL specification\n        temperature: 0.7                     # Creativity level\n        max_tokens: 1000                     # Response length limit\n      batch_size: 5                          # Optional: Process in batches\n\n  # Alternative: Load complex prompt from file with advanced LLM config\n  complex_analyzer:\n    type: PromptProcessor\n    config:\n      prompt_file: \"prompts/statute_extraction.txt\"  # Option 2: Load from file\n      llm:                                   # Advanced LLM configuration\n        model_url: \"openai://gpt-4o-mini\"    # Full model URL specification\n        temperature: 0                       # Deterministic results\n        mute_stream: true                    # Quiet processing\n        timeout: 60                          # Request timeout\n      batch_size: 2                          # Optional: Process in batches\nLoading Prompts from Files:\nFor complex prompts, you can store them in separate text files and reference them with prompt_file:\n# File: prompts/resume_parser.txt\nAnalyze the resume and extract details in JSON format:\n{\n  \"name\": \"...\",\n  \"skills\": [\"...\", \"...\"],\n  \"experience\": [...]\n}\n\nResume text: {content}\n\n# Workflow configuration\nconfig:\n  prompt_file: \"prompts/resume_parser.txt\"\nBenefits of External Prompt Files: - Better organization for complex prompts - Version control and collaboration - Reusability across workflows - Easier prompt engineering and testing\nLLM Configuration Options:\nThe llm section accepts all parameters supported by the OnPrem LLM class.\nLLM Instance Sharing:\nThe workflow engine automatically shares LLM instances between processors that use identical configurations, improving performance and memory usage:\nnodes:\n  extractor:\n    type: PromptProcessor\n    config:\n      prompt_file: \"prompts/extraction.txt\"\n      llm:\n        model_url: \"openai://gpt-4o-mini\"  # LLM instance created\n        temperature: 0\n        \n  cleaner:\n    type: ResponseCleaner  \n    config:\n      cleanup_prompt_file: \"prompts/cleanup.txt\"\n      llm:\n        model_url: \"openai://gpt-4o-mini\"  # Same instance reused!\n        temperature: 0\nConfiguration Options:\nllm:\n  # Model specification\n  model_url: \"openai://gpt-4o-mini\"      # Model URL (recommended format)\n  \n  # Generation parameters\n  temperature: 0.7                       # Randomness (0.0-2.0)\n  max_tokens: 1500                       # Maximum response length\n  top_p: 0.9                            # Nucleus sampling\n  frequency_penalty: 0.0                 # Repetition penalty\n  presence_penalty: 0.0                  # Topic diversity penalty\n  \n  # Behavior options\n  mute_stream: true                      # Suppress streaming output\n  timeout: 120                          # Request timeout in seconds\n  \n  # Provider-specific options (passed through)\n  api_key: \"${OPENAI_API_KEY}\"          # API authentication\n  base_url: \"https://api.openai.com/v1\"  # Custom API endpoint\nPrompt Variables: - {content} - Document content - {source} - Document source path - {page} - Page number (if available) - Any metadata field (e.g., {meta_author})\nInput Ports: - documents: List[Document] - Documents to process\nOutput Ports: - results: List[Dict] - Analysis results with prompt responses\n\n\n\nPost-processes and cleans LLM responses using another LLM call.\nnodes:\n  # Default usage - cleans \"response\" field from PromptProcessor results\n  response_cleaner:\n    type: ResponseCleaner\n    config:\n      source_field: \"response\"           # Optional: field to clean (default: \"response\")\n      cleanup_prompt: |                  # Inline cleanup instructions\n        Remove XML tags and clean up formatting while preserving all valid content:\n        {response}\n        \n        Keep all important information, just remove formatting artifacts.\n      llm:\n        model_url: \"openai://gpt-3.5-turbo\"\n        temperature: 0               # Deterministic cleanup\n\n  # Clean different field - useful after DocumentToResults or PythonDocumentProcessor\n  content_cleaner:\n    type: ResponseCleaner\n    config:\n      source_field: \"page_content\"       # Clean the page_content field instead\n      cleanup_prompt: |\n        Clean and standardize this document content:\n        {response}\n        \n        Remove any formatting artifacts while preserving meaning.\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0\n\n  # Alternative: Load cleanup prompt from file\n  citation_cleaner:\n    type: ResponseCleaner\n    config:\n      cleanup_prompt_file: \"prompts/statute_cleanup.txt\"  # Complex cleanup rules\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0\n        mute_stream: true\n      # source_field defaults to \"response\" if not specified\nUse Cases: - Clean LLM responses from PromptProcessor (default “response” field) - Clean document content from DocumentToResults (“page_content” field) - Clean custom outputs from PythonDocumentProcessor (any field name) - Remove formatting artifacts like XML tags or unwanted text\n- Clean statutory citations while preserving all valid references (U.S.C., Public Laws, etc.) - Standardize outputs for consistent data processing - Chain with any processor that produces text fields needing cleanup\nImportant Notes: - Cleanup prompts should be carefully designed to avoid over-aggressive cleaning - Always test with representative examples to ensure valid data isn’t removed - Consider the specific domain and format of your LLM responses\nInput Ports: - results: List[Dict] - Results from PromptProcessor to clean\nOutput Ports: - results: List[Dict] - Cleaned results (original kept in original_response field)\n\n\n\nGenerates summaries for documents using an LLM.\nnodes:\n  summarizer:\n    type: SummaryProcessor\n    config:\n      max_length: 150                        # Optional: Max summary length in words\n      model_name: \"gpt-3.5-turbo\"          # Optional: LLM model\n      llm_type: \"openai\"                   # Optional: LLM provider\nInput Ports: - documents: List[Document] - Documents to summarize\nOutput Ports: - results: List[Dict] - Summaries with metadata\n\n\n\nConverts List[Document] to List[Dict] format suitable for Exporter nodes. This node bridges Query nodes (which output documents) to Exporter nodes (which expect processing results), enabling direct export from search results.\nnodes:\n  # Basic document-to-results conversion\n  prepare_export:\n    type: DocumentToResults\n    config:\n      include_content: true                      # Optional: Include document content (default: true)\n      include_metadata: true                     # Optional: Include document metadata (default: true)\n      content_field: \"content\"                   # Optional: Name for content field (default: \"content\")\n      metadata_prefix: \"meta_\"                   # Optional: Prefix for metadata fields (default: \"meta_\")\n      flatten_metadata: true                     # Optional: Flatten metadata with prefix (default: true)\n\n  # Custom conversion configuration\n  custom_export_prep:\n    type: DocumentToResults\n    config:\n      include_content: false                     # Exclude content, metadata only\n      content_field: \"document_text\"             # Custom content field name\n      metadata_prefix: \"doc_\"                    # Custom metadata prefix\n      custom_fields:                             # Optional: Add static fields\n        export_date: \"2025-01-16\"\n        processed_by: \"workflow_v2\"\n        department: \"research\"\nUse Cases: - Direct Query Export - Query → Convert → Export in 3 nodes - Search Result Analysis - Export search results with metadata for analysis - Document Cataloging - Create spreadsheets of document collections - Content Audit - Export document inventories with metadata - Research Data Collection - Prepare search results for external analysis tools\nConfiguration Options: - include_content: Whether to include document text content - include_metadata: Whether to include document metadata - content_field: Field name for document content (default: “content”) - metadata_prefix: Prefix for metadata fields (default: “meta_”) - flatten_metadata: Flatten metadata with prefix vs nested object - custom_fields: Dictionary of static fields to add to all results\nOutput Format: Each document becomes a dictionary with: - document_id: Sequential ID (0, 1, 2, …) - source: Document source path - content: Document content (if included) - content_length: Length of document content - meta_*: Flattened metadata fields with prefix - Custom fields from configuration\nExample Workflow - Direct Query Export:\nnodes:\n  search_docs:\n    type: QueryDualStore\n    config:\n      persist_location: \"vectordb\"\n      query: \"artificial intelligence trends\"\n      search_type: \"hybrid\"\n      limit: 20\n\n  prepare_export:\n    type: DocumentToResults\n    config:\n      content_field: \"document_text\"\n      metadata_prefix: \"\"  # No prefix for cleaner column names\n      custom_fields:\n        search_query: \"artificial intelligence trends\"\n        export_date: \"2025-01-16\"\n\n  export_results:\n    type: ExcelExporter\n    config:\n      output_path: \"ai_research_papers.xlsx\"\n      sheet_name: \"Search_Results\"\n\nconnections:\n  - from: search_docs\n    from_port: documents\n    to: prepare_export\n    to_port: documents\n    \n  - from: prepare_export\n    from_port: results\n    to: export_results\n    to_port: results\nInput Ports: - documents: List[Document] - Documents to convert\nOutput Ports: - results: List[Dict] - Export-ready results with content and metadata\n\n\n\nExecutes custom Python code on documents with proper security controls, allowing unlimited customization of document processing logic.\nnodes:\n  # Inline Python code\n  custom_analyzer:\n    type: PythonDocumentProcessor\n    config:\n      code: |\n        # Available variables:\n        # - doc: Document object\n        # - content: doc.page_content (string)\n        # - metadata: doc.metadata (dict) \n        # - document_id: index of document (int)\n        # - source: source file path (string)\n        # - result: dictionary to populate (dict)\n        \n        # Extract key information (re module is pre-imported)\n        word_count = len(content.split())\n        sentence_count = len(re.findall(r'[.!?]+', content))\n        \n        # Find email addresses\n        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n        emails = re.findall(email_pattern, content)\n        \n        # Populate result dictionary\n        result['analysis'] = {\n            'word_count': word_count,\n            'sentence_count': sentence_count,\n            'emails_found': emails,\n            'document_length': len(content)\n        }\n        result['processing_status'] = 'completed'\n\n  # Load Python code from external file\n  external_processor:\n    type: PythonDocumentProcessor\n    config:\n      code_file: \"scripts/document_analyzer.py\"\nAvailable Python Environment: - Built-in functions: len, str, int, float, bool, list, dict, min, max, etc. - Safe modules: re, json, math, datetime (pre-imported) - Document class: Available for creating new Document objects - Security: No file I/O, network access, or system operations\nInput Ports: - documents: List[Document] - Documents to process\nOutput Ports: - results: List[Dict] - Processing results with custom analysis\n\n\n\nExecutes custom Python code on processing results, enabling post-processing and enhancement of analysis results.\nnodes:\n  result_enhancer:\n    type: PythonResultProcessor\n    config:\n      code: |\n        # Available variables:\n        # - result: original result dictionary (modifiable copy)\n        # - original_result: read-only original result\n        # - result_id: index of result (int) \n        # - processed_result: dictionary to populate (dict)\n        \n        # Enhance analysis results\n        analysis = result.get('analysis', {})\n        word_count = analysis.get('word_count', 0)\n        \n        # Categorize document by length\n        if word_count &lt; 100:\n            category = 'short'\n        elif word_count &lt; 500:\n            category = 'medium'\n        else:\n            category = 'long'\n        \n        # Create enhanced result\n        processed_result['enhanced_analysis'] = {\n            'original_analysis': analysis,\n            'document_category': category,\n            'complexity_score': min(10, word_count // 50),\n            'has_emails': len(analysis.get('emails_found', [])) &gt; 0\n        }\n        \n        # Add summary\n        processed_result['summary'] = f\"Document categorized as '{category}'\"\nVariable Naming Conventions: - Document Processor: Populate the result dictionary with your analysis - Result Processor: Populate the processed_result dictionary with enhanced data\nInput Ports: - results: List[Dict] - Results to process\nOutput Ports: - results: List[Dict] - Enhanced processing results\n\n\n\nAggregates multiple processing results into a single collapsed result using LLM-based analysis. Perfect for creating summaries of summaries, extracting top themes from multiple responses, or producing executive summaries from document collections.\nnodes:\n  # Topic aggregation - analyze topic keywords across documents\n  topic_aggregator:\n    type: AggregatorNode\n    config:\n      prompt: |\n        Analyze these {num_results} topic keywords from different documents and identify the top 5 most important topics:\n        \n        {responses}\n        \n        Please provide:\n        1. TOP TOPICS: List the 5 most important topics with frequency\n        2. TRENDS: What patterns do you see across documents?\n        3. SUMMARY: One sentence summary of the overall theme\n        \n        Format your response clearly with headings.\n      llm:\n        model_url: \"openai://gpt-3.5-turbo\"\n        temperature: 0.3\n        max_tokens: 500\n\n  # Summary aggregation - create summary of summaries\n  meta_summarizer:\n    type: AggregatorNode\n    config:\n      prompt_file: \"prompts/summary_aggregation.txt\"  # Load from file\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0.1\nUse Cases: - Topic Analysis - Individual documents → topic keywords → top themes across collection - Executive Summaries - Document summaries → executive summary of key findings - Survey Analysis - Individual responses → overall trends and insights - Meeting Analysis - Multiple meeting notes → key decisions and action items - Research Synthesis - Paper summaries → research landscape overview\nInput Ports: - results: List[Dict] - Multiple results to aggregate (from PromptProcessor, SummaryProcessor, etc.)\nOutput Ports:\n- result: Dict - Single aggregated result containing: - aggregated_response: LLM’s aggregated analysis - source_count: Number of original results processed - aggregation_method: “llm_prompt” - original_results: Reference to source data\n\n\n\nAggregates multiple processing results using custom Python code for precise control over aggregation logic. Ideal for statistical analysis, data consolidation, and algorithmic aggregation.\nnodes:\n  # Topic frequency analysis\n  topic_frequency_analyzer:\n    type: PythonAggregatorNode\n    config:\n      code: |\n        # Available variables:\n        # - results: list of all result dictionaries\n        # - num_results: number of results\n        # - result: dictionary to populate with aggregated result\n        \n        # Count topic frequency across all responses\n        topic_counts = {}\n        total_responses = 0\n        \n        for res in results:\n            response = res.get('response', '')\n            if response:\n                topics = [t.strip() for t in response.split(',') if t.strip()]\n                total_responses += 1\n                for topic in topics:\n                    topic_counts[topic] = topic_counts.get(topic, 0) + 1\n        \n        # Get top topics by frequency\n        sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n        top_topics = sorted_topics[:5]\n        \n        # Calculate statistics\n        result['topic_analysis'] = {\n            'top_topics': [\n                {'topic': topic, 'frequency': count, 'percentage': round(count/total_responses*100, 1)}\n                for topic, count in top_topics\n            ],\n            'total_unique_topics': len(topic_counts),\n            'total_documents': total_responses,\n            'coverage': f\"{len([t for t in topic_counts.values() if t &gt; 1])} topics appear in multiple documents\"\n        }\n        \n        result['aggregation_summary'] = f\"Analyzed {total_responses} documents, found {len(topic_counts)} unique topics\"\n\n  # Statistical summary aggregator  \n  statistical_aggregator:\n    type: PythonAggregatorNode\n    config:\n      code: |\n        # Aggregate numerical metrics from document analysis\n        import json\n        import math\n        \n        word_counts = []\n        sentiment_scores = []\n        complexity_scores = []\n        \n        for res in results:\n            # Extract metrics from results\n            metadata = res.get('metadata', {})\n            analysis = res.get('analysis', {})\n            \n            if 'word_count' in metadata:\n                word_counts.append(metadata['word_count'])\n            \n            if 'sentiment_score' in analysis:\n                sentiment_scores.append(analysis['sentiment_score'])\n                \n            if 'complexity_score' in analysis:\n                complexity_scores.append(analysis['complexity_score'])\n        \n        # Calculate statistics\n        def calc_stats(values):\n            if not values:\n                return {}\n            return {\n                'count': len(values),\n                'mean': sum(values) / len(values),\n                'median': sorted(values)[len(values)//2],\n                'min': min(values),\n                'max': max(values),\n                'std_dev': math.sqrt(sum((x - sum(values)/len(values))**2 for x in values) / len(values))\n            }\n        \n        result['statistical_summary'] = {\n            'document_metrics': {\n                'total_documents': len(results),\n                'word_count_stats': calc_stats(word_counts),\n                'sentiment_stats': calc_stats(sentiment_scores),\n                'complexity_stats': calc_stats(complexity_scores)\n            },\n            'data_quality': {\n                'documents_with_word_count': len(word_counts),\n                'documents_with_sentiment': len(sentiment_scores),\n                'documents_with_complexity': len(complexity_scores)\n            }\n        }\n        \n        # Create readable summary\n        avg_words = result['statistical_summary']['document_metrics']['word_count_stats'].get('mean', 0)\n        result['executive_summary'] = f\"Processed {len(results)} documents, average length: {avg_words:.0f} words\"\nUse Cases: - Topic Frequency Analysis - Count and rank topics across document collections - Statistical Aggregation - Calculate means, medians, distributions from analysis results - Data Consolidation - Merge and deduplicate information from multiple sources - Custom Scoring - Apply business logic to rank or categorize results - Format Conversion - Transform aggregated data into specific output formats\nAvailable Python Environment: - Built-in functions: len, str, int, float, bool, list, dict, min, max, sum, etc. - Safe modules: re, json, math, datetime (pre-imported) - Security: No file I/O, network access, or system operations\nInput Ports: - results: List[Dict] - Multiple results to aggregate\nOutput Ports: - result: Dict - Single aggregated result containing: - Custom fields based on your aggregation logic - source_count: Number of original results processed - aggregation_method: “python_code”\n\n\n\n\nExporter nodes save processed results to various file formats.\n\n\nExports results to CSV format for spreadsheet analysis.\nnodes:\n  csv_output:\n    type: CSVExporter\n    config:\n      output_path: \"results.csv\"            # Optional: Output file (default: results.csv)\n      columns: [\"source\", \"response\"]       # Optional: Columns to include (default: all)\nInput Ports: - results: List[Dict] - Results to export\nOutput Ports: - status: str - Export status message\n\n\n\nExports results to Excel format with formatting support.\nnodes:\n  excel_output:\n    type: ExcelExporter\n    config:\n      output_path: \"analysis.xlsx\"          # Optional: Output file (default: results.xlsx)\n      sheet_name: \"Document_Analysis\"       # Optional: Sheet name (default: Results)\nInput Ports: - results: List[Dict] - Results to export\nOutput Ports: - status: str - Export status message\n\n\n\nExports results to JSON format for programmatic access. Can handle both regular processing results and single aggregated results.\nnodes:\n  json_output:\n    type: JSONExporter\n    config:\n      output_path: \"results.json\"           # Optional: Output file (default: results.json)\n      pretty_print: true                    # Optional: Format JSON nicely (default: true)\nInput Ports: - results: List[Dict] - Multiple results to export (from processors) - result: Dict - Single aggregated result to export (from aggregators)\nOutput Ports: - status: str - Export status message\n\n\n\nExtracts JSON content from LLM responses and exports as clean JSON array. Ideal for structured data extraction workflows where LLM responses contain JSON objects that need to be exported without metadata.\nnodes:\n  extract_resume_data:\n    type: JSONResponseExporter\n    config:\n      output_path: \"extracted_resumes.json\"     # Optional: Output file (default: extracted_responses.json)\n      pretty_print: true                        # Optional: Format JSON nicely (default: true)\n      response_field: \"response\"                # Optional: Field containing JSON response (default: \"response\")\nKey Features: - JSON Extraction: Uses llm.helpers.extract_json to parse JSON from text responses - Field Flexibility: Searches multiple response fields (“response”, “aggregated_response”, “output”) - Error Handling: Includes raw response and error info when JSON extraction fails - Clean Output: Exports only the extracted JSON data, removing all metadata\nInput Ports: - results: List[Dict] - Multiple results with JSON responses to extract (from processors)\n- result: Dict - Single aggregated result with JSON response to extract (from aggregators)\nOutput Ports: - status: str - Export status message\nExample Use Case - Resume Parsing:\n# Extract structured data from resume analysis\nextract_structured_resumes:\n  type: JSONResponseExporter\n  config:\n    output_path: \"candidate_profiles.json\"\n    response_field: \"analysis\"\nWhere LLM responses like:\n{\"name\": \"John Doe\", \"skills\": [\"Python\", \"Machine Learning\"], \"experience\": \"5 years\"}\nAre exported as a clean JSON array without workflow metadata.",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#configuration-options",
    "href": "workflows.html#configuration-options",
    "title": "Buildling Workflows",
    "section": "",
    "text": "config:\n  pdf_markdown: true          # Convert PDFs to markdown format\n  pdf_unstructured: false     # Use unstructured parsing for complex PDFs\n  infer_table_structure: true # Extract and preserve table structure\n\n\n\nconfig:\n  store_md5: true            # Add MD5 hash to document metadata\n  store_mimetype: true       # Add MIME type to document metadata  \n  store_file_dates: true     # Add creation/modification dates\n  extract_document_titles: true  # Extract document titles (requires LLM)\n\n\n\nconfig:\n  n_proc: 4                  # Use 4 CPU cores for parallel processing\n  verbose: true              # Show detailed progress information\n  batch_size: 1000          # Process documents in batches\n\n\n\n\nDifferent storage backends support different configuration options:\n# ChromaDB\nconfig:\n  persist_location: \"./chroma_db\"\n  collection_name: \"documents\"\n  \n# Whoosh\nconfig:\n  persist_location: \"./whoosh_index\"\n  schema_fields: [\"content\", \"title\", \"source\"]\n\n# Elasticsearch\nconfig:\n  persist_location: \"https://elastic:password@localhost:9200\"\n  index_name: \"document_index\"\n  basic_auth: [\"username\", \"password\"]",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#advanced-examples",
    "href": "workflows.html#advanced-examples",
    "title": "Buildling Workflows",
    "section": "",
    "text": "Process documents from multiple sources with different strategies:\nnodes:\n  # Load PDFs with table extraction\n  pdf_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"pdfs/\"\n      pdf_markdown: true\n      infer_table_structure: true\n  \n  # Load text files\n  text_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"texts/\"\n  \n  # Chunk PDFs by paragraph (preserve structure)\n  pdf_chunker:\n    type: SplitByParagraph\n    config:\n      chunk_size: 1000\n      chunk_overlap: 100\n  \n  # Chunk text files by character count\n  text_chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 500\n      chunk_overlap: 50\n  \n  # Store everything in unified index\n  unified_store:\n    type: WhooshStore\n    config:\n      persist_location: \"unified_index\"\n\nconnections:\n  - from: pdf_loader\n    from_port: documents\n    to: pdf_chunker\n    to_port: documents\n  \n  - from: text_loader\n    from_port: documents\n    to: text_chunker\n    to_port: documents\n  \n  - from: pdf_chunker\n    from_port: documents\n    to: unified_store\n    to_port: documents\n  \n  - from: text_chunker\n    from_port: documents\n    to: unified_store\n    to_port: documents\n\n\n\nMultiple processing steps in sequence:\nnodes:\n  loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"documents/\"\n      extract_document_titles: true\n  \n  # First pass: large chunks for context\n  coarse_chunker:\n    type: SplitByParagraph\n    config:\n      chunk_size: 2000\n      chunk_overlap: 200\n  \n  # Second pass: fine chunks for retrieval\n  fine_chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 400\n      chunk_overlap: 40\n  \n  # Store in vector database\n  vector_store:\n    type: ChromaStore\n    config:\n      persist_location: \"vector_db\"\n\nconnections:\n  - from: loader\n    from_port: documents\n    to: coarse_chunker\n    to_port: documents\n  \n  - from: coarse_chunker\n    from_port: documents\n    to: fine_chunker\n    to_port: documents\n  \n  - from: fine_chunker\n    from_port: documents\n    to: vector_store\n    to_port: documents\n\n\n\nDownload and process documents from URLs:\nnodes:\n  web_loader:\n    type: LoadWebDocument\n    config:\n      url: \"https://example.com/report.pdf\"\n      username: \"api_user\"\n      password: \"secret_key\"\n  \n  doc_processor:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 800\n      chunk_overlap: 80\n  \n  search_store:\n    type: ElasticsearchStore\n    config:\n      persist_location: \"http://elasticsearch:9200\"\n      index_name: \"web_documents\"\n\nconnections:\n  - from: web_loader\n    from_port: documents\n    to: doc_processor\n    to_port: documents\n  \n  - from: doc_processor\n    from_port: documents\n    to: search_store\n    to_port: documents\n\n\n\nProcess documents with comprehensive metadata enrichment and content transformation:\nnodes:\n  # Load meeting documents\n  meeting_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"meeting_docs/\"\n      include_patterns: [\"*.pdf\", \"*.docx\", \"*.txt\"]\n      \n  # Tag all documents with meeting metadata\n  tag_meeting_info:\n    type: AddMetadata\n    config:\n      metadata:\n        meeting_id: \"meeting20251001\"\n        department: \"engineering\"\n        project: \"Project Alpha\"\n        classification: \"internal\"\n        attendees: \"team_leads\"\n        \n  # Add confidential header to all documents\n  mark_confidential:\n    type: ContentPrefix\n    config:\n      prefix: \"[CONFIDENTIAL - PROJECT ALPHA TEAM ONLY]\"\n      separator: \"\\n\\n\"\n      \n  # Extract key information and enrich metadata\n  analyze_content:\n    type: PythonDocumentTransformer\n    config:\n      code: |\n        # Analyze document content for key information\n        import re\n        \n        # Basic statistics\n        word_count = len(content.split())\n        paragraph_count = len([p for p in content.split('\\n\\n') if p.strip()])\n        \n        # Look for action items and decisions\n        action_items = len(re.findall(r'(?:action item|todo|task):', content, re.IGNORECASE))\n        decisions = len(re.findall(r'(?:decision|resolved|agreed):', content, re.IGNORECASE))\n        \n        # Find mentions of team members\n        team_members = re.findall(r'@(\\w+)', content)\n        \n        # Classify document type\n        content_lower = content.lower()\n        if 'agenda' in content_lower:\n            doc_type = 'meeting_agenda'\n        elif action_items &gt; 0 or 'action' in content_lower:\n            doc_type = 'action_items'\n        elif 'minutes' in content_lower or 'notes' in content_lower:\n            doc_type = 'meeting_minutes'\n        else:\n            doc_type = 'meeting_document'\n        \n        # Update metadata with extracted information\n        metadata.update({\n            'word_count': word_count,\n            'paragraph_count': paragraph_count,\n            'action_items_count': action_items,\n            'decisions_count': decisions,\n            'mentioned_members': list(set(team_members)),\n            'document_type': doc_type,\n            'priority_score': min(10, (action_items * 2) + decisions),\n            'has_action_items': action_items &gt; 0,\n            'complexity': 'high' if word_count &gt; 1000 else 'medium' if word_count &gt; 300 else 'low'\n        })\n        \n        # Add document summary at the beginning\n        summary = f\"[{doc_type.upper()}: {word_count} words, {action_items} action items, Priority: {metadata['priority_score']}/10]\"\n        content = summary + \"\\n\\n\" + content\n        \n        # Create enriched document\n        transformed_doc = Document(\n            page_content=content,\n            metadata=metadata\n        )\n        \n  # Filter to keep only relevant documents\n  filter_important:\n    type: DocumentFilter\n    config:\n      metadata_filters:\n        classification: \"internal\"\n      # Keep documents with action items or decisions\n      content_contains: [\"action\", \"decision\", \"task\", \"todo\"]\n      min_length: 100\n      \n  # Add processing footer\n  add_footer:\n    type: ContentSuffix\n    config:\n      suffix: |\n        \n        ---\n        Document processed: 2025-01-16\n        Meeting ID: meeting20251001\n        Next review: 2025-01-23\n        Contact: project-alpha-admin@company.com\n        \n  # Chunk for storage\n  chunk_docs:\n    type: SplitByParagraph\n    config:\n      chunk_size: 1000\n      chunk_overlap: 100\n      \n  # Store enriched documents\n  meeting_store:\n    type: WhooshStore\n    config:\n      persist_location: \"meeting_20251001_index\"\n\nconnections:\n  - from: meeting_loader\n    from_port: documents\n    to: tag_meeting_info\n    to_port: documents\n    \n  - from: tag_meeting_info\n    from_port: documents\n    to: mark_confidential\n    to_port: documents\n    \n  - from: mark_confidential\n    from_port: documents\n    to: analyze_content\n    to_port: documents\n    \n  - from: analyze_content\n    from_port: documents\n    to: filter_important\n    to_port: documents\n    \n  - from: filter_important\n    from_port: documents\n    to: add_footer\n    to_port: documents\n    \n  - from: add_footer\n    from_port: documents\n    to: chunk_docs\n    to_port: documents\n    \n  - from: chunk_docs\n    from_port: documents\n    to: meeting_store\n    to_port: documents\nThis example demonstrates the power of DocumentTransformer nodes:\n\nMetadata Tagging - Organizes documents by meeting, project, and department\nContent Marking - Adds confidential headers for security\nIntelligent Analysis - Extracts action items, decisions, and team mentions\nQuality Filtering - Keeps only documents with actionable content\nProcessing Attribution - Adds footer with processing information\nSearchable Storage - Creates indexed, searchable document collection\n\nThe enriched metadata enables powerful queries like: - “Find all documents from meeting20251001 with action items” - “Show high-priority engineering documents from Project Alpha” - “List all documents mentioning specific team members”\n\n\n\nA real-world example that extracts statutory citations from Federal Acquisition Regulation (FAR) documents using specialized legal analysis prompts:\n# Federal Acquisition Regulation (FAR) Legal Analysis\n# Extracts statutory citations from FAR HTML files using LLM analysis and cleanup\n\nnodes:\n  # Load FAR HTML files (Part 9 - contractor qualifications)\n  far_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"/tmp/far_data\"  # Directory containing extracted FAR HTML files\n      include_patterns: [\"9.*.html\"]     # Focus on Part 9 sections\n      verbose: true\n  \n  # Keep each FAR section as complete document (no chunking needed)\n  full_sections:\n    type: KeepFullDocument\n    config: {}\n  \n  # Apply statute extraction prompt directly to all FAR sections\n  statute_extractor:\n    type: PromptProcessor\n    config:\n      prompt_file: \"prompts/statute_extraction.txt\"  # Complex legal analysis prompt\n      llm:\n        model_url: \"openai://gpt-4o-mini\"  # GPT-4o-mini for accurate legal analysis\n        temperature: 0                     # Deterministic results for legal work\n        mute_stream: true                  # Quiet processing\n      batch_size: 5                        # Process 5 sections at a time\n  \n  # Clean up the raw LLM responses to extract just the citations\n  citation_cleaner:\n    type: ResponseCleaner\n    config:\n      cleanup_prompt_file: \"prompts/statute_cleanup.txt\"  # Specialized cleanup prompt\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0\n        mute_stream: true\n  \n  # Export cleaned results to Excel spreadsheet\n  statute_analysis:\n    type: ExcelExporter\n    config:\n      output_path: \"far_statutory_citations_cleaned.xlsx\"\n      sheet_name: \"FAR_Part9_Analysis\"\n\nconnections:\n  # Pipeline: Load -&gt; Process -&gt; Extract -&gt; Cleanup -&gt; Export\n  - from: far_loader\n    from_port: documents\n    to: full_sections\n    to_port: documents\n  \n  - from: full_sections\n    from_port: documents\n    to: statute_extractor\n    to_port: documents\n  \n  - from: statute_extractor\n    from_port: results\n    to: citation_cleaner\n    to_port: results\n  \n  - from: citation_cleaner\n    from_port: results\n    to: statute_analysis\n    to_port: results\nData Preparation (run before workflow):\n# 1. Download FAR data\nwget https://www.acquisition.gov/sites/default/files/current/far/zip/html/FARHTML.zip\n\n# 2. Extract to processing directory\nunzip FARHTML.zip -d /tmp/far_data/\n\n# 3. Run the analysis workflow\npython -m onprem.workflow far_legal_analysis_simple.yaml\nKey Features: - Specialized Legal Processing: Uses domain-specific prompts for accurate legal text analysis - Two-Stage LLM Pipeline: Initial extraction followed by response cleanup for precision - External Prompt Files: Complex prompts stored in separate files for maintainability - Deterministic Results: Zero temperature ensures consistent legal analysis - Structured Output: Excel export with organized columns for legal review\nExpected Results: - Excel file with statutory citations extracted from Part 9 sections - Columns: document_id, source, response (cleaned citations), metadata - Ready for legal team review and compliance analysis\nThis example demonstrates how workflow pipelines can handle complex, domain-specific document analysis tasks with multiple LLM processing stages and specialized prompts.",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#validation-and-error-handling",
    "href": "workflows.html#validation-and-error-handling",
    "title": "Buildling Workflows",
    "section": "",
    "text": "The workflow engine performs comprehensive validation:\n\n\n\nPort Existence: Verifies source and target ports exist\nType Compatibility: Ensures data types match between connections\nNode Compatibility: Enforces valid connection patterns:\n\n✅ Loader → TextSplitter\n✅ Loader → DocumentTransformer\n✅ TextSplitter → TextSplitter\n\n✅ TextSplitter → DocumentTransformer\n✅ TextSplitter → Storage\n✅ DocumentTransformer → TextSplitter\n✅ DocumentTransformer → DocumentTransformer\n\n✅ DocumentTransformer → Storage\n✅ Query → DocumentTransformer\n❌ Loader → Storage (must have TextSplitter or DocumentTransformer in between)\n❌ Storage → Any (Storage nodes are terminal)\n\n\n\n\n\n\nFile Existence: Checks that source directories and files exist\nConfiguration Validation: Validates required parameters\nDependency Resolution: Uses topological sorting to determine execution order\nCycle Detection: Prevents infinite loops in workflows\n\n\n\n\nThe engine provides detailed error messages:\n# Invalid node type\nWorkflowValidationError: Unknown node type: InvalidNodeType\n\n# Missing required configuration\nNodeExecutionError: Node my_loader: source_directory is required\n\n# Invalid connection\nWorkflowValidationError: Loader node loader can only connect to TextSplitter nodes, not ChromaStoreNode\n\n# Type mismatch\nWorkflowValidationError: Type mismatch: loader.documents (List[Document]) -&gt; storage.text (str)\n\n# Missing port\nWorkflowValidationError: Source node loader has no output port 'data'. Available: ['documents']\n\n\n\nQuery existing storage, apply AI analysis, and export to spreadsheet:\nnodes:\n  # Search for research documents\n  research_query:\n    type: QueryWhooshStore\n    config:\n      persist_location: \"research_index\"\n      query: \"methodology results conclusions findings\"\n      limit: 25\n  \n  # Analyze each document with custom prompts\n  research_analysis:\n    type: PromptProcessor\n    config:\n      prompt: |\n        Analyze this research document and extract:\n        \n        1. RESEARCH QUESTION: What is the main research question?\n        2. METHODOLOGY: What research methods were used?\n        3. KEY FINDINGS: What are the 3 most important findings?\n        4. LIMITATIONS: What limitations are mentioned?\n        5. CONFIDENCE: How confident are the conclusions (High/Medium/Low)?\n        \n        Document: {source}\n        Content: {content}\n        \n        Please format each answer on a separate line.\n      model_name: \"gpt-4\"\n      batch_size: 3\n  \n  # Export to Excel for review and analysis\n  analysis_report:\n    type: ExcelExporter\n    config:\n      output_path: \"research_analysis_report.xlsx\"\n      sheet_name: \"Document_Analysis\"\n\nconnections:\n  - from: research_query\n    from_port: documents\n    to: research_analysis\n    to_port: documents\n  \n  - from: research_analysis\n    from_port: results\n    to: analysis_report\n    to_port: results\n\n\n\nExtract individual document topics and create an aggregated analysis:\nnodes:\n  # Load documents from directory\n  document_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"../sample_data\"\n      include_patterns: [\"*.pdf\", \"*.txt\", \"*.html\"]\n      pdf_markdown: true\n      store_file_dates: true\n  \n  # Keep documents whole for comprehensive analysis\n  full_document_processor:\n    type: KeepFullDocument\n    config:\n      concatenate_pages: true\n  \n  # Extract topic keyword from each document\n  document_analyzer:\n    type: PromptProcessor\n    config:\n      prompt: |\n        Provide a single short keyword or keyphrase that captures the topic of the following text from {source} (only output the keyphrase without quotes and nothing else): {content}\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0.2\n        mute_stream: true\n      batch_size: 2\n  \n  # Clean up responses for consistency\n  response_cleaner:\n    type: ResponseCleaner\n    config:\n      cleanup_prompt: |\n        Clean up this analysis response to ensure consistent formatting:\n        {original_response}\n        \n        Requirements:\n        - Ensure the response is a single short keyword or keyphrase that captures the topic and nothing else.\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0\n        mute_stream: true\n  \n  # Aggregate all topics into comprehensive analysis\n  topic_aggregator:\n    type: AggregatorNode\n    config:\n      prompt: |\n        Analyze these {num_results} topic keywords from different documents and provide a comprehensive topic analysis:\n        \n        {responses}\n        \n        Please provide your analysis in the following structure:\n        1. TOP TOPICS: List the 5 most frequently mentioned topics with their frequency counts\n        2. TOPIC CATEGORIES: Group related topics into broader categories (e.g., Technology, Business, etc.)\n        3. COVERAGE ANALYSIS: What percentage of documents cover each major theme?\n        4. INSIGHTS: What patterns or trends do you observe across the document collection?\n        5. EXECUTIVE SUMMARY: One paragraph summarizing the overall thematic landscape\n        \n        Format your response with clear headings and bullet points for easy reading.\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0.3\n        max_tokens: 1000\n        mute_stream: true\n  \n  # Export individual document topics to CSV\n  individual_topics_export:\n    type: CSVExporter\n    config:\n      output_path: \"document_analysis_summary.csv\"\n  \n  # Export aggregated analysis to JSON\n  aggregated_export:\n    type: JSONExporter\n    config:\n      output_path: \"document_topic_analysis.json\"\n      pretty_print: true\n\nconnections:\n  # Pipeline: Load → Process → Analyze → Clean → Export Both Ways\n  - from: document_loader\n    from_port: documents\n    to: full_document_processor\n    to_port: documents\n  \n  - from: full_document_processor\n    from_port: documents\n    to: document_analyzer\n    to_port: documents\n  \n  - from: document_analyzer\n    from_port: results\n    to: response_cleaner\n    to_port: results\n  \n  # Export individual document topics to CSV\n  - from: response_cleaner\n    from_port: results\n    to: individual_topics_export\n    to_port: results\n  \n  # Aggregate topics for comprehensive analysis\n  - from: response_cleaner\n    from_port: results\n    to: topic_aggregator\n    to_port: results\n  \n  # Export aggregated analysis to JSON (note: result → result port)\n  - from: topic_aggregator\n    from_port: result\n    to: aggregated_export\n    to_port: result\nThis example demonstrates the power of aggregation workflows:\nBenefits: - Individual Analysis: CSV file with topic keyword for each document - Collection Insights: JSON file with aggregated analysis of all topics - Dual Output: Both granular and summary views of your document collection - Executive Summary: High-level insights perfect for reporting and decision-making\nOutput Files: - document_analysis_summary.csv - Individual document topics (one row per document) - document_topic_analysis.json - Aggregated topic analysis with patterns and insights\nKey Pattern: Documents → Individual Analysis → Split → (CSV Export + Aggregation → JSON Export)\n\n\n\nExport search results directly from vector stores without additional processing, perfect for document cataloging, research data collection, and content audits:\nnodes:\n  # Search for documents matching specific criteria\n  research_search:\n    type: QueryDualStore\n    config:\n      persist_location: \"~/vectordb\"\n      query: \"machine learning neural networks deep learning\"\n      search_type: \"hybrid\"\n      limit: 50\n      weights: [0.7, 0.3]  # Favor semantic search\n\n  # Convert documents to export-ready format\n  prepare_export:\n    type: DocumentToResults\n    config:\n      include_content: true\n      content_field: \"document_content\"\n      metadata_prefix: \"\"  # No prefix for cleaner column names\n      custom_fields:\n        search_query: \"machine learning neural networks deep learning\"\n        export_timestamp: \"2025-01-16T10:30:00Z\"\n        search_type: \"hybrid\"\n        analyst: \"research_team\"\n        project: \"ML_Literature_Review\"\n\n  # Export to Excel for analysis and sharing\n  research_export:\n    type: ExcelExporter\n    config:\n      output_path: \"ml_research_documents.xlsx\"\n      sheet_name: \"Search_Results\"\n\n  # Also export to CSV for data processing\n  data_export:\n    type: CSVExporter\n    config:\n      output_path: \"ml_research_data.csv\"\n\n  # And to JSON for programmatic access\n  json_export:\n    type: JSONExporter\n    config:\n      output_path: \"ml_research_index.json\"\n      pretty_print: true\n\nconnections:\n  # Single search feeds multiple export formats\n  - from: research_search\n    from_port: documents\n    to: prepare_export\n    to_port: documents\n\n  - from: prepare_export\n    from_port: results\n    to: research_export\n    to_port: results\n\n  - from: prepare_export\n    from_port: results\n    to: data_export\n    to_port: results\n\n  - from: prepare_export\n    from_port: results\n    to: json_export\n    to_port: results\nBenefits: - Simple Pipeline: Only 3 nodes for Query → Convert → Export - Multiple Formats: Single search exported to Excel, CSV, and JSON simultaneously - Rich Metadata: All document metadata preserved and exported - Search Tracking: Custom fields track search parameters and export details - Ready for Analysis: Excel format perfect for manual review and annotation\nOutput Files: - ml_research_documents.xlsx - Formatted spreadsheet for manual analysis - ml_research_data.csv - Raw data for statistical analysis or import into other tools - ml_research_index.json - Structured data for programmatic processing\nUse Cases: - Literature Reviews - Export relevant papers with metadata for analysis - Document Audits - Create inventories of document collections - Research Data Collection - Gather documents matching specific criteria - Content Cataloging - Build searchable catalogs of document libraries - Compliance Reporting - Export document lists with metadata for regulatory review\nKey Pattern: Query → Convert → Multiple Exports\n\n\n\nProcess documents and export results in multiple formats:\nnodes:\n  # Query for AI/ML papers\n  ai_papers:\n    type: QueryChromaStore\n    config:\n      persist_location: \"vector_db\"\n      query: \"artificial intelligence machine learning neural networks\"\n      limit: 15\n  \n  # Generate structured summaries\n  paper_summaries:\n    type: SummaryProcessor\n    config:\n      max_length: 200\n      model_name: \"gpt-3.5-turbo\"\n  \n  # Export to CSV for spreadsheet analysis\n  csv_export:\n    type: CSVExporter\n    config:\n      output_path: \"ai_paper_summaries.csv\"\n      columns: [\"document_id\", \"source\", \"summary\", \"original_length\"]\n  \n  # Export to JSON for programmatic access\n  json_export:\n    type: JSONExporter\n    config:\n      output_path: \"ai_paper_summaries.json\"\n      pretty_print: true\n  \n  # Export to Excel for presentation\n  excel_export:\n    type: ExcelExporter\n    config:\n      output_path: \"ai_paper_report.xlsx\"\n      sheet_name: \"AI_ML_Summaries\"\n\nconnections:\n  # Single source, multiple outputs\n  - from: ai_papers\n    from_port: documents\n    to: paper_summaries\n    to_port: documents\n  \n  - from: paper_summaries\n    from_port: results\n    to: csv_export\n    to_port: results\n  \n  - from: paper_summaries\n    from_port: results\n    to: json_export\n    to_port: results\n  \n  - from: paper_summaries\n    from_port: results\n    to: excel_export\n    to_port: results",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#best-practices",
    "href": "workflows.html#best-practices",
    "title": "Buildling Workflows",
    "section": "",
    "text": "# Use descriptive node names\nnodes:\n  legal_document_loader:        # Not: loader1\n    type: LoadFromFolder\n    config:\n      source_directory: \"legal_docs/\"\n  \n  contract_chunker:            # Not: splitter1\n    type: SplitByParagraph\n    config:\n      chunk_size: 1500\n  \n  contract_search_index:       # Not: storage1\n    type: WhooshStore\n    config:\n      persist_location: \"contract_index\"\n\n\n\nUse environment variables for paths and sensitive data:\nnodes:\n  loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"${DOCUMENT_PATH:-./documents}\"  # Default fallback\n  \n  es_store:\n    type: ElasticsearchStore\n    config:\n      persist_location: \"${ELASTICSEARCH_URL}\"\n      basic_auth: [\"${ES_USERNAME}\", \"${ES_PASSWORD}\"]\n\n\n\nChoose appropriate chunk sizes for your use case:\n# Small chunks (200-400 chars) - Good for:\n# - Precise retrieval\n# - Question answering\n# - Fine-grained search\n\n# Medium chunks (500-1000 chars) - Good for:\n# - General purpose RAG\n# - Balanced context/precision\n# - Most common use case\n\n# Large chunks (1000-2000 chars) - Good for:\n# - Document summarization\n# - Context-heavy tasks\n# - Preserving document structure\n\n\n\nnodes:\n  bulk_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"large_corpus/\"\n      n_proc: 8                    # Parallel processing\n      verbose: false               # Reduce logging overhead\n      batch_size: 500             # Process in batches\n  \n  efficient_chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 500\n      chunk_overlap: 25           # Reduce overlap for speed\n\n\n\nnodes:\n  rich_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"documents/\"\n      store_md5: true              # Document integrity\n      store_mimetype: true         # File type tracking\n      store_file_dates: true       # Temporal information\n      extract_document_titles: true # Content-aware metadata\n      infer_table_structure: true  # Preserve structure",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "workflows.html#troubleshooting",
    "href": "workflows.html#troubleshooting",
    "title": "Buildling Workflows",
    "section": "",
    "text": "# Ensure you're in the correct directory\ncd /path/to/onprem/project\n\n# Or add to Python path\nexport PYTHONPATH=/path/to/onprem:$PYTHONPATH\n\n\n\n# Use absolute paths for reliability\nnodes:\n  loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"/full/path/to/documents\"  # Not: \"documents/\"\n\n\n\n# Process in smaller batches\nnodes:\n  loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"large_docs/\"\n      batch_size: 100              # Reduce batch size\n      n_proc: 2                    # Reduce parallelism\n  \n  chunker:\n    type: SplitByCharacterCount\n    config:\n      chunk_size: 300              # Smaller chunks\n      chunk_overlap: 30            # Less overlap\n\n\n\n# Test connectivity first\nnodes:\n  es_store:\n    type: ElasticsearchStore\n    config:\n      persist_location: \"http://localhost:9200\"\n      # Add authentication if needed\n      basic_auth: [\"username\", \"password\"]\n      # Increase timeouts if needed\n      timeout: 30\n\n\n\n\nControl document size for LLM processing and cost optimization:\nnodes:\n  # Load large documents\n  document_loader:\n    type: LoadFromFolder\n    config:\n      source_directory: \"large_documents/\"\n  \n  # Keep full documents but truncate to manageable size\n  size_control:\n    type: KeepFullDocument\n    config:\n      concatenate_pages: true    # First combine multi-page documents\n      max_words: 2000           # Then truncate to first 2000 words\n  \n  # Analyze the controlled-size documents\n  quick_analysis:\n    type: PromptProcessor\n    config:\n      prompt: |\n        Analyze this document excerpt (first 2000 words):\n        \n        Source: {source}\n        Content: {content}\n        \n        Provide:\n        1. Document type and purpose\n        2. Main topics covered\n        3. Key findings or conclusions\n        4. Whether this appears to be the beginning, middle, or complete document\n      llm:\n        model_url: \"openai://gpt-4o-mini\"\n        temperature: 0.1\n        max_tokens: 500\n\nconnections:\n  - from: document_loader\n    from_port: documents\n    to: size_control\n    to_port: documents\n    \n  - from: size_control\n    from_port: documents\n    to: quick_analysis\n    to_port: documents\nBenefits: - Cost Control - Process only document beginnings instead of entire files - Speed - Faster analysis with consistent processing times - Context Management - Ensure documents fit within LLM context windows - Preview Analysis - Get quick insights from document openings - Metadata Tracking - Know original document size and truncation status\n\n\n\nEnable verbose output to see detailed execution:\nfrom onprem.workflow import execute_workflow\n\n# Detailed logging\nresults = execute_workflow(\"workflow.yaml\", verbose=True)\n\n# Check results\nfor node_id, result in results.items():\n    print(f\"Node {node_id}: {result}\")\nValidate before execution:\nfrom onprem.workflow import WorkflowEngine\n\nengine = WorkflowEngine()\ntry:\n    engine.load_workflow_from_yaml(\"workflow.yaml\")\n    print(\"✓ Workflow validation passed\")\nexcept Exception as e:\n    print(f\"✗ Validation failed: {e}\")\n\n\n\nTrack processing times and document counts:\nimport time\nfrom onprem.workflow import execute_workflow\n\nstart_time = time.time()\nresults = execute_workflow(\"workflow.yaml\", verbose=True)\nend_time = time.time()\n\nprint(f\"Processing time: {end_time - start_time:.2f} seconds\")\n\n# Count processed documents\ntotal_docs = 0\nfor node_id, result in results.items():\n    if 'documents' in result:\n        count = len(result['documents'])\n        print(f\"{node_id}: {count} documents\")\n        total_docs += count\n\nprint(f\"Total documents processed: {total_docs}\")\n\nThis tutorial covers all aspects of the OnPrem workflow engine.",
    "crumbs": [
      "Examples",
      "Buildling Workflows"
    ]
  },
  {
    "objectID": "hf.models.registry.html",
    "href": "hf.models.registry.html",
    "title": "hf.models.registry",
    "section": "",
    "text": "source\n\nRegistry\n\ndef Registry(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nMethods to register models and fully support pipelines."
  },
  {
    "objectID": "hf.data.tokens.html",
    "href": "hf.data.tokens.html",
    "title": "hf.data.tokens",
    "section": "",
    "text": "source\n\nTokens\n\ndef Tokens(\n    columns\n):\n\nDefault dataset used to hold tokenized data."
  },
  {
    "objectID": "examples_summarization.html",
    "href": "examples_summarization.html",
    "title": "Summarization",
    "section": "",
    "text": "The pipelines modules in OnPrem.LLM includes the Summarizer to summarize one or more documents with an LLM in different ways. This notebook shows a couple of examples.",
    "crumbs": [
      "Examples",
      "Summarization"
    ]
  },
  {
    "objectID": "examples_summarization.html#document-summarization",
    "href": "examples_summarization.html#document-summarization",
    "title": "Summarization",
    "section": "Document Summarization",
    "text": "Document Summarization\nThe Summarizer.summarize method runs multiple intermediate prompts and inferences, so we will set verbose-False and mute_stream=True. We will also set temperature=0 for more consistency in outputs. Finally, we will use the Zephyr-7B-beta and use the appropriate prompt template obtained from here. You can experiment with different, newer models to improve results.\n\nfrom onprem import LLM\nfrom onprem.pipelines import Summarizer\nllm = LLM(model_url='https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf', \n          prompt_template= \"&lt;|system|&gt;\\n&lt;/s&gt;\\n&lt;|user|&gt;\\n{prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\",\n          n_gpu_layers=-1, verbose=False, mute_stream=True, temperature=0) # set based on your system\nsummarizer = Summarizer(llm)\n\nNext, let’s download the ktrain paper and summarize it.\n\n!wget --user-agent=\"Mozilla\" https://arxiv.org/pdf/2004.10703.pdf -O /tmp/ktrain.pdf -q\n\n\ntext = summarizer.summarize('/tmp/ktrain.pdf', max_chunks_to_use=5)\nprint(text['output_text'])\n\n/home/amaiya/projects/ghub/onprem/onprem/pipelines/summarizer.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n  map_chain = LLMChain(llm=langchain_llm, prompt=map_prompt)\n/home/amaiya/projects/ghub/onprem/onprem/pipelines/summarizer.py:157: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain/\n  combine_documents_chain = StuffDocumentsChain(\n/home/amaiya/projects/ghub/onprem/onprem/pipelines/summarizer.py:162: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n  reduce_documents_chain = ReduceDocumentsChain(\n/home/amaiya/projects/ghub/onprem/onprem/pipelines/summarizer.py:171: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n  map_reduce_chain = MapReduceDocumentsChain(\n\n\n\nKtrain is a low-code Python library that simplifies the machine learning process by providing a unified interface for building, training, inspecting, and applying models using various types of data such as text, vision, and tabular. It automates where possible but also allows users to make choices based on their unique requirements. Ktrain supports TensorFlow Keras models and includes out-of-the-box support for tasks like text classification, sequence tagging, image classification, node classification, and link prediction. It offers state-of-the-art models like BERT and fastText, learning rate finders, optimization techniques, explainable AI tools, and a simple prediction API for deployment. Ktrain is an open-source machine learning platform that automates various tasks beyond just model selection and architecture search in AutoML approaches. It provides text classification, regression, sequence tagging, topic modeling, document similarity, recommendation, summarization, and question answering for text data, as well as node classification and link prediction for graph data. Overall, ktrain aims to augment human engineers' strengths rather than replace them in the ML process.\n\n\nTips: For faster summarizations, we set max_chunks_to_use=5, so that only the first five chunks of 1000 characters are considered (where chunk_size=1000 is set as the default). You can set max_chunks_to_use to None (or omit the parameter) to consider the entire document when generating the summarization.",
    "crumbs": [
      "Examples",
      "Summarization"
    ]
  },
  {
    "objectID": "examples_summarization.html#concept-focused-summarization",
    "href": "examples_summarization.html#concept-focused-summarization",
    "title": "Summarization",
    "section": "Concept-Focused Summarization",
    "text": "Concept-Focused Summarization\nConcept-focused summarization allows you to summarize a long document with respect to a concept of interest. This can be accomplished by invoking the summarizer.summarize_by_concept method and supplying a concept_description.\nIn this example, we will use Ollama as the backend. You can install Ollama from here and download the model with: ollama pull llama3.1.\nLet’s summarize a National Defense Authorization Act (NDAA) report by the concept (or topic) of hypersonics.\n\nSummarizing the NDAA by Concept of Hypersonics\n\nfrom onprem import LLM, utils\nfrom onprem.pipelines import Summarizer\n\n\n# STEP 1: Load LLM and setup Summarizer\nllm = LLM('ollama_chat/llama3.1', mute_stream=True, temperature=0)\nsummarizer = Summarizer(llm)\n\n# STEP 2: download NDAA report\nutils.download('https://www.congress.gov/118/crpt/hrpt125/CRPT-118hrpt125.pdf', '/tmp/ndaa/ndaa.pdf', verify=True)\n\n# STEP 3: Summarize with respect to concept (e.g., hypersonics)\nsummary, sources = summarizer.summarize_by_concept('/tmp/ndaa/ndaa.pdf', concept_description=\"hypersonics\")\nprint()\nprint(summary)\n\n[██████████████████████████████████████████████████]\nThe context discusses \"hypersonics\" in several sections, highlighting the importance of advancing this technology for national defense. Here are some key points related to hypersonics:\n\n1. **Education and Workforce Development**: The committee recommends strengthening partnerships with academic institutions to promote and educate students in hypersonic technology. It also suggests establishing a pilot program at select institutions to expand graduate and pre-doctoral degree programs.\n2. **Funding for Advanced Hypersonics Facilities**: The committee recommends increasing funding for advanced hypersonics facilities for research and graduate-level education, allocating $543.9 million (an increase of $3.0 million) in PE 0601153N for hypersonic education efforts.\n3. **Hypersonics Prototyping**: The committee mentions two programs related to hypersonics prototyping: HYPERSONICS PROTOTYPING ($150,340) and HYPERSONICS PROTOTYPING—HYPERSONIC ATTACK CRUISE MISSILE (HACM) ($381,528).\n4. **Multi-Service Advanced Capability Hypersonics Test Bed (MACH-TB)**: The committee encourages the Department of Defense to fully fund the MACH-TB program in future budget requests to achieve full-scale flight test objectives and expansion of critical test infrastructure.\n5. **Hypersonic Workforce Development**: The committee expresses concern about the Department's ability to sustain a highly skilled workforce for hypersonic technology development and recommends expanding and fully funding science, technology, engineering, and mathematics (STEM) programs, particularly in the field of hypersonics.\n\nOverall, the context emphasizes the importance of advancing hypersonic technology for national defense and highlights the need for education, workforce development, and research investments to support this effort.\n\n\n\n\nSummarize a blog post on LLMs with respect to prompting\n\nfrom langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\ndocs = loader.load()\nwith open('/tmp/blog.txt', 'w') as f:\n    f.write(docs[0].page_content)\n\nsummary, sources = summarizer.summarize_by_concept('/tmp/blog.txt', concept_description=\"prompting\")\n\n\nprint(summary)\n\nThe context discusses \"prompting\" as a technique used to interact with Large Language Models (LLMs) and influence their behavior, particularly in the context of autonomous agent systems.\n\nIn this context, prompting refers to providing specific instructions or questions to an LLM to elicit a particular response or action. The goal is to guide the model's thinking process and encourage it to generate more accurate, relevant, or useful outputs.\n\nThere are several types of prompts mentioned:\n\n1. **Simple prompting**: Providing basic instructions, such as \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\"\n2. **Task-specific instructions**: Providing domain-specific guidance, like \"Write a story outline\" for writing a novel.\n3. **Human inputs**: Incorporating human feedback or input into the prompting process.\n\nThe context also highlights various techniques that use prompting to enhance LLM performance on complex tasks, including:\n\n1. **Chain of Thought (CoT)**: A technique that instructs the model to \"think step by step\" and decompose hard tasks into smaller steps.\n2. **Tree of Thoughts**: An extension of CoT that explores multiple reasoning possibilities at each step.\n3. **LLM+P**: A method that relies on an external classical planner to do long-horizon planning.\n\nOverall, the context emphasizes the importance of prompting in guiding LLMs and enabling them to perform more effectively in complex tasks.",
    "crumbs": [
      "Examples",
      "Summarization"
    ]
  },
  {
    "objectID": "examples_summarization.html#more-options",
    "href": "examples_summarization.html#more-options",
    "title": "Summarization",
    "section": "More Options",
    "text": "More Options\nIf there is a need, you can experiment with different parameters, as described in our documentation.",
    "crumbs": [
      "Examples",
      "Summarization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OnPrem.LLM",
    "section": "",
    "text": "A privacy-conscious toolkit for document intelligence — local by default, cloud-capable\nOnPrem.LLM (or “OnPrem” for short) is a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. Inspired largely by the privateGPT project, OnPrem.LLM is designed for fully local execution, but also supports integration with a wide range of cloud LLM providers (e.g., OpenAI, Anthropic).\nKey Features:\nThe full documentation is here.\nQuick Start\nMany LLM backends are supported (e.g., llama_cpp, transformers, Ollama, vLLM, OpenAI, Anthropic, etc.).\nLatest News 🔥",
    "crumbs": [
      "OnPrem.LLM"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "OnPrem.LLM",
    "section": "Install",
    "text": "Install\nOnce you have installed PyTorch, you can install OnPrem.LLM with the following steps:\n\nInstall llama-cpp-python (optional - see below):\n\nCPU: pip install llama-cpp-python (extra steps required for Microsoft Windows)\nGPU: Follow instructions below.\n\nInstall OnPrem.LLM with Chroma packages: pip install onprem[chroma]\n\nFor RAG using only a sparse vectorstore, you can install OnPrem.LLM without the extra chroma packages: pip install onprem.\nNote: Installing llama-cpp-python is optional if any of the following is true:\n\nYou are using Ollama as the LLM backend.\nYou use Hugging Face Transformers (instead of llama-cpp-python) as the LLM backend by supplying the model_id parameter when instantiating an LLM, as shown here.\nYou are using OnPrem.LLM with an LLM being served through an external REST API (e.g., vLLM, OpenLLM).\nYou are using OnPrem.LLM with a cloud LLM (more information below).\n\n\nOn GPU-Accelerated Inference With llama-cpp-python\nWhen installing llama-cpp-python with pip install llama-cpp-python, the LLM will run on your CPU. To generate answers much faster, you can run the LLM on your GPU by building llama-cpp-python based on your operating system.\n\nLinux: CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\nMac: CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\nWindows 11: Follow the instructions here.\nWindows Subsystem for Linux (WSL2): Follow the instructions here.\n\nFor Linux and Windows, you will need an up-to-date NVIDIA driver along with the CUDA toolkit installed before running the installation commands above.\nAfter following the instructions above, supply the n_gpu_layers=-1 parameter when instantiating an LLM to use your GPU for fast inference:\nllm = LLM(n_gpu_layers=-1, ...)\nQuantized models with 8B parameters and below can typically run on GPUs with as little as 6GB of VRAM. If a model does not fit on your GPU (e.g., you get a “CUDA Error: Out-of-Memory” error), you can offload a subset of layers to the GPU by experimenting with different values for the n_gpu_layers parameter (e.g., n_gpu_layers=20). Setting n_gpu_layers=-1, as shown above, offloads all layers to the GPU.\nSee the FAQ for extra tips, if you experience issues with llama-cpp-python installation.",
    "crumbs": [
      "OnPrem.LLM"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "OnPrem.LLM",
    "section": "How to Use",
    "text": "How to Use\n\nSetup\n\nfrom onprem import LLM\n\nllm = LLM(verbose=False) # default model and backend are used\n\n\nCheat Sheet\nLocal Models: A number of different local LLM backends are supported.\n\nLlama-cpp: llm = LLM(default_model=\"llama\", n_gpu_layers=-1)\nLlama-cpp with selected GGUF model via URL:\n # prompt templates are required for user-supplied GGUF models (see FAQ)\n llm = LLM(model_url='https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf', \n           prompt_template= \"&lt;|system|&gt;\\n&lt;/s&gt;\\n&lt;|user|&gt;\\n{prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\", n_gpu_layers=-1)\nLlama-cpp with selected GGUF model via file path:\n # prompt templates are required for user-supplied GGUF models (see FAQ)\n llm = LLM(model_url='zephyr-7b-beta.Q4_K_M.gguf', \n           model_download_path='/path/to/folder/to/where/you/downloaded/model',\n           prompt_template= \"&lt;|system|&gt;\\n&lt;/s&gt;\\n&lt;|user|&gt;\\n{prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\", n_gpu_layers=-1)\nHugging Face Transformers: llm = LLM(model_id='Qwen/Qwen2.5-0.5B-Instruct', device='cuda')\nOllama: llm = LLM(model_url=\"ollama://llama3.2\", api_key='na')\nAlso Ollama: llm = LLM(model_url=\"ollama/llama3.2\", api_key='na')\nAlso Ollama: llm = LLM(model_url='http://localhost:11434/v1', api_key='na', model='llama3.2')\nvLLM: llm = LLM(model_url='http://localhost:8666/v1', api_key='na', model='Qwen/Qwen2.5-0.5B-Instruct')\nAlso vLLM: llm = LLM('hosted_vllm/served-model-name', api_base=\"http://localhost:8666/v1\", api_key=\"test123\") (assumes served-model-name parameter is supplied to vllm.entrypoints.openai.api_server).\nvLLM with gpt-oss (assumes served-model-name parameter is supplied to vLLM):\n# important: set max_tokens to high value due to intermediate reasoning steps that are generated\nllm = LLM(model_url='http://localhost:8666/v1', api_key='your_api_key', model=served_model_name, max_tokens=32000)\nresult = llm.prompt(prompt, reasoning_effort=\"high\")\n\nCloud Models: In addition to local LLMs, all cloud LLM providers supported by LiteLLM are compatible:\n\nAnthropic Claude: llm = LLM(model_url=\"anthropic/claude-3-7-sonnet-latest\")\nOpenAI GPT-4o: llm = LLM(model_url=\"openai/gpt-4o\")\nAWS GovCloud Bedrock (assumes AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set as environment variables)\nfrom onprem import LLM\ninference_arn = \"YOUR INFERENCE ARN\"\nendpoint_url = \"YOUR ENDPOINT URL\"\nregion_name = \"us-gov-east-1\" # replace as necessary\n# set up LLM connection to Bedrock on AWS GovCloud\nllm = LLM( f\"govcloud-bedrock://{inference_arn}\", region_name=region_name, endpoint_url=endpoint_url)\nresponse = llm.prompt(\"Write a haiku about the moon.\")\n\nThe instantiations above are described in more detail below.\n\n\nGGUF Models and Llama.cpp\nThe default LLM backend is llama-cpp-python, and the default model is currently a 7B-parameter model called Zephyr-7B-beta, which is automatically downloaded and used. Llama.cpp run models in GGUF format. The two other default models are llama and mistral. For instance, if default_model='llama' is supplied, then a Llama-3.1-8B-Instsruct model is automatically downloaded and used:\n# Llama 3.1 is downloaded here and the correct prompt template for Llama-3.1 is automatically configured and used\nllm = LLM(default_model='llama')\nChoosing Your Own Models: Of course, you can also easily supply the URL or path to an LLM of your choosing to LLM (see the FAQ for an example).\nSupplying Extra Parameters: Any extra parameters supplied to LLM are forwarded directly to llama-cpp-python, the default LLM backend.\n\n\nChanging the Default LLM Backend\nIf default_engine=\"transformers\" is supplied to LLM, Hugging Face transformers is used as the LLM backend. Extra parameters to LLM (e.g., ‘device=’cuda’) are forwarded diretly totransformers.pipeline. If supplying amodel_id` parameter, the default LLM backend is automatically changed to Hugging Face transformers.\n# LLama-3.1 model quantized using AWQ is downloaded and run with Hugging Face transformers (requires GPU)\nllm = LLM(default_model='llama', default_engine='transformers')\n\n# Using a custom model with Hugging Face Transformers\nllm = LLM(model_id='Qwen/Qwen2.5-0.5B-Instruct', device_map='cpu')\nSee here for more information about using Hugging Face transformers as the LLM backend.\nYou can also connect to Ollama, local LLM APIs (e.g., vLLM), and cloud LLMs.\n# connecting to an LLM served by Ollama\nlm = LLM(model_url='ollama/llama3.2')\n\n# connecting to an LLM served through vLLM (set API key as needed)\nllm = LLM(model_url='http://localhost:8000/v1', api_key='token-abc123', model='Qwen/Qwen2.5-0.5B-Instruct')`\n\n# connecting to a cloud-backed LLM (e.g., OpenAI, Anthropic).\nllm = LLM(model_url=\"openai/gpt-4o-mini\")  # OpenAI\nllm = LLM(model_url=\"anthropic/claude-3-7-sonnet-20250219\") # Anthropic\nOnPrem.LLM suppports any provider and model supported by the LiteLLM package.\nSee here for more information on local LLM APIs.\nMore information on using OpenAI models specifically with OnPrem.LLM is here.\n\n\nSupplying Parameters to the LLM Backend\nExtra parameters supplied to LLM and LLM.prompt are passed directly to the LLM backend. Parameter names will vary depending on the backend you chose.\nFor instance, with the default llama-cpp backend, the default context window size (n_ctx) is set to 3900 and the default output size (max_tokens) is set 512. Both are configurable parameters to LLM. Increase if you have larger prompts or need longer outputs. Other parameters (e.g., api_key, device_map, etc.) can be supplied directly to LLM and will be routed to the LLM backend or API (e.g., llama-cpp-python, Hugging Face transformers, vLLM, OpenAI, etc.). The max_tokens parameter can also be adjusted on-the-fly by supplying it to LLM.prompt.\nOn the other hand, for Ollama models, context window and output size are controlled by num_ctx and num_predict, respectively.\nWith the Hugging Face transformers, setting the context window size is not needed, but the output size is controlled by the max_new_tokens parameter to LLM.prompt.\n\n\n\nSend Prompts to the LLM to Solve Problems\nThis is an example of few-shot prompting, where we provide an example of what we want the LLM to do.\n\nprompt = \"\"\"Extract the names of people in the supplied sentences.\nSeparate names with commas and place on a single line.\n\n# Example 1:\nSentence: James Gandolfini and Paul Newman were great actors.\nPeople:\nJames Gandolfini, Paul Newman\n\n# Example 2:\nSentence:\nI like Cillian Murphy's acting. Florence Pugh is great, too.\nPeople:\"\"\"\n\nsaved_output = llm.prompt(prompt, stop=['\\n\\n'])\n\n\nCillian Murphy, Florence Pugh\n\n\nAdditional prompt examples are shown here.\n\n\nTalk to Your Documents\nAnswers are generated from the content of your documents (i.e., retrieval augmented generation or RAG). Here, we will use GPU offloading to speed up answer generation using the default model. However, the Zephyr-7B model may perform even better, responds faster, and is used in our RAG example notebook.\n\nfrom onprem import LLM\n\nllm = LLM(n_gpu_layers=-1, store_type='sparse', verbose=False)\n\nllama_new_context_with_model: n_ctx_per_seq (3904) &lt; n_ctx_train (32768) -- the full capacity of the model will not be utilized\n\n\nThe default embedding model is: sentence-transformers/all-MiniLM-L6-v2. You can change it by supplying the embedding_model_name to LLM.\n\nStep 1: Ingest the Documents into a Vector Database\nAs of v0.10.0, you have the option of storing documents in either a dense vector store (i.e., Chroma) or a sparse vector store (i.e., a built-in keyword search index). Sparse vector stores sacrifice a small amount of inference speed for significant improvements in ingestion speed (useful for larger document sets) and also assume answer sources will include at least one word from the question. To select the store type, supply either store_type=\"dense\" or store_type=\"sparse\" when creating the LLM. As you can see above, we use a sparse vector store here.\n\nllm.ingest(\"./tests/sample_data\")\n\nCreating new vectorstore at /home/amaiya/onprem_data/vectordb/sparse\nLoading documents from ./tests/sample_data\n\n\nLoading new documents: 100%|██████████████████████| 6/6 [00:09&lt;00:00,  1.51s/it]\nProcessing and chunking 43 new documents: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 116.11it/s]\n\n\nSplit into 354 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:00&lt;00:00, 2548.70it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\nThe default chunk_size is set quite low at 1000 characters. You increase by supplying chunk_size to llm.ingest. You can customize the ingestion process even further by accessing the underlying vector store directly, as illustrated in the advanced RAG example.\n\n\nStep 2: Answer Questions About the Documents\n\nquestion = \"\"\"What is  ktrain?\"\"\"\nresult = llm.ask(question)\n\n ktrain is a low-code machine learning platform. It provides out-of-the-box support for training models on various types of data such as text, vision, graph, and tabular.\n\n\nThe sources used by the model to generate the answer are stored in result['source_documents']. You can adjust the number of sources (i.e., chunks) considered by suppyling the limit parameter to llm.ask.\n\nprint(\"\\nSources:\\n\")\nfor i, document in enumerate(result[\"source_documents\"]):\n    print(f\"\\n{i+1}.&gt; \" + document.metadata[\"source\"] + \":\")\n    print(document.page_content)\n\n\nSources:\n\n\n1.&gt; /home/amaiya/projects/ghub/onprem/nbs/tests/sample_data/ktrain_paper/ktrain_paper.pdf:\ntransferred to, and executed on new data in a production environment.\nktrain is a Python library for machine learning with the goal of presenting a simple,\nuniﬁed interface to easily perform the above steps regardless of the type of data (e.g., text\nvs. images vs. graphs). Moreover, each of the three steps above can be accomplished in\n©2022 Arun S. Maiya.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are\n\n2.&gt; /home/amaiya/projects/ghub/onprem/nbs/tests/sample_data/ktrain_paper/ktrain_paper.pdf:\ncustom models and data formats, as well. Inspired by other low-code (and no-code) open-\nsource ML libraries such as fastai (Howard and Gugger, 2020) and ludwig (Molino et al.,\n2019), ktrain is intended to help further democratize machine learning by enabling begin-\nners and domain experts with minimal programming or data science experience to build\nsophisticated machine learning models with minimal coding. It is also a useful toolbox for\n\n3.&gt; /home/amaiya/projects/ghub/onprem/nbs/tests/sample_data/ktrain_paper/ktrain_paper.pdf:\nApache license, and available on GitHub at: https://github.com/amaiya/ktrain.\n2. Building Models\nSupervised learning tasks in ktrain follow a standard, easy-to-use template.\nSTEP 1: Load and Preprocess Data. This step involves loading data from diﬀerent\nsources and preprocessing it in a way that is expected by the model. In the case of text,\nthis may involve language-speciﬁc preprocessing (e.g., tokenization). In the case of images,\n\n4.&gt; /home/amaiya/projects/ghub/onprem/nbs/tests/sample_data/ktrain_paper/ktrain_paper.pdf:\nAutoKeras (Jin et al., 2019) and AutoGluon (Erickson et al., 2020) lack some key “pre-\ncanned” features in ktrain, which has the strongest support for natural language processing\nand graph-based data. Support for additional features is planned for the future.\n5. Conclusion\nThis work presented ktrain, a low-code platform for machine learning. ktrain currently in-\ncludes out-of-the-box support for training models on text, vision, graph, and tabular\n\n\n\n\n\nExtract Text from Documents\nThe load_single_document function can extract text from a range of different document formats (e.g., PDFs, Microsoft PowerPoint, Microsoft Word, etc.). It is automatically invoked when calling LLM.ingest. Extracted text is represented as LangChain Document objects, where Document.page_content stores the extracted text and Document.metadata stores any extracted document metadata.\nFor PDFs, in particular, a number of different options are available depending on your use case.\nFast PDF Extraction (default)\n\nPro: Fast\nCon: Does not infer/retain structure of tables in PDF documents\n\n\nfrom onprem.ingest import load_single_document\n\ndocs = load_single_document('tests/sample_data/ktrain_paper/ktrain_paper.pdf')\ndocs[0].metadata\n\n{'source': '/home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf',\n 'file_path': '/home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf',\n 'page': 0,\n 'total_pages': 9,\n 'format': 'PDF 1.4',\n 'title': '',\n 'author': '',\n 'subject': '',\n 'keywords': '',\n 'creator': 'LaTeX with hyperref',\n 'producer': 'dvips + GPL Ghostscript GIT PRERELEASE 9.22',\n 'creationDate': \"D:20220406214054-04'00'\",\n 'modDate': \"D:20220406214054-04'00'\",\n 'trapped': ''}\n\n\nAutomatic OCR of PDFs\n\nPro: Automatically extracts text from scanned PDFs\nCon: Slow\n\nThe load_single_document function will automatically OCR PDFs that require it (i.e., PDFs that are scanned hard-copies of documents). If a document is OCR’ed during extraction, the metadata['ocr'] field will be populated with True.\n\ndocs = load_single_document('tests/sample_data/ocr_document/lynn1975.pdf')\ndocs[0].metadata\n\n{'source': '/home/amaiya/projects/ghub/onprem/nbs/sample_data/4/lynn1975.pdf',\n 'ocr': True}\n\n\nMarkdown Conversion in PDFs\n\nPro: Better chunking for QA\nCon: Slower than default PDF extraction\n\nThe load_single_document function can convert PDFs to Markdown instead of plain text by supplying the pdf_markdown=True as an argument:\ndocs = load_single_document('your_pdf_document.pdf', \n                            pdf_markdown=True)\nConverting to Markdown can facilitate downstream tasks like question-answering. For instance, when supplying pdf_markdown=True to LLM.ingest, documents are chunked in a Markdown-aware fashion (e.g., the abstract of a research paper tends to be kept together into a single chunk instead of being split up). Note that Markdown will not be extracted if the document requires OCR.\nInferring Table Structure in PDFs\n\nPro: Makes it easier for LLMs to analyze information in tables\nCon: Slower than default PDF extraction\n\nWhen supplying infer_table_structure=True to either load_single_document or LLM.ingest, tables are inferred and extracted from PDFs using a TableTransformer model. Tables are represented as Markdown (or HTML if Markdown conversion is not possible).\ndocs = load_single_document('your_pdf_document.pdf', \n                            infer_table_structure=True)\nParsing Extracted Text Into Sentences or Paragraphs\nFor some analyses (e.g., using prompts for information extraction), it may be useful to parse the text extracted from documents into individual sentences or paragraphs. This can be accomplished using the segment function:\n\nfrom onprem.ingest import load_single_document\nfrom onprem.utils import segment\ntext = load_single_document('tests/sample_data/sotu/state_of_the_union.txt')[0].page_content\n\n\nsegment(text, unit='paragraph')[0]\n\n'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.  Members of Congress and the Cabinet.  Justices of the Supreme Court.  My fellow Americans.'\n\n\n\nsegment(text, unit='sentence')[0]\n\n'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.'\n\n\n\n\nSummarization Pipeline\nSummarize your raw documents (e.g., PDFs, MS Word) with an LLM.\n\nMap-Reduce Summarization\nSummarize each chunk in a document and then generate a single summary from the individual summaries.\n\nfrom onprem import LLM\nllm = LLM(n_gpu_layers=-1, verbose=False, mute_stream=True) # disabling viewing of intermediate summarization prompts/inferences\n\n\nfrom onprem.pipelines import Summarizer\nsumm = Summarizer(llm)\n\nresp = summ.summarize('tests/sample_data/ktrain_paper/ktrain_paper.pdf', max_chunks_to_use=5) # omit max_chunks_to_use parameter to consider entire document\nprint(resp['output_text'])\n\n Ktrain is an open-source machine learning library that offers a unified interface for various machine learning tasks. The library supports both supervised and non-supervised machine learning, and includes methods for training models, evaluating models, making predictions on new data, and providing explanations for model decisions. Additionally, the library integrates with various explainable AI libraries such as shap, eli5 with lime, and others to provide more interpretable models.\n\n\n\n\nConcept-Focused Summarization\nSummarize a large document with respect to a particular concept of interest.\n\nfrom onprem import LLM\nfrom onprem.pipelines import Summarizer\n\n\nllm = LLM(default_model='zephyr', n_gpu_layers=-1, verbose=False, temperature=0)\nsumm = Summarizer(llm)\nsummary, sources = summ.summarize_by_concept('tests/sample_data/ktrain_paper/ktrain_paper.pdf', concept_description=\"question answering\")\n\n\nThe context provided describes the implementation of an open-domain question-answering system using ktrain, a low-code library for augmented machine learning. The system follows three main steps: indexing documents into a search engine, locating documents containing words in the question, and extracting candidate answers from those documents using a BERT model pretrained on the SQuAD dataset. Confidence scores are used to sort and prune candidate answers before returning results. The entire workflow can be implemented with only three lines of code using ktrain's SimpleQA module. This system allows for the submission of natural language questions and receives exact answers, as demonstrated in the provided example. Overall, the context highlights the ease and accessibility of building sophisticated machine learning models, including open-domain question-answering systems, through ktrain's low-code interface.\n\n\n\n\n\nInformation Extraction Pipeline\nExtract information from raw documents (e.g., PDFs, MS Word documents) with an LLM.\n\nfrom onprem import LLM\nfrom onprem.pipelines import Extractor\n# Notice that we're using a cloud-based, off-premises model here! See \"OpenAI\" section below.\nllm = LLM(model_url='openai://gpt-3.5-turbo', verbose=False, mute_stream=True, temperature=0) \nextractor = Extractor(llm)\nprompt = \"\"\"Extract the names of research institutions (e.g., universities, research labs, corporations, etc.) \nfrom the following sentence delimited by three backticks. If there are no organizations, return NA.  \nIf there are multiple organizations, separate them with commas.\n```{text}```\n\"\"\"\ndf = extractor.apply(prompt, fpath='tests/sample_data/ktrain_paper/ktrain_paper.pdf', pdf_pages=[1], stop=['\\n'])\ndf.loc[df['Extractions'] != 'NA'].Extractions[0]\n\n/home/amaiya/projects/ghub/onprem/onprem/core.py:159: UserWarning: The model you supplied is gpt-3.5-turbo, an external service (i.e., not on-premises). Use with caution, as your data and prompts will be sent externally.\n  warnings.warn(f'The model you supplied is {self.model_name}, an external service (i.e., not on-premises). '+\\\n\n\n'Institute for Defense Analyses'\n\n\n\n\nFew-Shot Classification\nMake accurate text classification predictions using only a tiny number of labeled examples.\n# create classifier\nfrom onprem.pipelines import FewShotClassifier\nclf = FewShotClassifier(use_smaller=True)\n\n# Fetching data\nfrom sklearn.datasets import fetch_20newsgroups\nimport pandas as pd\nimport numpy as np\nclasses = [\"soc.religion.christian\", \"sci.space\"]\nnewsgroups = fetch_20newsgroups(subset=\"all\", categories=classes)\ncorpus, group_labels = np.array(newsgroups.data), np.array(newsgroups.target_names)[newsgroups.target]\n\n# Wrangling data into a dataframe and selecting training examples\ndata = pd.DataFrame({\"text\": corpus, \"label\": group_labels})\ntrain_df = data.groupby(\"label\").sample(5)\ntest_df = data.drop(index=train_df.index)\n\n# X_sample only contains 5 examples of each class!\nX_sample, y_sample = train_df['text'].values, train_df['label'].values\n\n# test set\nX_test, y_test = test_df['text'].values, test_df['label'].values\n\n# train\nclf.train(X_sample,  y_sample, max_steps=20)\n\n# evaluate\nprint(clf.evaluate(X_test, y_test, print_report=False)['accuracy'])\n#output: 0.98\n\n# make predictions\nclf.predict(['Elon Musk likes launching satellites.']).tolist()[0]\n#output: sci.space\nTIP: You can also easily train a wide range of traditional text classification models using both Hugging Face transformers and scikit-learn as backends.\n\n\nUsing Hugging Face Transformers Instead of Llama.cpp\nBy default, the LLM backend employed by OnPrem.LLM is llama-cpp-python, which requires models in GGUF format. As of v0.5.0, it is now possible to use Hugging Face transformers as the LLM backend instead. This is accomplished by using the model_id parameter (instead of supplying a model_url argument). In the example below, we run the Llama-3.1-8B model.\n# llama-cpp-python does NOT need to be installed when using model_id parameter\nllm = LLM(model_id=\"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\", device_map='cuda')\nThis allows you to more easily use any model on the Hugging Face hub in SafeTensors format provided it can be loaded with the Hugging Face transformers.pipeline. Note that, when using the model_id parameter, the prompt_template is set automatically by transformers.\nThe Llama-3.1 model loaded above was quantized using AWQ, which allows the model to fit onto smaller GPUs (e.g., laptop GPUs with 6GB of VRAM) similar to the default GGUF format. AWQ models will require the autoawq package to be installed: pip install autoawq (AWQ only supports Linux system, including Windows Subsystem for Linux). If you do need to load a model that is not quantized, you can supply a quantization configuration at load time (known as “inflight quantization”). In the following example, we load an unquantized Zephyr-7B-beta model that will be quantized during loading to fit on GPUs with as little as 6GB of VRAM:\nfrom transformers import BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_use_double_quant=True,\n)\nllm = LLM(model_id=\"HuggingFaceH4/zephyr-7b-beta\", device_map='cuda', \n          model_kwargs={\"quantization_config\":quantization_config})\nWhen supplying a quantization_config, the bitsandbytes library, a lightweight Python wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and 8 & 4-bit quantization functions, is used. There are ongoing efforts by the bitsandbytes team to support multiple backends in addition to CUDA. If you receive errors related to bitsandbytes, please refer to the bitsandbytes documentation.\n\n\nStructured and Guided Outputs\nLLMs do not always listen to instructions properly. Structured outputs for LLMs are a feature ensuring model responses follow a strict, user-defined format (like JSON or XML schema) instead of free-form text, making outputs predictable, machine-readable, and easily integrable into applications.\n\nNatively Supported Structured Outputs\nA number of LLM services (e.g., vLLM, OpenAI, Anthropic Claude, AWS GovCloud Bedrock) include native support for producing structured outputs. To take advantage of this capability when it exists, you can supply a Pydantic model representing the desired output format to the response_format parameter ofLLM.prompt.\nStructured outputs for LLMs are a feature ensuring model responses follow a strict, user-defined format (like JSON or XML schema) instead of free-form text, making outputs predictable, machine-readable, and easily integrable into applications.\nAnthropic or OpenAI\nfrom onprem import LLM\nfrom pydantic import BaseModel\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    plan_interest: str\n    demo_requested: bool\n\n# Create LLM instance for Claude\nllm = LLM(\"anthropic/claude-3-7-sonnet-latest\")\n\n# Use structured output - this should automatically use Claude's native API\nresult = llm.prompt(\n    \"Extract info from: John Smith (john@example.com) is interested in our Enterprise plan and wants to schedule a demo for next Tuesday  at 2pm.\",\n      response_format=ContactInfo\n  )\n\nprint(f\"Name: {result.name}\")\nprint(f\"Email: {result.email}\")\nprint(f\"Plan: {result.plan_interest}\")\nprint(f\"Demo: {result.demo_requested}\")\nThe above approach using the response_format parameter works with both Anthropic and OpenAI as LLM backends.\nAWS GovCloud Bedrock\nA structured output example using AWS GovCloud Bedrock is shown here.\nVLLM\nFor vLLM, you can generate structured outputs using documented extra parameters like extra_body argument as illustrated below:\nfrom onprem import LLM\nllm = LLM(model_url='http://localhost:8666/v1', api_key='test123', model='MyGPT')\n\n# classification-based structured outputs\nresult = llm.prompt('Classify this sentiment: vLLM is wonderful!',\n                     extra_body={\"structured_outputs\": {\"choice\": [\"positive\", \"negative\"]}})\n# OUTPUT: positive\n\n# JSON-based structured outputs\nfrom pydantic import BaseModel, Field\nclass MeasuredQuantity(BaseModel):\n    value: str = Field(description=\"numerical value - number only\")\n    unit: str = Field(description=\"unit of measurement\")\nresponse_format = {\"type\": \"json_schema\",\n                     \"json_schema\": {\n                     \"name\": MeasuredQuantity.__name__.lower(),\n                      \"schema\": MeasuredQuantity.model_json_schema()}}\nresult = llm.prompt('Extract unit and value from the following: He was going 35 mph.',                                                                                       response_format=response_format)\n# OUTPUT: { \"value\": \"35\", \"unit\": \"mph\" }\n\n# RegEx-based strucured outputs\nresult = llm.prompt(\n    \"Generate an example email address for Alan Turing, who works in Enigma. End in \"\n    \".com and new line.\",\n    extra_body={\"structured_outputs\": {\"regex\": r\"\\w+@\\w+\\.com\\n\"}, \"stop\": [\"\\n\"]},\n)\n# OUTPUT: Alan_Turing@enigma.com\nOllama\nfrom pydantic import BaseModel\n\nclass Pet(BaseModel):\n  name: str\n  animal: str\n  age: int\n  color: str | None\n  favorite_toy: str | None\n\nclass PetList(BaseModel):\n  pets: list[Pet]\n\nllm = LLM('ollama/llama3.1')\nresult = llm.prompt('I have two cats named Luna and Loki...', format=PetList.model_json_schema())\nWhen using an LLM backend that does not natively support structured outputs, supplying a Pydantic model via the response_format parameter to LLM.prompt should result in an automatic fall back to a prompt-based approach to structured outputs as described next.\nTip: When using natively-supported structured outputs, it is important to include an actual instruction in the prompt (e.g., “Classify this sentiment”, “Extract info from”, etc.). With prompt-based structured outputs (described below), the instruction can often be omitted.\n\n\nPrompt-Based Structured Outputs\nThe LLM.pydantic_prompt method also allows you to specify the desired structure of the LLM’s output as a Pydantic model. Internally, LLM.pydantic_prompt wraps the user-supplied prompt within a larger prompt telling the LLM to output results in a specific JSON format. It is sometimes less efficient/reliable than aforementioned native methods, but is more generally applicable to any LLM. Since calling LLM.prompt with the response_format parameter will automatically invoke LLM.pydantic_prompt when necessary, you will typically not have to call LLM.pydantic_prompt directly.\n\nfrom pydantic import BaseModel, Field\n\nclass Joke(BaseModel):\n    setup: str = Field(description=\"question to set up a joke\")\n    punchline: str = Field(description=\"answer to resolve the joke\")\n\nfrom onprem import LLM\nllm = LLM(default_model='llama', verbose=False)\nstructured_output = llm.pydantic_prompt('Tell me a joke.', pydantic_model=Joke)\n\nllama_new_context_with_model: n_ctx_per_seq (3904) &lt; n_ctx_train (131072) -- the full capacity of the model will not be utilized\n\n\n{\n  \"setup\": \"Why couldn't the bicycle stand alone?\",\n  \"punchline\": \"Because it was two-tired!\"\n}\n\n\nThe output is a Pydantic object instead of a string:\n\nstructured_output\n\nJoke(setup=\"Why couldn't the bicycle stand alone?\", punchline='Because it was two-tired!')\n\n\n\nprint(structured_output.setup)\nprint()\nprint(structured_output.punchline)\n\nWhy couldn't the bicycle stand alone?\n\nBecause it was two-tired!\n\n\nYou can also use OnPrem.LLM with the Guidance package to guide the LLM to generate outputs based on your conditions and constraints. We’ll show a couple of examples here, but see our documentation on guided prompts for more information.\n\nfrom onprem import LLM\n\nllm = LLM(n_gpu_layers=-1, verbose=False)\nfrom onprem.pipelines.guider import Guider\nguider = Guider(llm)\n\nWith the Guider, you can use use Regular Expressions to control LLM generation:\n\nprompt = f\"\"\"Question: Luke has ten balls. He gives three to his brother. How many balls does he have left?\nAnswer: \"\"\" + gen(name='answer', regex='\\d+')\n\nguider.prompt(prompt, echo=False)\n\n{'answer': '7'}\n\n\n\nprompt = '19, 18,' + gen(name='output', max_tokens=50, stop_regex='[^\\d]7[^\\d]')\nguider.prompt(prompt)\n\n19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8,\n\n\n{'output': ' 17, 16, 15, 14, 13, 12, 11, 10, 9, 8,'}\n\n\nSee the documentation for more examples of how to use Guidance with OnPrem.LLM.\n\n\n\nSolving Tasks With Agents\nfrom onprem import LLM\nfrom onprem.pipelines import Agent\nllm = LLM('openai/gpt-4o-mini', mute_stream=True) \nagent = Agent(llm)\nagent.add_webview_tool()\nanswer = agent.run(\"What is the highest level of education of the person listed on this page: https://arun.maiya.net?\")\n# ANSWER: Ph.D. in Computer Science\nSee the example notebook on agents for more information",
    "crumbs": [
      "OnPrem.LLM"
    ]
  },
  {
    "objectID": "index.html#built-in-web-app",
    "href": "index.html#built-in-web-app",
    "title": "OnPrem.LLM",
    "section": "Built-In Web App",
    "text": "Built-In Web App\nOnPrem.LLM includes a built-in Web app to access the LLM. To start it, run the following command after installation:\nonprem --port 8000\nThen, enter localhost:8000 (or &lt;domain_name&gt;:8000 if running on remote server) in a Web browser to access the application:\n\nFor more information, see the corresponding documentation.",
    "crumbs": [
      "OnPrem.LLM"
    ]
  },
  {
    "objectID": "index.html#examples",
    "href": "index.html#examples",
    "title": "OnPrem.LLM",
    "section": "Examples",
    "text": "Examples\nThe documentation includes many examples, including:\n\nPrompts for Problem-Solving\nRAG Example\nCode Generation\nSemantic Similarity\nDocument Summarization\nInformation Extraction\nText Classification\nAgent-Based Task Execution\nAudo-Coding Survey Responses\nLegal and Regulatory Analysis",
    "crumbs": [
      "OnPrem.LLM"
    ]
  },
  {
    "objectID": "index.html#faq",
    "href": "index.html#faq",
    "title": "OnPrem.LLM",
    "section": "FAQ",
    "text": "FAQ\n\nHow do I use other models with OnPrem.LLM?\n\nYou can supply any model of your choice using the model_url and model_id parameters to LLM (see cheat sheet above).\n\n\nHere, we will go into detail on how to supply a custom GGUF model using the llma.cpp backend.\n\n\nYou can find llama.cpp-supported models with GGUF in the file name on huggingface.co.\n\n\nMake sure you are pointing to the URL of the actual GGUF model file, which is the “download” link on the model’s page. An example for Mistral-7B is shown below:\n\n\n\n\n\nWhen using the llama.cpp backend, GGUF models have specific prompt formats that need to supplied to LLM. For instance, the prompt template required for Zephyr-7B, as described on the model’s page, is:\n&lt;|system|&gt;\\n&lt;/s&gt;\\n&lt;|user|&gt;\\n{prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\nSo, to use the Zephyr-7B model, you must supply the prompt_template argument to the LLM constructor (or specify it in the webapp.yml configuration for the Web app).\n# how to use Zephyr-7B with OnPrem.LLM\nllm = LLM(model_url='https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf',\n          prompt_template = \"&lt;|system|&gt;\\n&lt;/s&gt;\\n&lt;|user|&gt;\\n{prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\",\n          n_gpu_layers=33)\nllm.prompt(\"List three cute names for a cat.\")\n\n\nPrompt templates are not required for any other LLM backend (e.g., when using Ollama as backend or when using model_id parameter for transformers models). Prompt templates are also not required if using any of the default models.\n\nWhen installing onprem, I’m getting “build” errors related to llama-cpp-python (or chroma-hnswlib) on Windows/Mac/Linux?\n\nSee this LangChain documentation on LLama.cpp for help on installing the llama-cpp-python package for your system. Additional tips for different operating systems are shown below:\n\n\nFor Linux systems like Ubuntu, try this: sudo apt-get install build-essential g++ clang. Other tips are here.\n\n\nFor Windows systems, please try following these instructions. We recommend you use Windows Subsystem for Linux (WSL) instead of using Microsoft Windows directly. If you do need to use Microsoft Window directly, be sure to install the Microsoft C++ Build Tools and make sure the Desktop development with C++ is selected.\n\n\nFor Macs, try following these tips.\n\n\nThere are also various other tips for each of the above OSes in this privateGPT repo thread. Of course, you can also easily use OnPrem.LLM on Google Colab.\n\n\nFinally, if you still can’t overcome issues with building llama-cpp-python, you can try installing the pre-built wheel file for your system:\n\n\nExample: pip install llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\nTip: There are pre-built wheel files for chroma-hnswlib, as well. If running pip install onprem fails on building chroma-hnswlib, it may be because a pre-built wheel doesn’t yet exist for the version of Python you’re using (in which case you can try downgrading Python).\n\nI’m behind a corporate firewall and am receiving an SSL error when trying to download the model?\n\nTry this:\nfrom onprem import LLM\nLLM.download_model(url, ssl_verify=False)\n\n\nYou can download the embedding model (used by LLM.ingest and LLM.ask) as follows:\nwget --no-check-certificate https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/all-MiniLM-L6-v2.zip\n\n\nSupply the unzipped folder name as the embedding_model_name argument to LLM.\n\n\nIf you’re getting SSL errors when even running pip install, try this:\npip install –-trusted-host pypi.org –-trusted-host files.pythonhosted.org pip_system_certs\n\nHow do I use this on a machine with no internet access?\n\nUse the LLM.download_model method to download the model files to &lt;your_home_directory&gt;/onprem_data and transfer them to the same location on the air-gapped machine.\n\n\nFor the ingest and ask methods, you will need to also download and transfer the embedding model files:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nmodel.save('/some/folder')\n\n\nCopy the some/folder folder to the air-gapped machine and supply the path to LLM via the embedding_model_name parameter.\n\nMy model is not loading when I call llm = LLM(...)?\n\nThis can happen if the model file is corrupt (in which case you should delete from &lt;home directory&gt;/onprem_data and re-download). It can also happen if the version of llama-cpp-python needs to be upgraded to the latest.\n\nI’m getting an “Illegal instruction (core dumped) error when instantiating a langchain.llms.Llamacpp or onprem.LLM object?\n\nYour CPU may not support instructions that cmake is using for one reason or another (e.g., due to Hyper-V in VirtualBox settings). You can try turning them off when building and installing llama-cpp-python:\n\n\n# example\nCMAKE_ARGS=\"-DGGML_CUDA=ON -DGGML_AVX2=OFF -DGGML_AVX=OFF -DGGML_F16C=OFF -DGGML_FMA=OFF\" FORCE_CMAKE=1 pip install --force-reinstall llama-cpp-python --no-cache-dir\n\nHow can I speed up LLM.ingest?\n\nBy default, a GPU, if available, will be used to compute embeddings, so ensure PyTorch is installed with GPU support. You can explicitly control the device used for computing embeddings with the embedding_model_kwargs argument.\nfrom onprem import LLM\nllm  = LLM(embedding_model_kwargs={'device':'cuda'})\n\n\nYou can also supply store_type=\"sparse\" to LLM to use a sparse vector store, which sacrifices a small amount of inference speed (LLM.ask) for significant speed ups during ingestion (LLM.ingest).\nfrom onprem import LLM\nllm  = LLM(store_type=\"sparse\")\nNote, however, that, unlike dense vector stores, sparse vector stores assume answer sources will contain at least one word in common with the question.",
    "crumbs": [
      "OnPrem.LLM"
    ]
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "OnPrem.LLM",
    "section": "How to Cite",
    "text": "How to Cite\nPlease cite the following paper when using OnPrem.LLM:\n@article{maiya2025generativeaiffrdcs,\n      title={Generative AI for FFRDCs}, \n      author={Arun S. Maiya},\n      year={2025},\n      eprint={2509.21040},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2509.21040}, \n}",
    "crumbs": [
      "OnPrem.LLM"
    ]
  },
  {
    "objectID": "sk.clf.html",
    "href": "sk.clf.html",
    "title": "sk.clf",
    "section": "",
    "text": "source\n\nNBSVM\n\ndef NBSVM(\n    alpha:float=0.75, C:float=0.01, beta:float=0.25, fit_intercept:bool=False\n):\n\nBase class for all estimators in scikit-learn.\nInheriting from this class provides default implementations of:\n\nsetting and getting parameters used by GridSearchCV and friends;\ntextual and HTML representation displayed in terminals and IDEs;\nestimator serialization;\nparameters validation;\ndata validation;\nfeature names validation.\n\nRead more in the :ref:User Guide &lt;rolling_your_own_estimator&gt;.\n\nsource\n\n\nClassifier\n\ndef Classifier(\n    model:NoneType=None\n):\n\ninstantiate a classifier with an optional previously-saved model"
  },
  {
    "objectID": "sk.base.html",
    "href": "sk.base.html",
    "title": "sk.base",
    "section": "",
    "text": "source\n\nget_random_colors\n\ndef get_random_colors(\n    n, name:str='hsv', hex_format:bool=True\n):\n\nReturns a function that maps each index in 0, 1, …, n-1 to a distinct RGB color; the keyword argument name must be a standard mpl colormap name.\n\nsource\n\n\nsplit_chinese\n\ndef split_chinese(\n    texts\n):\n\n\nsource\n\n\nis_chinese\n\ndef is_chinese(\n    lang, strict:bool=True\n):\n\n** Args:   lang(str): language code (e.g., en)   strict(bool):  If False, include additional languages due to mistakes on short texts by langdetect\n\nsource\n\n\nis_nospace_lang\n\ndef is_nospace_lang(\n    lang\n):\n\n\nsource\n\n\ndetect_lang\n\ndef detect_lang(\n    texts:list, sample_size:int=32\n):\n\ndetect language of texts\n\nsource\n\n\ndecode_by_line\n\ndef decode_by_line(\n    texts, encoding:str='utf-8', verbose:int=1\n):\n\n** Decode text line by line and skip over errors.\n\nsource\n\n\ndetect_encoding\n\ndef detect_encoding(\n    texts, sample_size:int=32\n):"
  },
  {
    "objectID": "hf.models.tokendetection.html",
    "href": "hf.models.tokendetection.html",
    "title": "hf.models.tokendetection",
    "section": "",
    "text": "source\n\nTokenDetection\n\ndef TokenDetection(\n    generator, discriminator, tokenizer, weight:float=50.0\n):\n\nRuns the replaced token detection training objective. This method was first proposed by the ELECTRA model. The method consists of a masked language model generator feeding data to a discriminator that determines which of the tokens are incorrect. More on this training objective can be found in the ELECTRA paper."
  },
  {
    "objectID": "examples_openai.html",
    "href": "examples_openai.html",
    "title": "Using OpenAI Models",
    "section": "",
    "text": "Even when using on-premises language models that run locally on your machine, it can sometimes be useful to have easy access to cloud-based models (e.g., OpenAI) for experimentation, baselines for comparison, generating synthetic data, etc. For these reasons, in spite of the name, OnPrem.LLM now includes support for OpenAI chat models.\nfrom onprem import LLM\nllm = LLM(model_url='openai://gpt-3.5-turbo', temperature=0, mute_stream=True)\n\n/home/amaiya/projects/ghub/onprem/onprem/core.py:139: UserWarning: The model you supplied is gpt-3.5-turbo, an external service (i.e., not on-premises). Use with caution, as your data and prompts will be sent externally.\n  warnings.warn(f'The model you supplied is {self.model_name}, an external service (i.e., not on-premises). '+\\",
    "crumbs": [
      "Examples",
      "Using OpenAI Models"
    ]
  },
  {
    "objectID": "examples_openai.html#general-prompting",
    "href": "examples_openai.html#general-prompting",
    "title": "Using OpenAI Models",
    "section": "General Prompting",
    "text": "General Prompting\n\nres = llm.prompt('I am an accountant, and I have to write a short resignation letter to my supervisor. '\n                 'Write a draft of this letter using at most 5 sentences.')\nprint(res)\n\nDear [Supervisor's Name],\n\nI hope this letter finds you well. I am writing to inform you of my decision to resign from my position as an accountant at [Company Name], effective [last working day, typically two weeks from the date of the letter]. I have thoroughly enjoyed my time working here and appreciate the opportunities for professional growth that I have been given. However, after careful consideration, I have decided to pursue a new opportunity that aligns more closely with my long-term career goals. I am committed to ensuring a smooth transition and will be available to assist with any necessary handover tasks. Thank you for your understanding.\n\nSincerely,\n[Your Name]",
    "crumbs": [
      "Examples",
      "Using OpenAI Models"
    ]
  },
  {
    "objectID": "examples_openai.html#summarizing-a-paper",
    "href": "examples_openai.html#summarizing-a-paper",
    "title": "Using OpenAI Models",
    "section": "Summarizing a Paper",
    "text": "Summarizing a Paper\n\nimport os\nos.makedirs('/tmp/somepaper', exist_ok=True)\n!wget --user-agent=\"Mozilla\" https://arxiv.org/pdf/2004.10703.pdf -O /tmp/somepaper/paper.pdf -q\n\n\nfrom onprem import pipelines\nsummarizer = pipelines.Summarizer(llm)\ntext = summarizer.summarize('/tmp/somepaper/paper.pdf', max_chunks_to_use=5)\nprint(text['output_text'])\n\nktrain is a low-code Python library that serves as a wrapper to TensorFlow and other libraries, simplifying the machine learning workflow for both beginners and experienced practitioners. It supports various data types and tasks such as text, vision, graph, and tabular data analysis. The library automates and streamlines processes like model building, inspection, and application. It offers features like text classification, regression, sequence tagging, topic modeling, document similarity, recommendation, summarization, and question-answering. ktrain provides options for choosing different models or using custom models and includes explainable AI features. It is open-source and available on GitHub.",
    "crumbs": [
      "Examples",
      "Using OpenAI Models"
    ]
  },
  {
    "objectID": "examples_openai.html#answer-questions-about-a-paper",
    "href": "examples_openai.html#answer-questions-about-a-paper",
    "title": "Using OpenAI Models",
    "section": "Answer Questions About a Paper",
    "text": "Answer Questions About a Paper\n\nllm.ingest('/tmp/somepaper')\n\nCreating new vectorstore at /home/amaiya/onprem_data/vectordb\nLoading documents from /tmp/somepaper\n\n\nLoading new documents: 100%|██████████████████████| 1/1 [00:00&lt;00:00, 16.04it/s]\n\n\nLoaded 9 new documents from /tmp/somepaper\nSplit into 57 chunks of text (max. 500 chars each)\nCreating embeddings. May take some minutes...\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01&lt;00:00,  1.53s/it]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n\n\n\n\nres = llm.ask('What is said about image classification?')\nprint(res['answer'])\n\nThe example provided demonstrates building an image classifier using a standard ResNet50 model pretrained on ImageNet. The steps for image classification are similar to the previous text classification example, despite the tasks being completely different.",
    "crumbs": [
      "Examples",
      "Using OpenAI Models"
    ]
  },
  {
    "objectID": "examples_openai.html#gpt-4o-vision-capabilities",
    "href": "examples_openai.html#gpt-4o-vision-capabilities",
    "title": "Using OpenAI Models",
    "section": "GPT-4o Vision Capabilities",
    "text": "GPT-4o Vision Capabilities\n\nfrom onprem import LLM\n\nllm = LLM(model_url='openai://gpt-4o', temperature=0)\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\nsaved_result = llm.prompt('Describe the weather in this image.', image_path_or_url=image_url)\n\n/home/amaiya/projects/ghub/onprem/onprem/core.py:196: UserWarning: The model you supplied is gpt-4o, an external service (i.e., not on-premises). Use with caution, as your data and prompts will be sent externally.\n  warnings.warn(f'The model you supplied is {self.model_name}, an external service (i.e., not on-premises). '+\\\n\n\nThe weather in the image appears to be clear and sunny. The sky is mostly blue with some scattered clouds, suggesting a pleasant day with good visibility. The sunlight is bright, casting clear shadows and illuminating the green landscape.\n\n\nYou can also supply OpenAI-style message dicitionaries:\n\nmessages = [\n    {'content': [{'text': 'Describe the weather in this image.', \n                  'type': 'text'},\n                 {'image_url': {'url': image_url},\n                  'type': 'image_url'}],\n     'role': 'user'}]\nsaved_result = llm.prompt(messages)\n\nThe weather in the image appears to be clear and sunny. The sky is mostly blue with some scattered clouds, suggesting a pleasant day with good visibility. The sunlight is bright, casting clear shadows and illuminating the green landscape.",
    "crumbs": [
      "Examples",
      "Using OpenAI Models"
    ]
  },
  {
    "objectID": "llm.helpers.html",
    "href": "llm.helpers.html",
    "title": "llm helpers",
    "section": "",
    "text": "source",
    "crumbs": [
      "Source",
      "llm helpers"
    ]
  },
  {
    "objectID": "llm.helpers.html#examples",
    "href": "llm.helpers.html#examples",
    "title": "llm helpers",
    "section": "Examples",
    "text": "Examples\n\nfrom onprem import LLM\n\n\nllm = LLM(default_model='llama', n_gpu_layers=-1, verbose=False, mute_stream=True)\n\nllama_new_context_with_model: n_ctx_per_seq (3904) &lt; n_ctx_train (131072) -- the full capacity of the model will not be utilized\n\n\n\nExtract Titles\n\nfrom onprem.ingest import load_single_document\n\n\ndocs = load_single_document('tests/sample_data/ktrain_paper/ktrain_paper.pdf')\ntitle = extract_title(docs, llm=llm)\nprint(title)\n\nA Low-Code Library for Augmented Machine Learning\n\n\n\n\nAuto-Caption Tables\n\ndocs = load_single_document('tests/sample_data/ktrain_paper/ktrain_paper.pdf', infer_table_structure=True)\ntable_doc = [d for d in docs if d.metadata['table']][0]\n\n\nsummarize_tables([table_doc], llm, only_caption_missing=False)\n\n\nprint(table_doc.page_content)\n\nComparison of ML Tasks Supported in Popular Libraries\n\nTable 1: A comparison of ML tasks supported out-of-the-box in popular low-code and AutoML libraries for tabular, image, audio, text and graph data.\n\nThe following table in markdown format has the caption: Table 1: A comparison of ML tasks supported out-of-the-box in popular low-code and AutoML libraries for tabular, image, audio, text and graph data. The following table in markdown format includes this list of columns:\n- Task\n- ktrain\n- fastai\n- Ludwig\n- AutoKeras\n- AutoGluon\n\n|Task|ktrain|fastai|Ludwig|AutoKeras|AutoGluon|\n|---|---|---|---|---|---|\n|Tabular: Classification/Regression|✓|✓|✓|✓|✓|\n|Tabular: Causal Machine Learning|✓|None|None|None|None|\n|Tabular: Time Series Forecasting|None|None|✓|✓|None|\n|Tabular: Collaborative Filtering|None|✓|None|None|None|\n|Image: Classification/Regression|✓|✓|✓|✓|✓|\n|Image: Object Detection|prefitted*|✓|None|None|✓|\n|Image: Image Captioning|prefitted*|None|✓|None|None|\n|Image: Segmentation|None|✓|None|None|None|\n|Image: GANs|None|✓|None|None|None|\n|Image: Keypoint/Pose Estimation|None|✓|None|None|None|\n|Audio: Classification/Regression|None|None|✓|None|None|\n|Audio: Speech Transcription|prefitted*|None|✓|None|None|\n|Text: Classification/Regression|✓|✓|✓|✓|✓|\n|Text: Sequence-Tagging|✓|None|✓|None|None|\n|Text: Unsupervised Topic Modeling|✓|None|None|None|None|\n|Text: Semantic Search|✓|None|None|None|None|\n|Text: End-to-End Question-Answering|✓*|None|None|None|None|\n|Text: Zero-Shot Learning|✓|None|None|None|None|\n|Text: Language Translation|prefitted*|None|✓|None|None|\n|Text: Summarization|prefitted*|None|✓|None|None|\n|Text: Text Extraction|✓|None|None|None|None|\n|Text: QA-Based Information Extraction|✓*|None|None|None|None|\n|Text: Keyphrase Extraction|✓|None|None|None|None|\n|Graph: Node Classification|✓|None|None|None|None|\n|Graph: Link Prediction|✓|None|None|None|None|\n\n\n\nThe caption_tables function pre-pended the table text with an alternative caption in this example. You can skip over tables that already have captions by supplying only_caption_missing=True.",
    "crumbs": [
      "Source",
      "llm helpers"
    ]
  },
  {
    "objectID": "ingest.stores.dense.html",
    "href": "ingest.stores.dense.html",
    "title": "ingest.stores.dense",
    "section": "",
    "text": "source\n\nDenseStore\n\ndef DenseStore(\n    kwargs:VAR_KEYWORD\n):\n\nA factory for built-in DenseStore instances.\n\nsource\n\n\nDenseStore.create\n\ndef create(\n    persist_location:NoneType=None, kind:NoneType=None, kwargs:VAR_KEYWORD\n)-&gt;DenseStore:\n\nFactory method to construct a DenseStore instance. \nExtra kwargs passed to object instantiation.\nArgs: persist_location: where the vector database is stored kind: one of {chroma, elasticsearch}\nReturns: DenseStore instance\n\nsource\n\n\nElasticsearchDenseStore\n\ndef ElasticsearchDenseStore(\n    dense_vector_field:str='dense_vector', kwargs:VAR_KEYWORD\n):\n\nElasticsearch store with dense vector search capabilities. Extends DenseStore to provide Elasticsearch-based dense vector storage.\n\nsource\n\n\nChromaStore\n\ndef ChromaStore(\n    persist_location:Optional=None, kwargs:VAR_KEYWORD\n):\n\nA dense vector store based on Chroma.\n\nsource\n\n\nChromaStore.exists\n\ndef exists(\n    \n):\n\nReturns True if vector store has been initialized and contains documents.\n\nsource\n\n\nChromaStore.add_documents\n\ndef add_documents(\n    documents, batch_size:int=41000\n):\n\nStores instances of langchain_core.documents.base.Document in vectordb\n\nsource\n\n\nChromaStore.remove_document\n\ndef remove_document(\n    id_to_delete\n):\n\nRemove a single document with ID, id_to_delete.\n\nsource\n\n\nChromaStore.remove_source\n\ndef remove_source(\n    source:str\n):\n\nDeletes all documents in a Chroma collection whose source metadata field starts with the given prefix. The source argument can either be a full path to a document or a prefix (e.g., parent folder).\nArgs: - source: The source value or prefix\nReturns: - The number of documents deleted\n\nsource\n\n\nChromaStore.update_documents\n\ndef update_documents(\n    doc_dicts:dict, # dictionary with keys 'page_content', 'source', 'id', etc.\n    kwargs:VAR_KEYWORD\n):\n\nUpdate a set of documents (doc in index with same ID will be over-written)\n\nsource\n\n\nChromaStore.get_all_docs\n\ndef get_all_docs(\n    \n):\n\nReturns all docs\n\nsource\n\n\nChromaStore.get_doc\n\ndef get_doc(\n    id\n):\n\nRetrieve a record by ID\n\nsource\n\n\nChromaStore.get_size\n\ndef get_size(\n    \n):\n\nGet total number of records\n\nsource\n\n\nChromaStore.erase\n\ndef erase(\n    confirm:bool=True\n):\n\nResets collection and removes and stored documents\n\nsource\n\n\nVectorStore.query\n\ndef query(\n    query:str, kwargs:VAR_KEYWORD\n):\n\nGeneric query method that invokes the store’s search method. This provides a consistent interface across all store types.\n\nsource\n\n\nChromaStore.semantic_search\n\ndef semantic_search(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nPerform a semantic search of the vector DB. Returns results as LangChain Document objects.\n\nsource\n\n\nVectorStore.ingest\n\ndef ingest(\n    source_directory:str, # path to folder containing document store\n    chunk_size:int=1000, # text is split to this many characters by [langchain.text_splitter.RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n    chunk_overlap:int=100, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n    ignore_fn:Optional=None, # Optional function that accepts the file path (including file name) as input and returns `True` if file path should not be ingested.\n    batch_size:int=41000, # batch size used when processing documents\n    kwargs:VAR_KEYWORD\n)-&gt;None:\n\nIngests all documents in source_directory (previously-ingested documents are ignored). When retrieved, the Document objects will each have a metadata dict with the absolute path to the file in metadata[\"source\"]. Extra kwargs fed to ingest.load_single_document.\n\nimport tempfile\n\n\ntemp_dir = tempfile.TemporaryDirectory()\ntempfolder = temp_dir.name\n\n\nstore = DenseStore.create(tempfolder)\nstore.ingest(\"tests/sample_data/ktrain_paper/\")\n\nCreating new vectorstore at /tmp/tmpmftvr854\nLoading documents from tests/sample_data/ktrain_paper/\n\n\nLoading new documents: 100%|██████████████████████| 1/1 [00:00&lt;00:00,  7.85it/s]\nProcessing and chunking 6 new documents: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 985.74it/s]\n\n\nSplit into 41 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\nCreating embeddings. May take some minutes...\n\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  3.01it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n\n\n\n\ntype(store)\n\n__main__.ChromaStore\n\n\n\nstore.get_size()\n\n41\n\n\n\na_document = store.get_all_docs()[0]\n\n\nstore.remove_document(a_document['id'])\n\n\nstore.get_size()\n\n40",
    "crumbs": [
      "Source",
      "ingest.stores.dense"
    ]
  },
  {
    "objectID": "examples_rag.html",
    "href": "examples_rag.html",
    "title": "Document Question-Answering",
    "section": "",
    "text": "This example of OnPrem.LLM demonstrates retrieval augmented generation or RAG.",
    "crumbs": [
      "Examples",
      "Document Question-Answering"
    ]
  },
  {
    "objectID": "examples_rag.html#basic-rag",
    "href": "examples_rag.html#basic-rag",
    "title": "Document Question-Answering",
    "section": "Basic RAG",
    "text": "Basic RAG\n\nSTEP 1: Setup the LLM instance\nIn this first example, we will use a model called Zephyr-7B-beta. When selecting a model, it is important to inspect the model’s home page and identify the correct prompt format. The prompt format for this model is located here, and we will supply it directly to the LLM constructor along with the URL to the specific model file we want (i.e., zephyr-7b-beta.Q4_K_M.gguf). We will offload layers to our GPU(s) to speed up inference using the n_gpu_layers parameter. (For more information on GPU acceleration, see here.) For the purposes of this notebook, we also supply temperature=0 so that there is no variability in outputs. You can increase this value for more creativity in the outputs. Finally, we will choose a non-default location for our vector database.\n\nfrom onprem import LLM, utils as U\nimport tempfile\nfrom textwrap import wrap\n\n\nvectordb_path = tempfile.mkdtemp()\n\nllm = LLM(model_url='https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf', \n          prompt_template= \"&lt;|system|&gt;\\n&lt;/s&gt;\\n&lt;|user|&gt;\\n{prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\",\n          n_gpu_layers=-1,\n          temperature=0,\n          store_type='dense',\n          vectordb_path=vectordb_path,\n         verbose=False)\n\nllama_new_context_with_model: n_ctx_per_seq (3904) &lt; n_ctx_train (32768) -- the full capacity of the model will not be utilized\n\n\nSince OnPrem.LLM includes built-in support for Zephyr, an easier way to instantiate the LLM with Zephyr is as follows:\nllm = LLM(default_model='zephyr', \n          n_gpu_layers=-1,\n          temperature=0,\n          store_type='dense',\n          vectordb_path=vectordb_path)\n\n\nSTEP 2: Ingest Documents\nWhen ingesting documents, they can be stored in one of two ways: 1. a dense vector store: a conventional vector database like Chroma 2. a sparse vector store: a keyword-search engine\nSparse vector stores compute embeddings on-the-fly at inference time. As a result, sparse vector stores sacrifice a small amount of inference speed for significant speed ups in ingestion speed. This makes it better suited for larger document sets. Note that sparse vector stores include the contraint that any passages considered as sources for answers should have at least one word in common with the question being asked. You can specify the kind of vector store by supplying either store_type=\"dense\" or store_type=\"sparse\" when creating the LLM above. We use a dense vector store in this example, as shown above.\nFor this example, we will download the 2024 National Defense Autorization Act (NDAA) report and ingest it.\n\nU.download('https://www.congress.gov/118/crpt/hrpt125/CRPT-118hrpt125.pdf', '/tmp/ndaa/ndaa.pdf', verify=True)\n\n[██████████████████████████████████████████████████]\n\n\n\nllm.ingest(\"/tmp/ndaa/\")\n\nCreating new vectorstore at /tmp/tmpmnt6g6l8/dense\nLoading documents from /tmp/ndaa/\n\n\nLoading new documents: 100%|██████████████████████| 1/1 [00:00&lt;00:00,  1.62it/s]\nProcessing and chunking 672 new documents: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 10.22it/s]\n\n\nSplit into 5202 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\nCreating embeddings. May take some minutes...\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:17&lt;00:00,  2.95s/it]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n\n\n\n\n\nSTEP 3: Asking Questions to Your Documents\n\nresult = llm.ask(\"What is said about artificial intelligence training and education?\")\n\n\nThe context provided discusses the implementation of an AI education strategy required by Section 256 of the National Defense Authorization Act for Fiscal Year 2020. The strategy aims to educate servicemembers in relevant occupational fields, with a focus on data literacy across a broader population within the Department of Defense. The committee encourages the Air Force and Space Force to leverage government-owned training platforms informed by private sector expertise to accelerate learning and career path development. Additionally, the committee suggests expanding existing mobile enabled platforms to train and develop the cyber workforce of the Air Force and Space Force. Overall, there is a recognition that AI continues to be central to warfighting and that proper implementation of these new technologies requires a focus on education and training.\n\n\nThe answer is stored in results['answer']. The documents retrieved from the vector store used to generate the answer are stored in results['source_documents'] above.\n\nprint('ANSWER:')\nprint(\"\\n\".join(wrap(result['answer'])))\nprint()\nprint()\nprint('REFERENCES')\nprint()\nfor d in result['source_documents']:\n    print(f\"On Page {d.metadata['page']} in {d.metadata['source']}:\")\n    print(d.page_content)\n    print('----------------------------------------')\n    print()\n\nANSWER:\n The context provided discusses the implementation of an AI education\nstrategy required by Section 256 of the National Defense Authorization\nAct for Fiscal Year 2020. The strategy aims to educate servicemembers\nin relevant occupational fields, with a focus on data literacy across\na broader population within the Department of Defense. The committee\nencourages the Air Force and Space Force to leverage government-owned\ntraining platforms informed by private sector expertise to accelerate\nlearning and career path development. Additionally, the committee\nsuggests expanding existing mobile enabled platforms to train and\ndevelop the cyber workforce of the Air Force and Space Force. Overall,\nthere is a recognition that AI continues to be central to warfighting\nand that proper implementation of these new technologies requires a\nfocus on education and training.\n\n\nREFERENCES\n\nOn Page 359 in /tmp/ndaa/ndaa.pdf:\n‘‘servicemembers in relevant occupational fields on matters relating \nto artificial intelligence.’’ \nGiven the continued centrality of AI to warfighting, the com-\nmittee directs the Chief Digital and Artificial Intelligence Officer of \nthe Department of Defense to provide a briefing to the House Com-\nmittee on Armed Services not later than March 31, 2024, on the \nimplementation status of the AI education strategy, with emphasis \non current efforts underway, such as the AI Primer course within\n----------------------------------------\n\nOn Page 359 in /tmp/ndaa/ndaa.pdf:\nintelligence (AI) and machine learning capabilities available within \nthe Department of Defense. To ensure the proper implementation \nof these new technologies, there must be a focus on data literacy \nacross a broader population within the Department. Section 256 of \nthe National Defense Authorization Act for Fiscal Year 2020 (Pub-\nlic Law 116–92) required the Department of Defense to develop an \nAI education strategy, with the stated objective to educate\n----------------------------------------\n\nOn Page 102 in /tmp/ndaa/ndaa.pdf:\ntificial intelligence and machine learning (AI/ML), and cloud com-\nputing. The committee encourages the Air Force and Space Force \nto leverage government owned training platforms with curricula in-\nformed by private sector expertise to accelerate learning and career \npath development. \nTo that end, the committee encourages the Secretary of the Air \nForce to expand existing mobile enabled platforms to train and de-\nvelop the cyber workforce of Air Force and Space Force. To better\n----------------------------------------\n\nOn Page 109 in /tmp/ndaa/ndaa.pdf:\n70 \nrole of senior official with principal responsibility for artificial intel-\nligence and machine learning. In February 2022, the Department \nstood up the Chief Digital and Artificial Intelligence Office to accel-\nerate the Department’s adoption of AI. The committee encourages \nthe Department to build upon this progress and sustain efforts to \nresearch, develop, test, and where appropriate, operationalize AI \ncapabilities. \nArtificial intelligence capabilities of foreign adversaries\n----------------------------------------",
    "crumbs": [
      "Examples",
      "Document Question-Answering"
    ]
  },
  {
    "objectID": "examples_rag.html#advanced-example-nsf-awards",
    "href": "examples_rag.html#advanced-example-nsf-awards",
    "title": "Document Question-Answering",
    "section": "Advanced Example: NSF Awards",
    "text": "Advanced Example: NSF Awards\nThe example above employed the use of the default dense vector store, Chroma. By supplying store_type=\"sparse\" to LLM, a sparse vector store (i.e., keyword search engine) is used instead. Sparse vector stores index documents faster but requires keyword matches between sources containing answers and the question or query. Semantic search is still supported through on-demand dense vectorization in OnPrem.LLM.\nIn this example, we will instantiate a spare store directly and customize the ingestion process to include custom fields using a dataset of 2024 NSF Awards.\n\nSTEP 1: Download the Pre-Process the NSF Data\nNSF awards data stores as thousands of JSON files. The code below downloads and parses each JSON file.\n\nimport os\nimport zipfile\nimport requests\nimport json\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\n\n# Step 1: Download the ZIP file\nurl = \"https://www.nsf.gov/awardsearch/download?DownloadFileName=2024&All=true&isJson=true\"\nzip_path = \"/tmp/nsf_awards_2024.zip\"\n\nif not os.path.exists(zip_path):\n    print(\"Downloading NSF data...\")\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(zip_path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n    print(\"Download complete.\")\nelse:\n    print(\"ZIP file already exists.\")\n\n# Step 2: Unzip the file\nextract_dir = \"nsf_awards_2024\"\n\nif not os.path.exists(extract_dir):\n    print(\"Extracting ZIP file...\")\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_dir)\n    print(\"Extraction complete.\")\nelse:\n    print(\"Already extracted.\")\n\n# Step 3: Function to extract fields from JSON\ndef extract_fields(data):\n    title = data.get(\"awd_titl_txt\", \"N/A\")\n    abstract = data.get(\"awd_abstract_narration\", \"N/A\")\n    \n    pgm_ele = data.get(\"pgm_ele\")\n    if isinstance(pgm_ele, list) and pgm_ele:\n        category = pgm_ele[0].get(\"pgm_ele_name\", \"N/A\")\n    else:\n        category = \"N/A\"\n\n    # Authors\n    authors = []\n    for pi in data.get(\"pi\", []):\n        full_name = pi.get(\"pi_full_name\", \"\")\n        if full_name:\n            authors.append(full_name)\n    authors_str = \", \".join(authors) if authors else \"N/A\"\n\n    # Affiliation\n    affiliation = data.get(\"inst\", {}).get(\"inst_name\", \"N/A\")\n\n    # Amount\n    raw_amount = data.get(\"awd_amount\", data.get(\"tot_intn_awd_amt\", None))\n    try:\n        amount = float(raw_amount)\n    except (TypeError, ValueError):\n        amount = None\n\n    return {\n        \"title\": title or '',\n        \"abstract\": f'{title or \"\"}' + '\\n\\n' + f'{abstract or \"\"}',\n        \"category\": category,\n        \"authors\": authors_str,\n        \"affiliation\": affiliation,\n        \"amount\": amount\n    }\n\n# Step 4: Process all JSON files and write results to .txt\noutput_dir = \"/tmp/nsf_text_output\"\nos.makedirs(output_dir, exist_ok=True)\n\njson_files = list(Path(extract_dir).glob(\"*.json\"))\n\nprint(f\"Processing {len(json_files)} JSON files...\")\n\nnsf_data = []\nfor json_file in tqdm(json_files):\n    with open(json_file, 'r', encoding='utf-8') as f:\n        try:\n            data = json.load(f)\n        except json.JSONDecodeError:\n            continue  # skip bad files\n\n    fields = extract_fields(data)\n    fields['source'] = str(json_file)\n    nsf_data.append(fields)\n\nprint(\"All JSON files processed and saved to list of dictionaries.\")\n\nZIP file already exists.\nAlready extracted.\nProcessing 11687 JSON files...\n\n\n\n\n\nAll JSON files processed and saved to list of dictionaries.\n\n\n\n\nSTEP 2: Ingest Documents\nLet’s now store these NSF awards data in a Whoosh-backed sparse vector store. This is equivalent to supplying store_type=\"sparse\" to LLM. However, we will explicitly create the SparseStore instance to customize the ingestion process for NSF data. See the vectorstore factory example for more information.\nSince award abstracts are relatively short, we will skip document chunking (whether using onprem.ingest.chunk_documents or an external tool like chonkie) and instead store each award as a single record in the index. By accessing the vector store directly, any custom chunking strategy can be applied at this stage.\n\nfrom onprem.ingest import helpers, chunk_documents\nfrom onprem.ingest.stores import VectorStoreFactory\nstore = VectorStoreFactory.create(\n    kind='whoosh', # you can change to kind=\"chroma\" to use ChromaDB.\n    persist_location='/tmp/nsf_store'\n)\n\nNote that the default embedding model is: sentence-transformers/all-MiniLM-L6-v2. You can change it by supplying embedding_model_name to VectorStoreFactory.create.\n\ndocs = []\nfor d in nsf_data:\n    doc = helpers.doc_from_dict(d, content_field='abstract')\n    docs.append(doc)\nstore.add_documents(docs)\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 11687/11687 [00:10&lt;00:00, 1119.63it/s]\n\n\nLet’s examine the total number of awards stored.\n\nstore.get_size()\n\n11687\n\n\n\n\nSTEP 3: Explore NSF Awards\nWe can explore NSF awards by either using an LLM or querying the vector store directly.\nThe NSF buckets awards into different catgories. Let’s examine all the material-related categories.\n\nset([d['category'] for d in store.search('category:*material*', limit=100)['hits']])\n\n{'BIOMATERIALS PROGRAM',\n 'ELECTRONIC/PHOTONIC MATERIALS',\n 'Mechanics of Materials and Str',\n 'SOLID STATE & MATERIALS CHEMIS'}\n\n\nLet’s see how many of the material-related awards mention AI.\nOne of the advantages of sparse vector stores is the ability to easily use complex boolean queries to target specific documents.\n\nstore.search('(\"machine learning\" OR \"artificial intelligence\") AND category:*material*', limit=100)['total_hits']\n\n15\n\n\nWe wil now use an LLM to summarize how AI is utilized in this research.\nSince NSF awards data are publicly-available, we will use OpenAI’s GPT-4o-mini, a cloud LLM.\n\nfrom onprem import LLM\nllm = LLM('openai/gpt-4o-mini')\nllm.load_vectorstore(custom_vectorstore=store)\nresult = llm.ask('How is articial intelligene and machine learning used in these research projects?', \n                 limit=16,\n                  where_document='(\"machine learning\" OR \"artificial intelligence\") AND category:*material*')\n\nArtificial intelligence (AI) and machine learning (ML) are utilized in various research projects described in the provided context in several ways:\n\n1. **Data-Driven Approaches**: Many projects leverage AI techniques to analyze complex datasets and identify patterns that are not easily discernible through traditional methods. For example, in the project on engineered photonic materials, AI is used to develop new materials with tailored properties by consolidating information on material compositions and geometries.\n\n2. **Model Development and Prediction**: AI and ML are employed to create predictive models that can simulate the behavior of materials under different conditions. The project on recycled polymers utilizes AI to predict deformation and failure mechanisms in recyclates, enhancing their mechanical performance.\n\n3. **Optimization**: Machine learning algorithms are used for optimizing the design and synthesis processes of materials. In the project focused on luminescent biomaterials, iterative Bayesian optimization is applied to screen proteins for desired luminescent properties, facilitating the discovery of new materials.\n\n4. **Enhanced Characterization**: In the study of nanofibers, machine learning aids in understanding and mitigating defects that compromise strength by predicting stress fields around nanovoids and assessing the impact of atomic crosslinks on material properties.\n\n5. **Multiscale Simulations**: AI enhances multiscale modeling approaches, where simulations at different scales (from atomic to macro) are integrated to provide insights into material performance. For instance, the project on 3D printed polymer composites combines computational modeling with experimental data to understand fracture mechanisms.\n\n6. **Education and Workforce Development**: Several projects include educational components that involve teaching AI and ML techniques to students from diverse backgrounds, thereby preparing the next generation of engineers and scientists with skills relevant to emerging technologies.\n\nOverall, AI and ML are essential tools in these projects, facilitating advancements in material science, improving predictive capabilities, and enhancing the understanding of complex physical phenomena.\n\n\nAwards used to answer the question are shown below.\n\nfor d in result['source_documents']:\n    print(d.metadata['title'])\n\nConference: Uncertainty Quantification for Machine Learning Integrated Physics Modeling (UQ-MLIP 2024); Arlington, Virginia; 12-14 August 2024\nCollaborative Research: DMREF: Accelerating the Design and Development of Engineered Photonic Materials based on a Data-Driven Deep Learning Approach\nCollaborative Research: DMREF: Accelerating the Design and Development of Engineered Photonic Materials based on a Data-Driven Deep Learning Approach\nEAGER: Generative AI for Learning Emergent Complexity in  Mechanics-driven Coupled Physics Problems\nCAREER: Investigating the Role of Microstructure in the High Strain Rate Behavior of Stable Nanocrystalline Alloys\nConference: 10th International Conference on Spectroscopic Ellipsometry\nCAREER: Informed Testing — From Full-Field Characterization of Mechanically Graded Soft Materials to Student Equity in the Classroom\n2024 Solid State Chemistry Gordon Research Conference and Gordon Research Seminar\nDesigning Luminescent Biomaterials from First Principles\nCAREER: Recycled Polymers of Enhanced Strength and Toughness: Predicting Failure and Unraveling Deformation to Enable Circular Transitions\nCollaborative Research: DMREF: Organic Materials Architectured for Researching Vibronic Excitations with Light in the Infrared (MARVEL-IR)\nDesigning Pyrolyzed Nanofibers at the Atomic Level: Toward Synthesis of Ultra-high-strength Nano-carbon\nCAREER: Design and synthesis of functional van der Waals magnets\nIntegrated Multiscale Computational and Experimental Investigations on Fracture of Additively Manufactured Polymer Composites",
    "crumbs": [
      "Examples",
      "Document Question-Answering"
    ]
  },
  {
    "objectID": "examples_rag.html#additional-tips",
    "href": "examples_rag.html#additional-tips",
    "title": "Document Question-Answering",
    "section": "Additional Tips",
    "text": "Additional Tips\nThe LLM.askand LLM.ingest methods include many options for more complex scenarios.\n\nLLM.ingest options\n\nChunk Size: The default chunk_size is 1000 characters. You can increase/decrease as necessary as a parameter to LLM.ingest.\nChunking Strategies: Supplying pdf_markdown=True to LLM.ingest will chunk documents in a markdown-aware way. As mentioned above, you can also use specialized chunking strategies (e.g., semantic chunking via the chonkie) by creating/storing chunks directly with store.add_documents, as illustrated above. Finally, suppling preserve_paragraphs=True to LLM.ingest will attempt to faithfully preserve paragraphs.\nEmbeddding Model: You can change the default embedding model used for retrieval (default is sentence-transformers/all-MiniLM-L6-v2). See here for more information.\nTable Structure: If supplying infer_table_structure=True to LLM.ingest, the LLM.ask method will automatically consider tables within PDFs when answering questions. This behavior can be controlled with the table_k and table_score_threshold parameters in LLM.ask.\nTable Captions: If supplying caption_tables=True, an LLM-generated caption will be added to every extracted table for potentially better table retrieval.\nDocument Titles: If suppyling extract_document_titles=True to LLM.ingest, the title of each document will be inferred and added to each document chunk for potentially better retrieval.\nCustom Metadata: You can assign custom metadata to document chunks in various ways. For instance, the file_callables parameter to LLM.ingest allows you to assign fields based on the file path. The text_callables parameter allows you to assign fields based on the text content. (See the RAG pipeline documentation for an example.) Such metadata can then be used by LLM.ask, as further explained below.\n\n\n\nLLM.ask options\n\nNumber of Sources: Increasing number of sources to consider (limit parameter to LLM.ask)\nQuestion Decomposition: If supplying selfask=True as an argument, a Self-Ask prompting strategy will be used to decompose the question into subquestions.\nQuery Routing: You can route questions to specific subsets of documents using the KVRouter. See the RAG pipeline documentation for an example.\nCustom RAG Prompts: Adjusting prompts for QA with prompt_template argument to LLM.ask\nMetadata Filtering: You can explicitly filter sources considered for an answer using filtersandwhere_documentparameters. For instance, thefilters` parameter is a dictionary of field_name:field_value pairs used to filter potential answer sources.\nScore Thresholds: Adding a score threshold for sources",
    "crumbs": [
      "Examples",
      "Document Question-Answering"
    ]
  },
  {
    "objectID": "hf.base.html",
    "href": "hf.base.html",
    "title": "hf.base",
    "section": "",
    "text": "source\n\nPipeline\n\ndef Pipeline(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nBase class for all Pipelines. The only interface requirement is to define a call_ method."
  },
  {
    "objectID": "examples_classification.html",
    "href": "examples_classification.html",
    "title": "Few-Shot Classification",
    "section": "",
    "text": "The FewShotClassifier in OnPrem.LLM is a simple wrapper around the SetFit package that allows you to make text classification predictions on only a few labeled examples (e.g., 8 examples per class). It is useful when only a small amount of labeled examples are available for training a model. We will supply the use_smaller=True argument to use the smaller version of the default model. You can also supply the Hugging Face model ID of an embedding model of your own choosing.\nfrom onprem.pipelines import FewShotClassifier\nclf = FewShotClassifier(use_smaller=True)\n\nmodel_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\nThe default model is sentence-transformers/paraphrase-mpnet-base-v2, but we’re using all-MiniLM-L6-v2 in this example:\nclf.model_id_or_path\n\n'sentence-transformers/all-MiniLM-L6-v2'",
    "crumbs": [
      "Examples",
      "Few-Shot Classification"
    ]
  },
  {
    "objectID": "examples_classification.html#step-1-construct-a-tiny-dataset",
    "href": "examples_classification.html#step-1-construct-a-tiny-dataset",
    "title": "Few-Shot Classification",
    "section": "STEP 1: Construct a Tiny Dataset",
    "text": "STEP 1: Construct a Tiny Dataset\nIn this example, we will classify a sample of the 20NewsGroup dataset.\n\ncategories = ['comp.graphics', 'sci.med', 'sci.space', 'soc.religion.christian']\nfrom sklearn.datasets import fetch_20newsgroups\ntrain_b = fetch_20newsgroups(subset='train',\n   categories=categories, shuffle=True, random_state=42)\ntest_b = fetch_20newsgroups(subset='test',\n   categories=categories, shuffle=True, random_state=42)\n\nX_train = train_b.data\ny_train = [train_b.target_names[y] for y in train_b.target]\nX_test = test_b.data\ny_test = [test_b.target_names[y] for y in test_b.target]\n\n# sample a small number of examples from full training set\nX_sample, y_sample = clf.sample_examples(X_train, y_train, num_samples=8)\n\nThere are only 32 training examples!\n\nlen(X_sample)\n\n32\n\n\nThere are 1502 test examples.\n\nlen(X_test)\n\n1577",
    "crumbs": [
      "Examples",
      "Few-Shot Classification"
    ]
  },
  {
    "objectID": "examples_classification.html#step-2-train-on-the-tiny-dataset",
    "href": "examples_classification.html#step-2-train-on-the-tiny-dataset",
    "title": "Few-Shot Classification",
    "section": "STEP 2: Train on the Tiny Dataset",
    "text": "STEP 2: Train on the Tiny Dataset\nLet’s train:\n\nclf.train(X_sample,  y_sample, max_steps=50)\n\nApplying column mapping to the training dataset\n\n\n\n\n\n***** Running training *****\n  Num unique pairs = 768\n  Batch size = 32\n  Num epochs = 10\n\n\n\n      \n      \n      [50/50 00:30, Epoch 2/3]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n1\n0.300400\n\n\n50\n0.110400\n\n\n\n\n\n\n\n\n\nNote: If you encounter an error above, downgrading your version of transformers can sometimes resolve it (e.g., see this issue, which was open at the time of this writing).",
    "crumbs": [
      "Examples",
      "Few-Shot Classification"
    ]
  },
  {
    "objectID": "examples_classification.html#step-3-evaluate",
    "href": "examples_classification.html#step-3-evaluate",
    "title": "Few-Shot Classification",
    "section": "STEP 3: Evaluate",
    "text": "STEP 3: Evaluate\nAfter training, clf.model.labels stores the labels.\n\nprint(clf.evaluate(X_test, y_test, print_report=True))\n\n                        precision    recall  f1-score   support\n\n         comp.graphics       0.93      0.93      0.93       389\n               sci.med       0.93      0.88      0.91       396\n             sci.space       0.89      0.93      0.91       394\nsoc.religion.christian       0.94      0.96      0.95       398\n\n              accuracy                           0.92      1577\n             macro avg       0.92      0.92      0.92      1577\n          weighted avg       0.92      0.92      0.92      1577\n\nNone\n\n\nA 92% accuracy using only 32 examples!",
    "crumbs": [
      "Examples",
      "Few-Shot Classification"
    ]
  },
  {
    "objectID": "examples_classification.html#step-4-make-predictions-on-new-data",
    "href": "examples_classification.html#step-4-make-predictions-on-new-data",
    "title": "Few-Shot Classification",
    "section": "STEP 4: Make Predictions on New Data",
    "text": "STEP 4: Make Predictions on New Data\nLet’s make some predictions on new data:\n\nclf.predict(['My grapics card sucks for machine learning.'])\n\narray(['comp.graphics'], dtype='&lt;U22')\n\n\n\nclf.predict(['My mom likes going to church.'])\n\narray(['soc.religion.christian'], dtype='&lt;U22')\n\n\n\nclf.predict(['SpaceX launches lots of satellites.'])\n\narray(['sci.space'], dtype='&lt;U22')\n\n\nShow prediction probabilities:\n\nclf.predict_proba(['SpaceX launches lots of satellites.'])\n\ntensor([[0.3000, 0.1330, 0.4385, 0.1285]], dtype=torch.float64)",
    "crumbs": [
      "Examples",
      "Few-Shot Classification"
    ]
  },
  {
    "objectID": "examples_classification.html#step-5-inspect-and-explain-predictions",
    "href": "examples_classification.html#step-5-inspect-and-explain-predictions",
    "title": "Few-Shot Classification",
    "section": "STEP 5: Inspect and Explain Predictions",
    "text": "STEP 5: Inspect and Explain Predictions\nExplain predictions:\n\nclf.explain(['My graphics card sucks for machine learning.'])\n\n\n\n\n[0]\n\n            \n\noutputs\ncomp.graphics\nsci.med\nsci.space\nsoc.religion.christian0.30.20.40.50.2980550.298055base value0.4888920.488892fcomp.graphics(inputs)0.097      graphics  0.043      machine  0.032      card  0.024      for  0.015      My  0.007      .  0.001      learning  0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -0.026      sucks  -0.0                                                                                        inputs-0.00.015My 0.097graphics 0.032card -0.026sucks 0.024for 0.043machine 0.001learning0.007.0.00.390.350.310.270.430.470.510.2980550.298055base value0.4888920.488892fcomp.graphics(inputs)0.097      graphics  0.043      machine  0.032      card  0.024      for  0.015      My  0.007      .  0.001      learning  0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -0.026      sucks  -0.0                                                                                        inputs-0.00.015My 0.097graphics 0.032card -0.026sucks 0.024for 0.043machine 0.001learning0.007.0.00.30.20.40.50.2547870.254787base value0.1697470.169747fsci.med(inputs)0.023      sucks  0.0                                                                                        -0.033      graphics  -0.023      machine  -0.015      for  -0.012      .  -0.01      card  -0.008      My  -0.007      learning  -0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        inputs0.0-0.008My -0.033graphics -0.01card 0.023sucks -0.015for -0.023machine -0.007learning-0.012.-0.00.210.190.170.150.230.250.270.2547870.254787base value0.1697470.169747fsci.med(inputs)0.023      sucks  0.0                                                                                        -0.033      graphics  -0.023      machine  -0.015      for  -0.012      .  -0.01      card  -0.008      My  -0.007      learning  -0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        inputs0.0-0.008My -0.033graphics -0.01card 0.023sucks -0.015for -0.023machine -0.007learning-0.012.-0.00.30.20.40.50.1984920.198492base value0.1932890.193289fsci.space(inputs)0.007      .  0.006      learning  0.005      machine  0.004      sucks  0.001      for  0.0        0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                -0.022      graphics  -0.003      card  -0.002      My                                                                                                                                                          inputs0.0-0.002My -0.022graphics -0.003card 0.004sucks 0.001for 0.005machine 0.006learning0.007.0.00.20.190.180.210.220.1984920.198492base value0.1932890.193289fsci.space(inputs)0.007      .  0.006      learning  0.005      machine  0.004      sucks  0.001      for  0.0        0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                -0.022      graphics  -0.003      card  -0.002      My                                                                                                                                                          inputs0.0-0.002My -0.022graphics -0.003card 0.004sucks 0.001for 0.005machine 0.006learning0.007.0.00.30.20.40.50.2486660.248666base value0.1480730.148073fsoc.religion.christian(inputs)0.001      learning  0.0                                                                                        -0.042      graphics  -0.024      machine  -0.019      card  -0.009      for  -0.004      My  -0.002      .  -0.0      sucks  -0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        inputs0.0-0.004My -0.042graphics -0.019card -0.0sucks -0.009for -0.024machine 0.001learning-0.002.-0.00.20.180.160.220.240.2486660.248666base value0.1480730.148073fsoc.religion.christian(inputs)0.001      learning  0.0                                                                                        -0.042      graphics  -0.024      machine  -0.019      card  -0.009      for  -0.004      My  -0.002      .  -0.0      sucks  -0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        inputs0.0-0.004My -0.042graphics -0.019card -0.0sucks -0.009for -0.024machine 0.001learning-0.002.-0.0\n\n\nIf you click on comp.graphics, you can see that the word “graphics” has the largest impact on the prediction after “card” and then “machine”.",
    "crumbs": [
      "Examples",
      "Few-Shot Classification"
    ]
  },
  {
    "objectID": "examples_classification.html#step-6-save-andor-reload-the-model",
    "href": "examples_classification.html#step-6-save-andor-reload-the-model",
    "title": "Few-Shot Classification",
    "section": "STEP 6: Save and/or Reload the Model",
    "text": "STEP 6: Save and/or Reload the Model\nSave and reload the model:\n\nclf.save('/tmp/my_fewshot_model')\n\n\nclf = FewShotClassifier('/tmp/my_fewshot_model')\n\n\nclf.predict(['Elon Musk likes launching satellites.'])\n\narray(['sci.space'], dtype='&lt;U22')\n\n\nTIPS: You can also easily train traditional text classification models using Hugging Face Transformers and scikit-learn. Please see the documentation for more details.",
    "crumbs": [
      "Examples",
      "Few-Shot Classification"
    ]
  },
  {
    "objectID": "llm.backends.html",
    "href": "llm.backends.html",
    "title": "llm backends",
    "section": "",
    "text": "source",
    "crumbs": [
      "Source",
      "llm backends"
    ]
  },
  {
    "objectID": "llm.backends.html#examples",
    "href": "llm.backends.html#examples",
    "title": "llm backends",
    "section": "Examples",
    "text": "Examples\nThis example shows how to use OnPrem.LLM with cloud LLMs served from AWS GovCloud.\nThe example below assumes you have set both AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as environment variables. You can adjust the inference_arn, endpoint_url, and region_name based on your application scenario.\nfrom onprem import LLM\n\ninference_arn = \"YOUR INFERENCE ARN\"\nendpoint_url = \"YOUR ENDPOINT URL\"\nregion_name = \"us-gov-east-1\" # replace as necessary\n\n# set up LLM connection to Bedrock on AWS GovCloud\nllm = LLM(\n  f\"govcloud-bedrock://{inference_arn}\",\n  region_name=region_name,\n  endpoint_url=endpoint_url,\n)\n\n# send prompt to LLM\nresponse = llm.prompt(\"Write a haiku about the moon.\")\n\nStructured Outputs with AWS GovCloud Bedrock\nAWS GovCloud Bedrock natively supports structured outputs.\n\nfrom onprem import LLM\nfrom pydantic import BaseModel, Field\n\n\ninference_arn = \"YOUR INFERENCE ARN\"\nendpoint = \"YOUR ENDPOINT URL\"\nregion = \"us-gov-east-1\" # replace as necessary\n\n# setup LLM\nllm = LLM(\n  f\"govcloud-bedrock://{inference_arn}\",\n  region_name=region,\n  endpoint_url=endpoint,\n)\n\n# Define a Pydantic model for structured output\nclass PersonInfo(BaseModel):\n    name: str = Field(description=\"name of person\")\n    age: int = Field(description=\"age of person\")\n    city:str = Field(description=\"city in which the person currently lives\")\n    occupation:str = Field(description=\"occupation of person\")\n\n# sent structured output prompt to LLM\nprompt = \"\"\"\n  Extract the following information from this text:\n  \"Hi, I'm Sarah Johnson, I'm 28 years old, live in Seattle, and work as a software engineer.\"\n\"\"\"\nresult = llm.prompt(prompt, response_format=PersonInfo)\n\n# Print the structured result\nprint(f\"Name: {result.name}\")\nprint(f\"Age: {result.age}\")\nprint(f\"City: {result.city}\")\nprint(f\"Occupation: {result.occupation}\")\nprint(f\"Type: {type(result)}\")\n\nName: Sarah Johnson\nAge: 28\nCity: Seattle\nOccupation: software engineer\nType: &lt;class '__main__.PersonInfo'&gt;",
    "crumbs": [
      "Source",
      "llm backends"
    ]
  },
  {
    "objectID": "ingest.base.html",
    "href": "ingest.base.html",
    "title": "ingest.base",
    "section": "",
    "text": "source\n\nPDF2MarkdownLoader\n\ndef PDF2MarkdownLoader(\n    file_path:Union, password:Optional=None, mode:Literal='page', pages_delimiter:str='\\n\\x0c',\n    extract_images:bool=False, images_parser:Optional=None, images_inner_format:Literal='text',\n    extract_tables:Optional=None, headers:Optional=None, extract_tables_settings:Optional=None, kwargs:Any\n)-&gt;None:\n\nCustom PDF to Markdown Loader\n\nsource\n\n\nMyUnstructuredPDFLoader\n\ndef MyUnstructuredPDFLoader(\n    file_path:Union, mode:str='single', unstructured_kwargs:Any\n):\n\nCustom PDF Loader\n\nsource\n\n\nMyElmLoader\n\ndef MyElmLoader(\n    file_path:Union, mode:str='single', unstructured_kwargs:Any\n):\n\nWrapper to fallback to text/plain when default does not work\n\nsource\n\n\nload_spreadsheet_documents\n\ndef load_spreadsheet_documents(\n    file_path, text_column, metadata_columns:NoneType=None, sheet_name:NoneType=None\n):\n\nLoad documents from a spreadsheet where each row becomes a document.\nArgs: file_path: Path to the spreadsheet file (.xlsx, .xls, .csv) text_column: Name of the column containing the text content metadata_columns: List of column names to include as metadata (default: all other columns) sheet_name: For Excel files, name of the sheet to read (default: first sheet)\nReturns: List of Document objects, one per row\n\nsource\n\n\nload_web_document\n\ndef load_web_document(\n    url, username:NoneType=None, password:NoneType=None\n):\n\nDownload and extract text from a web document using load_single_document.\nArgs: url: The URL to download from username: Optional username for authentication (e.g., for SharePoint) password: Optional password for authentication (e.g., for SharePoint)\nReturns: List of Document objects\n\nsource\n\n\nbatchify_chunks\n\ndef batchify_chunks(\n    texts, batch_size:int=41000\n):\n\nsplit texts into batches specifically for Chroma\n\nsource\n\n\ndoes_vectorstore_exist\n\ndef does_vectorstore_exist(\n    db\n)-&gt;bool:\n\nChecks if vectorstore exists\n\nsource\n\n\nchunk_documents\n\ndef chunk_documents(\n    documents:list, # list of LangChain Documents or list of text strings\n    chunk_size:int=1000, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n    chunk_overlap:int=100, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n    infer_table_structure:bool=False, # This should be set to True if `documents` may contain contain tables (i.e., `doc.metadata['table']=True`).\n    preserve_paragraphs:bool=False, # If True, strictly chunk by paragraph and only split if paragraph exceeds `chunk_size`. If False, small paragraphs will be accumulated into a single chunk until `chunk_size` is exceeded.\n    keep_full_document:bool=False, # If True, skip chunking and return documents as-is\n    kwargs:VAR_KEYWORD\n)-&gt;List:\n\nProcess list of Documents or text strings by splitting into chunks. If text strings are provided, they will be converted to Document objects internally. If keep_full_document=True, documents are returned as-is without chunking.\n\nsource\n\n\nprocess_folder\n\ndef process_folder(\n    source_directory:str, # path to folder containing document store\n    chunk_size:int=1000, # text is split to this many characters by `langchain.text_splitter.RecursiveCharacterTextSplitter`\n    chunk_overlap:int=100, # character overlap between chunks in `langchain.text_splitter.RecursiveCharacterTextSplitter`\n    ignored_files:List=[], # list of files to ignore\n    ignore_fn:Optional=None, # Callable that accepts the file path (including file name) as input and ignores if returns True\n    batch_size:int=41000, # batch size used when processing documents\n    kwargs:VAR_KEYWORD\n)-&gt;List:\n\nLoad documents from folder, extract text from them, split texts into chunks. Extra kwargs fed to ingest.load_documents and ingest.load_single_document.\n\nsource\n\n\nload_documents\n\ndef load_documents(\n    source_dir:str, # path to folder containing documents\n    ignored_files:List=[], # list of filepaths to ignore\n    ignore_fn:Optional=None, # callable that accepts file path and returns True for ignored files\n    caption_tables:bool=False, # If True, agument table text with summaries of tables if infer_table_structure is True.\n    extract_document_titles:bool=False, # If True, infer document title and attach to individual chunks\n    llm:NoneType=None, # a reference to the LLM (used by `caption_tables` and `extract_document_titles`\n    n_proc:Optional=None, # number of CPU cores to use for text extraction. If None, use maximum for system.\n    verbose:bool=True, # verbosity\n    preserve_paragraphs:bool=False, # This is not used here and is only included to mask it from being forwarded to [`load_single_document`](https://amaiya.github.io/onprem/ingest.base.html#load_single_document).\n    kwargs:VAR_KEYWORD\n)-&gt;List:\n\nLoads all documents from the source documents directory, ignoring specified files. Extra kwargs fed to ingest.load_single_document.\nReturns a generator over documents.\n\nsource\n\n\nload_single_document\n\ndef load_single_document(\n    file_path:str, # path to file\n    pdf_unstructured:bool=False, # use unstructured for PDF extraction if True (will also OCR if necessary)\n    pdf_markdown:bool=False, # Convert PDFs to Markdown instead of plain text if True.\n    store_md5:bool=False, # Extract and store MD5 of document in metadata\n    store_mimetype:bool=False, # Guess and store mime type of document in metadata\n    store_file_dates:bool=False, # Extract snd store file dates in metadata\n    keep_full_document:bool=False, # If True, concatenate multi-page documents into single documents and disable chunking\n    max_words:Optional=None, # If provided, truncate document content to first N words (applied after concatenation)\n    file_callables:Optional=None, # optional dict with  keys and functions called with filepath as argument. Results stored as metadata.\n    text_callables:Optional=None, # optional dict with  keys and functions called with file text as argument. Results stored as metadata.\n    kwargs:VAR_KEYWORD\n)-&gt;List:\n\nExtract text from a single document. Will attempt to OCR PDFs, if necessary.\nNote that extra kwargs can be supplied to configure the behavior of PDF loaders. For instance, supplying infer_table_structure will cause load_single_document to try and infer and extract tables from PDFs. When pdf_unstructured=True and infer_table_structure=True, tables are represented as HTML within the main body of extracted text. In all other cases, inferred tables are represented as Markdown and appended to the end of the extracted text when infer_table_structure=True.\nThe keep_full_document option will combine multi-page documents into single documents with page breaks and disable chunking downstream. The max_words option will truncate documents to the specified number of words (applied after concatenation). When truncation occurs, metadata is updated to include original word count and truncation information.",
    "crumbs": [
      "Source",
      "ingest.base"
    ]
  },
  {
    "objectID": "hf.models.pooling.cls.html",
    "href": "hf.models.pooling.cls.html",
    "title": "hf.models.pooling.cls",
    "section": "",
    "text": "source\n\nClsPooling\n\ndef ClsPooling(\n    path, device, tokenizer:NoneType=None, maxlength:NoneType=None, modelargs:NoneType=None\n):\n\nBuilds CLS pooled vectors using outputs from a transformers model."
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nget_webapp_dir\n\ndef get_webapp_dir(\n    \n):\n\nGet the webapp directory path\n\nsource\n\n\nget_models_dir\n\ndef get_models_dir(\n    \n):\n\nGet the models directory path\n\nsource\n\n\nget_datadir\n\ndef get_datadir(\n    subfolder:NoneType=None\n):\n\nGet the data directory path, optionally with a subfolder. Creates the main data dir and any requested subfolder if they don’t exist.\nArgs: subfolder: Optional subfolder name to append to the data directory path\nReturns: Path to the data directory or subfolder\n\nsource\n\n\ndownload\n\ndef download(\n    url, filename, verify:bool=False\n):\n\n\nsource\n\n\ndf_to_md\n\ndef df_to_md(\n    df, caption:NoneType=None\n):\n\nConverts pd.Dataframe to markdown\n\nsource\n\n\nhtml_to_df\n\ndef html_to_df(\n    html_str:str\n)-&gt;Any:\n\nConvert HTML to dataframe.\n\nsource\n\n\nmd_to_df\n\ndef md_to_df(\n    md_str:str\n)-&gt;Any:\n\nConvert Markdown to dataframe.\n\nsource\n\n\nextract_noun_phrases\n\ndef extract_noun_phrases(\n    text:str\n):\n\nExtracts noun phrases from text, including coordinated phrases like “generative AI and live fire testing”, and removes subphrases like “AI” if “generative AI” is also found. Example: text = “Natural language processing (NLP) is a field of computer science, artificial intelligence,” “and computational linguistics concerned with the interactions between computers and human” “(natural) languages.” extract_noun_phrases(text) [‘Natural language processing’, ‘NLP’, ‘field’, ‘computer science’, ‘artificial intelligence’, ‘computational linguistics’, ‘interactions’, ‘computers’, ‘languages’, ‘human’]\n\nsource\n\n\ncontains_sentence\n\ndef contains_sentence(\n    sentence, text\n):\n\nReturns True if sentence is contained in text ignoring whether tokens are delmited by spaces or newlines or tabs.\n\nsource\n\n\nremove_sentence\n\ndef remove_sentence(\n    sentence, text, remove_follow:bool=False, flags:RegexFlag=re.IGNORECASE\n):\n\nRemoves a sentence or phrase from text ignoring whether tokens are delimited by spaces or newlines or tabs.\nIf remove_follow=True, then subsequent text until the first newline is also removed.\n\nsource\n\n\nsegment\n\ndef segment(\n    text:str, unit:str='paragraph', maxchars:int=2048\n):\n\nSegments text into a list of paragraphs or sentences depending on value of unit  (one of {'paragraph', 'sentence'}. The maxchars parameter is the maximum size of any unit of text.\n\nsource\n\n\nfiltered_generator\n\ndef filtered_generator(\n    generator, criteria:list=[]\n):\n\nFilters a generator based on a given predicate function.\nArgs: generator: The generator to filter. criteria: List of functions that take an element from the generator and return True if the element should be included, False otherwise.\nYields: Elements from the original generator that satisfy the predicate.\n\nsource\n\n\nbatch_generator\n\ndef batch_generator(\n    iterable, batch_size\n):\n\nBatched results from generator\n\nsource\n\n\nbatch_list\n\ndef batch_list(\n    input_list, batch_size\n):\n\nSplit list into chunks\n\nsource\n\n\nget_template_vars\n\ndef get_template_vars(\n    template_str:str\n)-&gt;List:\n\nGet template variables from a template string.\n\nsource\n\n\nformat_string\n\ndef format_string(\n    string_to_format:str, kwargs:str\n)-&gt;str:\n\nFormat a string with kwargs\n\nsource\n\n\nSafeFormatter\n\ndef SafeFormatter(\n    format_dict:Optional=None\n):\n\nSafe string formatter that does not raise KeyError if key is missing. Adapted from llama_index.",
    "crumbs": [
      "Source",
      "utils"
    ]
  },
  {
    "objectID": "pipelines.agent.model.html",
    "href": "pipelines.agent.model.html",
    "title": "pipelines.agent.model",
    "section": "",
    "text": "source\n\nremove_stop_sequences\n\ndef remove_stop_sequences(\n    text, stop_sequences\n):\n\nRemove stop sequences from text\n\nsource\n\n\nAgentModel\n\ndef AgentModel(\n    llm, kwargs:VAR_KEYWORD\n):\n\nA smolagents Model implementation that wraps an onprem LLM instance.\nThis adapter allows onprem LLM instances to be used with smolagents Agents.\nParameters: llm (LLM): An instance of onprem.llm.base.LLM **kwargs: Additional keyword arguments to pass to the parent Model class",
    "crumbs": [
      "Source",
      "pipelines.agent.model"
    ]
  },
  {
    "objectID": "hf.train.mlonnx.html",
    "href": "hf.train.mlonnx.html",
    "title": "hf.train.mlonnx",
    "section": "",
    "text": "source\n\nMLOnnx\n\ndef MLOnnx(\n    \n):\n\nExports a machine learning model to ONNX using ONNXMLTools."
  },
  {
    "objectID": "hf.data.questions.html",
    "href": "hf.data.questions.html",
    "title": "hf.data.questions",
    "section": "",
    "text": "source\n\nQuestions\n\ndef Questions(\n    tokenizer, columns, maxlength, stride\n):\n\nTokenizes question-answering datasets as input for training question-answering models."
  },
  {
    "objectID": "examples_guided_prompts.html",
    "href": "examples_guided_prompts.html",
    "title": "Guided Prompts",
    "section": "",
    "text": "The Guider in OnPrem.LLM, a simple interface to the Guidance package, can be used to guide the output of an LLM based on conditions and constraints that you supply.\nLet’s begin by creating an onprem.LLM instance.\nfrom onprem import LLM\nllm = LLM(n_gpu_layers=-1, verbose=False) # set based on your system\nNext, let’s create a Guider instance.\nfrom onprem.pipelines.guider import Guider\nguider = Guider(llm)\nThe guider.prompt method accepts Guidance prompts as input. (You can refer to the Guidance documentation for information on how to construct such prompts.)\nHere, we’ll show some examples (mostly taken from the Guidance documentation) and begin with importing some Guidance functions.",
    "crumbs": [
      "Examples",
      "Guided Prompts"
    ]
  },
  {
    "objectID": "examples_guided_prompts.html#the-select-function",
    "href": "examples_guided_prompts.html#the-select-function",
    "title": "Guided Prompts",
    "section": "The select function",
    "text": "The select function\nThe select function allows you to guide the LLM to generate output from only a finite set of alternatives. The Guider.prompt method returns a dictionary with the answer associated with the key you supply in the prompt.\n\nfrom guidance import select\n\n\nguidance_program = f'Do you want a joke or a poem? A ' + select(['joke', 'poem'], name='answer') # example from Guidance documentation\nguider.prompt(guidance_program)\n\nDo you want a joke or a poem? A joke\n\n\n{'answer': 'joke'}",
    "crumbs": [
      "Examples",
      "Guided Prompts"
    ]
  },
  {
    "objectID": "examples_guided_prompts.html#the-gen-function",
    "href": "examples_guided_prompts.html#the-gen-function",
    "title": "Guided Prompts",
    "section": "The gen function",
    "text": "The gen function\nThe gen function allows you to place conditions and constraints on the generated output.\n\nfrom guidance import gen\n\n\nguider.prompt(f'The capital of France is {gen(\"answer\", max_tokens=1, stop=\".\")}')\n\nThe capital of France is Paris\n\n\n{'answer': 'Paris'}\n\n\nYou can also use regular expressions to guide the output.\n\nprompt = f\"\"\"Question: Luke has ten balls. He gives three to his brother. How many balls does he have left?\nAnswer: \"\"\" + gen('answer', regex='\\d+')\n\n\nguider.prompt(prompt)\n\nQuestion: Luke has ten balls. He gives three to his brother. How many balls does he have left?\nAnswer: 7\n\n\n{'answer': '7'}\n\n\n\nprompt = 'Generate a list of numberes in descending order. 19, 18,' + gen('answer', max_tokens=50, stop_regex='[^\\d]7[^\\d]')\nguider.prompt(prompt)\n\nGenerate a list of numberes in descending order. 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8,\n\n\n{'answer': ' 17, 16, 15, 14, 13, 12, 11, 10, 9, 8,'}",
    "crumbs": [
      "Examples",
      "Guided Prompts"
    ]
  },
  {
    "objectID": "examples_guided_prompts.html#structured-outputs",
    "href": "examples_guided_prompts.html#structured-outputs",
    "title": "Guided Prompts",
    "section": "Structured Outputs",
    "text": "Structured Outputs\nUsing select and gen, you can guide the LLM to produce outputs conforming to the structure that you want (e.g., JSON).\nLet’s create a prompt for generating fictional D&D-type characters.\n\nsample_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\nsample_armour = [\"leather\", \"chainmail\", \"plate\"]\n\ndef generate_character_prompt(\n    character_one_liner,\n    weapons: list[str] = sample_weapons,\n    armour: list[str] = sample_armour,\n    n_items: int = 3\n):\n\n    prompt = ''\n    prompt += \"{\"\n    prompt += f'\"description\" : \"{character_one_liner}\",'\n    prompt += '\"name\" : \"' + gen(name=\"character_name\", stop='\"') + '\",'\n    # With guidance, we can call a GPU rather than merely random.randint()\n    prompt += '\"age\" : ' + gen(name=\"age\", regex=\"[0-9]+\") + ','\n    prompt += '\"armour\" : \"' + select(armour, name=\"armour\") + '\",'\n    prompt += '\"weapon\" : \"' + select(weapons, name=\"weapon\") + '\",'\n    prompt += '\"class\" : \"' + gen(name=\"character_class\", stop='\"') + '\",'\n    prompt += '\"mantra\" : \"' + gen(name=\"mantra\", stop='\"') + '\",'\n    # Again, we can avoid calling random.randint() like a pleb\n    prompt += '\"strength\" : ' + gen(name=\"age\", regex=\"[0-9]+\") + ','\n    prompt += '\"quest_items\" : [ '\n    for i in range(n_items):\n        prompt += '\"' + gen(name=\"items\", list_append=True, stop='\"') + '\"'  \n        # We now pause a moment to express our thoughts on the JSON\n        # specification's dislike of trailing commas\n        if i &lt; n_items - 1:\n            prompt += ','\n    prompt += \"]\"\n    prompt += \"}\"\n    return prompt\n\n\nd = guider.prompt(generate_character_prompt(\"A quick and nimble fighter\"))\n\n{\"description\" : \"A quick and nimble fighter\",\"name\" : \"Rogue\",\"age\" : 0,\"armour\" : \"leather\",\"weapon\" : \"crossbow\",\"class\" : \"rogue\",\"mantra\" : \"Stay nimble, stay quick.\",\"strength\" : 10,\"quest_items\" : [ \"a set of thieves' tools\",\"a map of the local area\",\"a set of lockpicks\"]}\n\n\n\nThe Generated Dictionary:\n\nd\n\n{'items': ['a set of lockpicks',\n  'a map of the local area',\n  \"a set of thieves' tools\"],\n 'age': '10',\n 'mantra': 'Stay nimble, stay quick.',\n 'character_class': 'rogue',\n 'weapon': 'crossbow',\n 'armour': 'leather',\n 'character_name': 'Rogue'}\n\n\n\n\nConvert to JSON\n\nimport json\nprint(json.dumps(d, indent=4))\n\n{\n    \"items\": [\n        \"a set of lockpicks\",\n        \"a map of the local area\",\n        \"a set of thieves' tools\"\n    ],\n    \"age\": \"10\",\n    \"mantra\": \"Stay nimble, stay quick.\",\n    \"character_class\": \"rogue\",\n    \"weapon\": \"crossbow\",\n    \"armour\": \"leather\",\n    \"character_name\": \"Rogue\"\n}",
    "crumbs": [
      "Examples",
      "Guided Prompts"
    ]
  },
  {
    "objectID": "hf.tensors.html",
    "href": "hf.tensors.html",
    "title": "hf.tensors",
    "section": "",
    "text": "source\n\nTensors\n\ndef Tensors(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nPipeline backed by a tensor processing framework. Currently supports PyTorch."
  },
  {
    "objectID": "llm.base.html",
    "href": "llm.base.html",
    "title": "llm",
    "section": "",
    "text": "source",
    "crumbs": [
      "Source",
      "llm"
    ]
  },
  {
    "objectID": "llm.base.html#example-usage",
    "href": "llm.base.html#example-usage",
    "title": "llm",
    "section": "Example Usage",
    "text": "Example Usage\nWe’ll use a small 3B-parameter model here for testing purposes. The vector database is stored under ~/onprem_data by default. In this example, we will store the vector store in temporary folders.\n\nimport tempfile\n\n\nvectordb_path = tempfile.mkdtemp()\n\n\nurl = 'https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf'\nllm = LLM(model_url=url,\n          prompt_template = \"&lt;|system|&gt;\\n&lt;/s&gt;\\n&lt;|user|&gt;\\n{prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\", verbose=False, confirm=False)\n\nllama_new_context_with_model: n_ctx_per_seq (3904) &lt; n_ctx_train (32768) -- the full capacity of the model will not be utilized\n\n\n\nassert os.path.isfile(\n    os.path.join(U.get_datadir(), os.path.basename(url))\n), \"missing model\"\n\n\nprompt = \"\"\"List three cute names for a cat.\"\"\"\n\n\nsaved_output = llm.prompt(prompt)\n\n\n1. Luna - this name means \"moon\" in Latin and is perfect for a cat with soft, moon-like fur or bright green eyes that seem to glow like the full moon.\n\n2. Willow - named after the delicate branches of a willow tree, this name would suit a sweet, gentle kitty who loves to snuggle and purr contentedly in your lap.\n\n3. Marshmallow - if you have a fluffy cat with a round tummy and a plump body, why not call her Marshmallow? This adorable name is sure to melt your heart as soon as you see her cute little face.\n\n\n\nllm.ingest(\"./tests/sample_data/ktrain_paper/\", chunk_size=500, chunk_overlap=50)\n\nAppending to existing vectorstore at /home/amaiya/onprem_data/vectordb\nLoading documents from ./sample_data/1/\n\n\nLoading new documents: 100%|██████████████████████| 1/1 [00:00&lt;00:00,  3.52it/s]\n\n\nLoaded 6 new documents from ./sample_data/1/\nSplit into 41 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\nCreating embeddings. May take some minutes...\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  1.18it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n\n\n\n\nquestion = \"\"\"What is ktrain?\"\"\"\nresult = llm.ask(question)\nprint(\"\\n\\nReferences:\\n\\n\")\nfor i, document in enumerate(result[\"source_documents\"]):\n    print(f\"\\n{i+1}.&gt; \" + document.metadata[\"source\"] + \":\")\n    print(document.page_content)\n\n\nKtrain is a Python library for machine learning that aims to provide a simple and unified interface for easily executing the three main steps of the machine learning process - preparing data, training models, and evaluating results - regardless of the type of data being used (such as text, images, or graphs). It is designed to help beginners and domain experts with limited programming or data science experience to build sophisticated machine learning models with minimal coding, while also serving as a useful toolbox for more experienced users. Ktrain follows a standard template for supervised learning tasks and supports custom models and data formats. It is licensed under the Apache license and can be found on GitHub at https://github.com/amaiya/ktrain. The text material mentions that ktrain was inspired by other low-code (and no-code) open-source ML libraries such as fastai and ludwig, and aims to further democratize machine learning by making it more accessible to a wider range of users.\n\nReferences:\n\n\n\n1.&gt; /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\ntransferred to, and executed on new data in a production environment.\nktrain is a Python library for machine learning with the goal of presenting a simple,\nuniﬁed interface to easily perform the above steps regardless of the type of data (e.g., text\nvs. images vs. graphs). Moreover, each of the three steps above can be accomplished in\n©2022 Arun S. Maiya.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are\n\n2.&gt; /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\ncustom models and data formats, as well. Inspired by other low-code (and no-code) open-\nsource ML libraries such as fastai (Howard and Gugger, 2020) and ludwig (Molino et al.,\n2019), ktrain is intended to help further democratize machine learning by enabling begin-\nners and domain experts with minimal programming or data science experience to build\nsophisticated machine learning models with minimal coding. It is also a useful toolbox for\n\n3.&gt; /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\nktrain.Learner instance, which is an abstraction to facilitate training.\n1. https://www.fast.ai/2018/07/16/auto-ml2/\n2\n\n4.&gt; /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:\nApache license, and available on GitHub at: https://github.com/amaiya/ktrain.\n2. Building Models\nSupervised learning tasks in ktrain follow a standard, easy-to-use template.\nSTEP 1: Load and Preprocess Data. This step involves loading data from diﬀerent\nsources and preprocessing it in a way that is expected by the model. In the case of text,\nthis may involve language-speciﬁc preprocessing (e.g., tokenization). In the case of images,",
    "crumbs": [
      "Source",
      "llm"
    ]
  },
  {
    "objectID": "hf.data.base.html",
    "href": "hf.data.base.html",
    "title": "hf.data.base",
    "section": "",
    "text": "source\n\nData\n\ndef Data(\n    tokenizer, columns, maxlength\n):\n\nBase data tokenization class."
  },
  {
    "objectID": "pipelines.rag.html",
    "href": "pipelines.rag.html",
    "title": "pipelines.rag",
    "section": "",
    "text": "source",
    "crumbs": [
      "Source",
      "pipelines.rag"
    ]
  },
  {
    "objectID": "pipelines.rag.html#example-using-query-routing-with-rag",
    "href": "pipelines.rag.html#example-using-query-routing-with-rag",
    "title": "pipelines.rag",
    "section": "Example: Using Query Routing with RAG",
    "text": "Example: Using Query Routing with RAG\nIn this example, we use the KVRouter to route RAG queries to the correct set of ingested documents.\nFirst, when we ingest documents, we assign a folder field to each document chunk using the file_callables argument. (You can also use the text_callables parameter to assign a field value based on text content.)\n\nfrom onprem import LLM\nfrom onprem.pipelines import KVRouter\nimport tempfile\n\n\n# Setup LLM and ingest with custom metadata\nllm = LLM('openai/gpt-4o-mini', vectordb_path=tempfile.mkdtemp())\ndef set_folder(filepath):\n    if 'sotu' in filepath:\n        return 'sotu'\n    elif 'ktrain_paper' in filepath:\n        return 'ktrain'\n    else:\n        return 'na'\n        \nllm.ingest('tests/sample_data/sotu', file_callables={'folder': set_folder})\nllm.ingest('tests/sample_data/ktrain_paper', file_callables={'folder': set_folder})\n\nCreating new vectorstore at /tmp/tmpzazbew9_/dense\nLoading documents from tests/sample_data/sotu\n\n\nLoading new documents: 100%|█████████████████████| 1/1 [00:00&lt;00:00, 215.95it/s]\nProcessing and chunking 1 new documents: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 994.15it/s]\n\n\nSplit into 43 chunks of text (max. 1000 chars each for text; max. 2000 chars for tables)\nCreating embeddings. May take some minutes...\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  2.18it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\nAppending to existing vectorstore at /tmp/tmpzazbew9_/dense\nLoading documents from tests/sample_data/ktrain_paper\n\n\n\nLoading new documents: 100%|██████████████████████| 1/1 [00:00&lt;00:00,  7.19it/s]\nProcessing and chunking 6 new documents: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 1353.87it/s]\n\n\nSplit into 22 chunks of text (max. 1000 chars each for text; max. 2000 chars for tables)\nCreating embeddings. May take some minutes...\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  9.80it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n\n\n\nNext, we setup a KVRouter that returns the best key-value pair (in this case, a specific folder value) based on the question or query. The key-value pair is then used to filter the documents appropriately when retrieving source documents for answer generation. The router can be supplied direclty to the ask method so that only documents in the appropriate folder are considered when generating answers.\n\n# Create router\nrouter = KVRouter(\n  field_name='folder',\n  field_descriptions={\n      'sotu': \"Biden's State of the Union Address\",\n      'ktrain': \"Research papers about ktrain library, a toolkit for machine learning, text classification, and computer vision.\"\n  },\n  llm=llm\n)\n\n# Example of router\nfilter_dict = router.route('Tell me about image classification')\nprint()\nprint(filter_dict)\n\n```json\n{\"category\":\"ktrain\"}\n```\n{'folder': 'ktrain'}\n\n\n\n# Use router with ask() - Method 1: Direct parameter\nresult = llm.ask(\n  \"What did Biden say about the economy?\",\n  router=router\n)\n\n```json\n{\"category\":\"sotu\"}\n```Biden discussed a new economic vision focused on investing in America, educating Americans, and growing the workforce. He criticized the trickle-down economic theory, stating it led to weaker economic growth, lower wages, and a widening wealth gap. He emphasized the importance of infrastructure investment, asserting that it would help the U.S. compete globally, particularly against China. Biden highlighted job creation through significant investments from companies like Ford and GM in electric vehicles. He acknowledged the struggles families face due to inflation and stated that his top priority is to get prices under control.\n\n\n\n# Use router with RAG pipeline - Method 2: Direct on pipeline\nrag_pipeline = llm.load_rag_pipeline()\nresult = rag_pipeline.ask(\n  \"How do I use ktrain for text classification?\",\n  router=router\n)\n\n```json\n{\"category\":\"ktrain\"}\n```To use ktrain for text classification, you can follow these simplified steps:\n\n1. **Load and Preprocess Data**: Use ktrain's preprocessing functions to load your text data and preprocess it. This typically involves tokenization and converting texts into a format that the model can understand.\n\n2. **Create Model**: Define your model using ktrain's built-in functions. You can customize it according to your needs, such as choosing the architecture or adjusting hyperparameters.\n\n3. **Train the Model**: Use ktrain's training functions to fit the model on your preprocessed data. You'll specify the number of epochs and other training parameters.\n\n4. **Evaluate the Model**: After training, you can evaluate your model's performance using ktrain's evaluation tools, which can include generating classification reports.\n\n5. **Make Predictions**: Finally, use the trained model to make predictions on new, unseen text data, leveraging the preprocessor instance created earlier.\n\nThis process can typically be done in just a few lines of code, making ktrain a low-code solution for text classification tasks. For detailed code examples, refer to the ktrain GitHub repository.",
    "crumbs": [
      "Source",
      "pipelines.rag"
    ]
  },
  {
    "objectID": "pipelines.rag.html#example-deciding-on-follow-up-questions",
    "href": "pipelines.rag.html#example-deciding-on-follow-up-questions",
    "title": "pipelines.rag",
    "section": "Example: Deciding On Follow-Up Questions",
    "text": "Example: Deciding On Follow-Up Questions\n\nrag_pipeline.needs_followup('What is ktrain?')\n\nNo\n\n\nFalse\n\n\n\nrag_pipeline.needs_followup('What is the capital of France?')\n\nNo\n\n\nFalse\n\n\n\nrag_pipeline.needs_followup(\"How was Paul Grahams life different before, during, and after YC?\")\n\nyes\n\n\nTrue\n\n\n\nrag_pipeline.needs_followup(\"Compare and contrast the customer segments and geographies of Lyft and Uber that grew the fastest.\")\n\nyes\n\n\nTrue\n\n\n\nrag_pipeline.needs_followup(\"Compare and contrast Uber and Lyft.\")\n\nyes\n\n\nTrue",
    "crumbs": [
      "Source",
      "pipelines.rag"
    ]
  },
  {
    "objectID": "pipelines.rag.html#example-generating-follow-up-questions",
    "href": "pipelines.rag.html#example-generating-follow-up-questions",
    "title": "pipelines.rag",
    "section": "Example: Generating Follow-Up Questions",
    "text": "Example: Generating Follow-Up Questions\n\nquestion = \"Compare and contrast the customer segments and geographies of Lyft and Uber that grew the fastest.\"\nsubquestions = rag_pipeline.decompose_question(question, parse=False)\nprint()\nprint(subquestions)\n\n```json\n{\n    \"items\": [\n        {\n            \"sub_question\": \"What are the customer segments of Lyft that grew the fastest\",\n        },\n        {\n            \"sub_question\": \"What are the customer segments of Uber that grew the fastest\",\n        },\n        {\n            \"sub_question\": \"Which geographies showed the fastest growth for Lyft\",\n        },\n        {\n            \"sub_question\": \"Which geographies showed the fastest growth for Uber\",\n        }\n    ]\n}\n```\n['What are the customer segments of Lyft that grew the fastest', 'What are the customer segments of Uber that grew the fastest', 'Which geographies showed the fastest growth for Lyft', 'Which geographies showed the fastest growth for Uber']",
    "crumbs": [
      "Source",
      "pipelines.rag"
    ]
  },
  {
    "objectID": "examples_information_extraction.html",
    "href": "examples_information_extraction.html",
    "title": "Information Extraction",
    "section": "",
    "text": "The pipelines module in OnPrem.LLM includes an Extractor to extract information of interest from a document using an LLM. This notebook we will show this module in action.\nThe Extractor runs multiple intermediate prompts and inferences, so we will set verbose-False and mute_stream=True. We will also set temperature=0 for more consistency in outputs. Finally, we will use OpenAI’s GPT-3.5-Turbo for this example, as it performs well out-of-box on extraction tasks with less prompt engineering.\nfrom onprem import LLM\nfrom onprem.pipelines import Extractor\nimport pandas as pd\npd.set_option('display.max_colwidth', None)\nllm = LLM(model_url='openai://gpt-3.5-turbo', verbose=False, mute_stream=True, temperature=0)\nextractor = Extractor(llm)\n\n/home/amaiya/projects/ghub/onprem/onprem/core.py:147: UserWarning: The model you supplied is gpt-3.5-turbo, an external service (i.e., not on-premises). Use with caution, as your data and prompts will be sent externally.\n  warnings.warn(f'The model you supplied is {self.model_name}, an external service (i.e., not on-premises). '+\\\nWhen using a cloud-based model with OnPrem.LLM, a warning will be issued notifying you that your prompts are being sent off-premises.",
    "crumbs": [
      "Examples",
      "Information Extraction"
    ]
  },
  {
    "objectID": "examples_information_extraction.html#example-extracting-institutions-from-research-papers",
    "href": "examples_information_extraction.html#example-extracting-institutions-from-research-papers",
    "title": "Information Extraction",
    "section": "Example: Extracting Institutions from Research Papers",
    "text": "Example: Extracting Institutions from Research Papers\nLet’s extract the institutions for ArXiv research papers using the prompt below.\n\nprompt = \"\"\"Extract the names of research institutions (e.g., universities, research labs, corporations, etc.) \nfrom the following sentence delimitated by three backticks. If there are no organizations, return NA.  \nIf there are multiple organizations, separate them with commas.\n```{text}```\n\"\"\"\n\n\n!wget --user-agent=\"Mozilla\" https://arxiv.org/pdf/2104.12871.pdf -O /tmp/mitchell.pdf -q\n\n\ndf = extractor.apply(prompt, fpath='/tmp/mitchell.pdf', pdf_pages=[1], stop=['\\n'])\ndf.loc[df['Extractions'] != 'NA'].Extractions[0]\n\n'Santa Fe Institute'\n\n\nThe apply method returns a dataframe of texts and prompt responses:\n\ndf.head()\n\n\n\n\n\n\n\n\nExtractions\nTexts\n\n\n\n\n0\nSanta Fe Institute\narXiv:2104.12871v2 [cs.AI] 28 Apr 2021 Why AI is Harder Than We Think Melanie Mitchell Santa Fe Institute Santa Fe, NM, USA mm@santafe.edu Abstract Since its beginning in the 1950s, the ﬁeld of artiﬁcial intelligence has cycled several times between periods of optimistic predictions and massive investment (“AI spring”) and periods of disappointment, loss of conﬁ- dence, and reduced funding (“AI winter”). Even with today’s seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconﬁdent predictions about the ﬁeld. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense. Introduction The year 2020 was supposed to herald the arrival of self-driving cars. Five years earlier, a headline in The Guardian predicted that “From 2020 you will become a permanent backseat driver” [1]. In 2016 Business Insider assured us that “10 million self-driving cars will be on the road by 2020” [2]. Tesla Motors CEO Elon Musk promised in 2019 that “A year from now, we’ll have over a million cars with full self-driving, software...everything” [3]. And 2020 was the target announced by several automobile companies to bring self-driving cars to market [4, 5, 6]. Despite attempts to redeﬁne “full self-driving” into existence [7], none of these predictions has come true. It’s worth quoting AI expert Drew McDermott on what can happen when over-optimism about AI systems—in particular, self-driving cars—turns out to be wrong: Perhaps expectations are too high, and... this will eventually result in disaster. [S]uppose that ﬁve years from now\n\n\n1\nNA\n[funding] collapses miserably as autonomous vehicles fail to roll. Every startup company fails. And there’s a big backlash so that you can’t get money for anything connected with AI. Everybody hurriedly changes the names of their research projects to something else. This condition [is] called the “AI Winter” [8]. What’s most notable is that McDermott’s warning is from 1984, when, like today, the ﬁeld of AI was awash with conﬁdent optimism about the near future of machine intelligence. McDermott was writing about a cyclical pattern in the ﬁeld. New, apparent breakthroughs would lead AI practitioners to predict rapid progress, successful commercialization, and the near-term prospects of “true AI.” Governments and companies would get caught up in the enthusiasm, and would shower the ﬁeld with research and development funding. AI Spring would be in bloom. When progress stalled, the enthusiasm, funding, and jobs would dry up. AI Winter would arrive. Indeed, about ﬁve years after McDermott’s warning, a new AI winter set in. In this chapter I explore the reasons for the repeating cycle of overconﬁdence followed by disappointment in expectations about AI. I argue that over-optimism among the public, the media, and even experts can 1\n\n\n\n\n\n\n\nLet’s try another:\n\n!wget --user-agent=\"Mozilla\" https://arxiv.org/pdf/2004.10703.pdf -O /tmp/ktrain.pdf -q\n\n\ndf = extractor.apply(prompt, fpath='/tmp/ktrain.pdf', pdf_pages=[1], stop=['\\n'])\ndf.loc[df['Extractions'] != 'NA'].Extractions[0]\n\n'Institute for Defense Analyses'\n\n\nLet’s try a paper with multiple affiliations.\n\n!wget --user-agent=\"Mozilla\" https://arxiv.org/pdf/2310.06643.pdf -O /tmp/multi-example.pdf -q\n\n\ndf = extractor.apply(prompt, fpath='/tmp/multi-example.pdf', pdf_pages=[1], stop=['\\n'])\ndf.loc[df['Extractions'] != 'NA'].Extractions[0]\n\n'Technical University of Denmark, University of Copenhagen'",
    "crumbs": [
      "Examples",
      "Information Extraction"
    ]
  },
  {
    "objectID": "examples_code.html",
    "href": "examples_code.html",
    "title": "Code Generation",
    "section": "",
    "text": "from onprem import LLM\n\nurl = \"https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGUF/resolve/main/codeup-llama-2-13b-chat-hf.Q4_K_M.gguf\"\nllm = LLM(url, n_gpu_layers=-1)\n\n\ntemplate = \"\"\"\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{prompt}\n\n### Response:\"\"\"\n\n\nanswer = llm.prompt(\n    \"Write a Python script that returns all file paths from a folder recursively.\",\n    prompt_template=template,\n)\n\n\nHere's a Python script that does just that! To use it, simply replace `folder_path` with the path to the folder you want to search.\n```python\nimport os\n\ndef get_file_paths(folder_path):\n    file_paths = []\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            file_paths.append(os.path.join(root, file))\n    return file_paths\n\nprint(get_file_paths(\"path/to/folder\"))\n```\nThis script uses the `os` module's `walk()` function to recursively explore the folder and collect all files. The `os.path.join()` function is used to join each file path with its parent directory, and return a list of all file paths. To use this script, simply replace `folder_path` with the path to the folder you want to search.\n\n\n\n\n\nfrom IPython.display import Markdown as md\n\nmd(answer)\n\nHere’s a Python script that does just that! To use it, simply replace folder_path with the path to the folder you want to search.\nimport os\n\ndef get_file_paths(folder_path):\n    file_paths = []\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            file_paths.append(os.path.join(root, file))\n    return file_paths\n\nprint(get_file_paths(\"path/to/folder\"))\nThis script uses the os module’s walk() function to recursively explore the folder and collect all files. The os.path.join() function is used to join each file path with its parent directory, and return a list of all file paths. To use this script, simply replace folder_path with the path to the folder you want to search.",
    "crumbs": [
      "Examples",
      "Code Generation"
    ]
  },
  {
    "objectID": "hf.models.pooling.mean.html",
    "href": "hf.models.pooling.mean.html",
    "title": "hf.models.pooling.mean",
    "section": "",
    "text": "source\n\nMeanPooling\n\ndef MeanPooling(\n    path, device, tokenizer:NoneType=None, maxlength:NoneType=None, modelargs:NoneType=None\n):\n\nBuilds mean pooled vectors usings outputs from a transformers model."
  },
  {
    "objectID": "hf.models.onnx.html",
    "href": "hf.models.onnx.html",
    "title": "hf.models.onnx",
    "section": "",
    "text": "source\n\nOnnxConfig\n\ndef OnnxConfig(\n    output_hidden_states:bool=False, # All models common arguments\n    output_attentions:bool=False, return_dict:bool=True, torchscript:bool=False, dtype:Union=None,\n    pruned_heads:Optional=None, # Common arguments\n    tie_word_embeddings:bool=True, chunk_size_feed_forward:int=0, is_encoder_decoder:bool=False,\n    is_decoder:bool=False, cross_attention_hidden_size:Optional=None, add_cross_attention:bool=False,\n    tie_encoder_decoder:bool=False, architectures:Optional=None, # Fine-tuning task arguments\n    finetuning_task:Optional=None, id2label:Optional=None, label2id:Optional=None, num_labels:Optional=None,\n    task_specific_params:Optional=None, problem_type:Optional=None, tokenizer_class:Optional=None, # Tokenizer kwargs\n    prefix:Optional=None, bos_token_id:Optional=None, pad_token_id:Optional=None, eos_token_id:Optional=None,\n    sep_token_id:Optional=None, decoder_start_token_id:Optional=None, kwargs:VAR_KEYWORD\n):\n\nConfiguration for ONNX models.\n\nsource\n\n\nOnnxModel\n\ndef OnnxModel(\n    model, config:NoneType=None\n):\n\nProvides a Transformers/PyTorch compatible interface for ONNX models. Handles casting inputs and outputs with minimal to no copying of data."
  },
  {
    "objectID": "ingest.stores.factory.html",
    "href": "ingest.stores.factory.html",
    "title": "ingest.stores.factory",
    "section": "",
    "text": "source\n\nVectorStoreFactory\n\ndef VectorStoreFactory(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nConvenience factory for creating vector stores with sensible defaults.\nProvides a simple interface to create the most commonly used vector stores: - ChromaStore (default) - Dense vector store using Chroma - WhooshStore - Sparse text search using Whoosh - ElasticsearchStore - Unified dense + sparse using Elasticsearch\n\nsource\n\n\nVectorStoreFactory.create\n\ndef create(\n    kind:str='chroma', persist_location:str=None, kwargs:VAR_KEYWORD\n):\n\nCreate a vector store instance.\nArgs: kind: Type of store to create. One of: - ‘chroma’ (default): ChromaStore for dense vector search - ‘whoosh’: WhooshStore for sparse text search\n- ‘chroma+whoosh’: a DualStore using ChromaStore and WhooshStore - ‘elasticsearch’: ElasticsearchStore for unified dense + sparse - ‘elasticsearch_sparse’: For use with pre-existing Elasticsearch indices without dense vectors persist_location: Where to store the index/database **kwargs: Additional arguments (e.g., embedding_model_name) passed to the store constructor\nReturns: VectorStore instance\nExamples: # Create default ChromaStore store = VectorStoreFactory.create()\n# Create WhooshStore for text search\nstore = VectorStoreFactory.create('whoosh', persist_location='./search_index')\n\n# Create ElasticsearchStore for hybrid search\nstore = VectorStoreFactory.create('elasticsearch', \n                                 persist_location='http://localhost:9200',\n                                 index_name='my_docs')",
    "crumbs": [
      "Source",
      "ingest.stores.factory"
    ]
  },
  {
    "objectID": "examples_semantic.html",
    "href": "examples_semantic.html",
    "title": "Semantic Similarity",
    "section": "",
    "text": "The underlying vector database in OnPrem.LLM can be used for detecting semantic similarity among pieces of text.\nYou can access the default vector store from an LLM object:\nfrom onprem import LLM\n\nvectordb_path = tempfile.mkdtemp()\nllm = LLM(\n    embedding_model_name=\"sentence-transformers/nli-mpnet-base-v2\",\n    embedding_encode_kwargs={\"normalize_embeddings\": True},\n    vectordb_path=vectordb_path,\n    store_type='dense',\n    verbose=False\n)\nstore = llm.load_vectorstore()\nHowever, in this example, we will create VectorStore instances explicitly to avoid loading an LLM, which is not needed in this example.\nThe VectorStoreFactory is useful in instantiating different backend vectorstores (e.g., Chroma, Whoosh, Elasticsearch).\n\nimport os, tempfile\n\nfrom onprem.ingest.stores import VectorStoreFactory\n\nstore = VectorStoreFactory.create(\n    kind='chroma',\n    persist_location='/tmp/my_vectordb',\n    embedding_model_name=\"sentence-transformers/nli-mpnet-base-v2\",\n    embedding_encode_kwargs={\"normalize_embeddings\": True},\n)\n\n\ndata = [  # from txtai\n    \"US tops 5 million confirmed virus cases\",\n    \"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\",\n    \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n    \"The National Park Service warns against sacrificing slower friends in a bear attack\",\n    \"Maine man wins $1M from $25 lottery ticket\",\n    \"Make huge profits without work, earn up to $100,000 a day\",\n]\nsource_folder = tempfile.mkdtemp()\nfor i, d in enumerate(data):\n    filename = os.path.join(source_folder, f\"doc{i}.txt\")\n    with open(filename, \"w\") as f:\n        f.write(d)\n\n\nstore.ingest(source_folder, chunk_size=500, chunk_overlap=0)\n\nCreating new vectorstore at /tmp/my_vectordb\nLoading documents from /tmp/tmpeg2wt1z7\n\n\nLoading new documents: 100%|████████████████████| 6/6 [00:00&lt;00:00, 1540.98it/s]\nProcessing and chunking 6 new documents: 100%|██████████████████████████████████████████| 1/1 [00:00&lt;00:00, 1940.01it/s]\n\n\nSplit into 6 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)\nCreating embeddings. May take some minutes...\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  2.13it/s]\n\n\nIngestion complete! You can now query your documents using the LLM.ask or LLM.chat methods\n\n\n\n\n\nHere, we get a reference to the underlying vector store and query it directly to find the best semantic match.\n\nfor query in (\n    \"feel good story\",\n    \"climate change\",\n    \"public health story\",\n    \"war\",\n    \"wildlife\",\n    \"asia\",\n    \"lucky\",\n    \"dishonest junk\",\n):\n    docs = store.semantic_search(query)\n    print(f\"{query} : {docs[0].page_content}\")\n\nfeel good story : Maine man wins $1M from $25 lottery ticket\nclimate change : Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\npublic health story : US tops 5 million confirmed virus cases\nwar : Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nwildlife : The National Park Service warns against sacrificing slower friends in a bear attack\nasia : Beijing mobilises invasion craft along coast as Taiwan tensions escalate\nlucky : Maine man wins $1M from $25 lottery ticket\ndishonest junk : Make huge profits without work, earn up to $100,000 a day",
    "crumbs": [
      "Examples",
      "Semantic Similarity"
    ]
  },
  {
    "objectID": "hf.train.hftrainer.html",
    "href": "hf.train.hftrainer.html",
    "title": "hf.train.hftrainer",
    "section": "",
    "text": "source\n\nTrainingArguments\n\ndef TrainingArguments(\n    output_dir:Optional=None, overwrite_output_dir:bool=False, do_train:bool=False, do_eval:bool=False,\n    do_predict:bool=False, eval_strategy:Union='no', prediction_loss_only:bool=False,\n    per_device_train_batch_size:int=8, per_device_eval_batch_size:int=8, per_gpu_train_batch_size:Optional=None,\n    per_gpu_eval_batch_size:Optional=None, gradient_accumulation_steps:int=1, eval_accumulation_steps:Optional=None,\n    eval_delay:float=0, torch_empty_cache_steps:Optional=None, learning_rate:float=5e-05, weight_decay:float=0.0,\n    adam_beta1:float=0.9, adam_beta2:float=0.999, adam_epsilon:float=1e-08, max_grad_norm:float=1.0,\n    num_train_epochs:float=3.0, max_steps:int=-1, lr_scheduler_type:Union='linear', lr_scheduler_kwargs:Union=None,\n    warmup_ratio:float=0.0, warmup_steps:int=0, log_level:str='passive', log_level_replica:str='warning',\n    log_on_each_node:bool=True, logging_dir:Optional=None, logging_strategy:Union='steps',\n    logging_first_step:bool=False, logging_steps:float=500, logging_nan_inf_filter:bool=True,\n    save_strategy:Union='steps', save_steps:float=500, save_total_limit:Optional=None, save_safetensors:bool=True,\n    save_on_each_node:bool=False, save_only_model:bool=False, restore_callback_states_from_checkpoint:bool=False,\n    no_cuda:bool=False, use_cpu:bool=False, use_mps_device:bool=False, seed:int=42, data_seed:Optional=None,\n    jit_mode_eval:bool=False, bf16:bool=False, fp16:bool=False, fp16_opt_level:str='O1',\n    half_precision_backend:str='auto', bf16_full_eval:bool=False, fp16_full_eval:bool=False, tf32:Optional=None,\n    local_rank:int=-1, ddp_backend:Optional=None, tpu_num_cores:Optional=None, tpu_metrics_debug:bool=False,\n    debug:Union='', dataloader_drop_last:bool=False, eval_steps:Optional=None, dataloader_num_workers:int=0,\n    dataloader_prefetch_factor:Optional=None, past_index:int=-1, run_name:Optional=None, disable_tqdm:Optional=None,\n    remove_unused_columns:bool=True, label_names:Optional=None, load_best_model_at_end:bool=False,\n    metric_for_best_model:Optional=None, greater_is_better:Optional=None, ignore_data_skip:bool=False,\n    fsdp:Union=None, fsdp_min_num_params:int=0, fsdp_config:Union=None,\n    fsdp_transformer_layer_cls_to_wrap:Optional=None, accelerator_config:Union=None,\n    parallelism_config:Optional=None, deepspeed:Union=None, label_smoothing_factor:float=0.0,\n    optim:Union='adamw_torch_fused', optim_args:Optional=None, adafactor:bool=False, group_by_length:bool=False,\n    length_column_name:str='length', report_to:Union=None, project:str='huggingface',\n    trackio_space_id:Optional='trackio', ddp_find_unused_parameters:Optional=None, ddp_bucket_cap_mb:Optional=None,\n    ddp_broadcast_buffers:Optional=None, dataloader_pin_memory:bool=True, dataloader_persistent_workers:bool=False,\n    skip_memory_metrics:bool=True, use_legacy_prediction_loop:bool=False, push_to_hub:bool=False,\n    resume_from_checkpoint:Optional=None, hub_model_id:Optional=None, hub_strategy:Union='every_save',\n    hub_token:Optional=None, hub_private_repo:Optional=None, hub_always_push:bool=False, hub_revision:Optional=None,\n    gradient_checkpointing:bool=False, gradient_checkpointing_kwargs:Union=None,\n    include_inputs_for_metrics:bool=False, include_for_metrics:list=&lt;factory&gt;, eval_do_concat_batches:bool=True,\n    fp16_backend:str='auto', push_to_hub_model_id:Optional=None, push_to_hub_organization:Optional=None,\n    push_to_hub_token:Optional=None, mp_parameters:str='', auto_find_batch_size:bool=False,\n    full_determinism:bool=False, torchdynamo:Optional=None, ray_scope:Optional='last', ddp_timeout:int=1800,\n    torch_compile:bool=False, torch_compile_backend:Optional=None, torch_compile_mode:Optional=None,\n    include_tokens_per_second:bool=False, include_num_input_tokens_seen:Union=False,\n    neftune_noise_alpha:Optional=None, optim_target_modules:Union=None, batch_eval_metrics:bool=False,\n    eval_on_start:bool=False, use_liger_kernel:bool=False, liger_kernel_config:Optional=None,\n    eval_use_gather_object:bool=False, average_tokens_across_devices:bool=True\n)-&gt;None:\n\nExtends standard TrainingArguments to make the output directory optional for transient models.\n\nsource\n\n\nHFTrainer\n\ndef HFTrainer(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nTrains a new Hugging Face Transformer model using the Trainer framework."
  },
  {
    "objectID": "pipelines.extractor.html",
    "href": "pipelines.extractor.html",
    "title": "pipelines.extractor",
    "section": "",
    "text": "source\n\nExtractor\n\ndef Extractor(\n    llm, # An `onprem.LLM` object\n    prompt_template:Optional=None, # A model specific prompt_template with a single placeholder named \"{prompt}\". If supplied, overrides the `prompt_template` supplied to the [`LLM`](https://amaiya.github.io/onprem/llm.base.html#llm) constructor.\n    kwargs:VAR_KEYWORD\n):\n\nExtractor applies a given prompt to each sentence or paragraph in a document and returns the results.\n\nsource\n\n\nExtractor.apply\n\ndef apply(\n    ex_prompt_template:str, # A prompt to apply to each `unit` in document. Should have a single variable, `{text}`\n    fpath:Optional=None, # A path to to a single file of interest (e.g., a PDF or MS Word document). Mutually-exclusive with `content`.\n    content:Optional=None, # Text content of a document of interest.  Mutually-exclusive with `fpath`.\n    unit:str='paragraph', # One of {'sentence', 'paragraph'}.\n    pydantic_model:NoneType=None, # If a Pydantic model is supplied, [`LLM.pydantic_prompt`](https://amaiya.github.io/onprem/llm.base.html#llm.pydantic_prompt) is used instead of [`LLM.prompt`](https://amaiya.github.io/onprem/llm.base.html#llm.prompt).\n    attempt_fix:bool=False, # If True and `pydantic_model` is supplied, attempt to fix malformed/incomplete outputs.\n    fix_llm:NoneType=None, # LLM used to attempt fix when `attempt_fix=True`. If None, then use `self.llm.llm`.\n    preproc_fn:Optional=None, # Function should accept a text string and returns a new preprocessed input.\n    filter_fn:Optional=None, # A function that accepts a sentence or paragraph and returns `True` if prompt should be applied to it.\n    clean_fn:Optional=None, # A function that accepts a sentence or paragraph and returns \"cleaned\" version of the text. (applied after `filter_fn`)\n    pdf_pages:List=[], # If `fpath` is a PDF document, only apply prompt to text on page numbers listed in `pdf_pages` (starts at 1).\n    maxchars:int=2048, # units (i.e., paragraphs or sentences) larger than `maxchars` split.\n    stop:list=[], # list of characters to trigger the LLM to stop generating.\n    kwargs:VAR_KEYWORD\n):\n\nApply the prompt to each unit (where a “unit” is either a paragraph or sentence) optionally filtered by filter_fn. Extra kwargs fed directly to load_single_document. Results are stored in a pandas.Dataframe.\n\nfrom onprem import LLM\nfrom onprem.pipelines import Extractor\n\n\nprompt_template = \"&lt;s&gt;[INST] {prompt} [/INST]\" # prompt template for Mistral\nllm = LLM(model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf', \n          n_gpu_layers=33,  # change based on your system\n          verbose=False, mute_stream=True, \n          prompt_template=prompt_template)\nextractor = Extractor(llm)\n\n/home/amaiya/mambaforge/envs/llm/lib/python3.9/site-packages/langchain_core/language_models/llms.py:239: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n  warnings.warn(\n\n\n\nprompt = \"\"\"Extract citations from the following sentences. Return #NA# if there are no citations in the text. Here are some examples:\n\n[SENTENCE]:pretrained BERT text classifier (Devlin et al., 2018), models for sequence tagging (Lample et al., 2016)\n[CITATIONS]:(Devlin et al., 2018), (Lample et al., 2016)\n[SENTENCE]:Machine learning (ML) is a powerful tool.\n[CITATIONS]:#NA#\n[SENTENCE]:Following inspiration from a blog post by Rachel Thomas of fast.ai (Howard and Gugger, 2020), we refer to this as Augmented Machine Learning or AugML\n[CITATIONS]:(Howard and Gugger, 2020)\n[SENTENCE]:{text}\n[CITATIONS]:\"\"\"\n\n\ncontent = \"\"\"\nFor instance, the fit_onecycle method employs a 1cycle policy (Smith, 2018). \n\"\"\"\ndf = extractor.apply(prompt, content=content, stop=['\\n'])\nassert df['Extractions'][0].strip().startswith('(Smith, 2018)')\n\n\ncontent =\"\"\"In the case of text, this may involve language-specific preprocessing (e.g., tokenization).\"\"\"\ndf = extractor.apply(prompt, content=content, stop=['\\n'])\nassert df['Extractions'][0].strip().startswith('#NA#')",
    "crumbs": [
      "Source",
      "pipelines.extractor"
    ]
  },
  {
    "objectID": "hf.models.pooling.base.html",
    "href": "hf.models.pooling.base.html",
    "title": "hf.models.pooling.base",
    "section": "",
    "text": "source\n\nPooling\n\ndef Pooling(\n    path, device, tokenizer:NoneType=None, maxlength:NoneType=None, modelargs:NoneType=None\n):\n\nBuilds pooled vectors usings outputs from a transformers model."
  }
]