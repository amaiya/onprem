<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Core functionality for LLMs">

<title>llm – onprem</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-000f0825b9c55ed21d14970d810c14a9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="llm – onprem">
<meta property="og:description" content="Core functionality for LLMs">
<meta property="og:site_name" content="onprem">
<meta name="twitter:title" content="llm – onprem">
<meta name="twitter:description" content="Core functionality for LLMs">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">onprem</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./llm.base.html">Source</a></li><li class="breadcrumb-item"><a href="./llm.base.html">llm</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">OnPrem.LLM</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Examples</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Use Prompts to Solve Problems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_rag.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Document Question-Answering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_code.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Code Generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_semantic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semantic Similarity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_guided_prompts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Guided Prompts</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_summarization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Summarization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_information_extraction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Information Extraction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Few-Shot Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_qualitative_survey_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Qualitative Survey Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_legal_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Legal and Regulatory Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_agent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Agent-Based Task Execution</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_openai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using OpenAI Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples_vectorstore_factory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Using Different Vector Stores</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./workflows.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Buildling Workflows</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./webapp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A Built-In Web App</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Source</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./llm.base.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">llm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./llm.helpers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">llm helpers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./llm.backends.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">llm backends</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ingest.base.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ingest.base</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ingest.helpers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ingest.helpers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ingest.stores.base.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ingest.stores.base</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ingest.stores.factory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ingest.stores.factory</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ingest.stores.dense.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ingest.stores.dense</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ingest.stores.sparse.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ingest.stores.sparse</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ingest.stores.dual.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ingest.stores.dual</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./utils.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">utils</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.extractor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pipelines.extractor</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.summarizer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pipelines.summarizer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.classifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pipelines.classifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.guider.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pipelines.guider</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.agent.base.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pipelines.agent.base</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.agent.model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pipelines.agent.model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.agent.tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">pipelines.agent.tools</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#llm" id="toc-llm" class="nav-link active" data-scroll-target="#llm">LLM</a></li>
  <li><a href="#llm.download_model" id="toc-llm.download_model" class="nav-link" data-scroll-target="#llm.download_model">LLM.download_model</a></li>
  <li><a href="#llm.load_llm" id="toc-llm.load_llm" class="nav-link" data-scroll-target="#llm.load_llm">LLM.load_llm</a></li>
  <li><a href="#llm.load_vectorstore" id="toc-llm.load_vectorstore" class="nav-link" data-scroll-target="#llm.load_vectorstore">LLM.load_vectorstore</a></li>
  <li><a href="#llm.load_chatbot" id="toc-llm.load_chatbot" class="nav-link" data-scroll-target="#llm.load_chatbot">LLM.load_chatbot</a></li>
  <li><a href="#llm.query" id="toc-llm.query" class="nav-link" data-scroll-target="#llm.query">LLM.query</a></li>
  <li><a href="#llm.prompt" id="toc-llm.prompt" class="nav-link" data-scroll-target="#llm.prompt">LLM.prompt</a></li>
  <li><a href="#llm.pydantic_prompt" id="toc-llm.pydantic_prompt" class="nav-link" data-scroll-target="#llm.pydantic_prompt">LLM.pydantic_prompt</a></li>
  <li><a href="#llm.ingest" id="toc-llm.ingest" class="nav-link" data-scroll-target="#llm.ingest">LLM.ingest</a></li>
  <li><a href="#llm.ask" id="toc-llm.ask" class="nav-link" data-scroll-target="#llm.ask">LLM.ask</a></li>
  <li><a href="#llm.chat" id="toc-llm.chat" class="nav-link" data-scroll-target="#llm.chat">LLM.chat</a></li>
  <li><a href="#example-usage" id="toc-example-usage" class="nav-link" data-scroll-target="#example-usage">Example Usage</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/amaiya/onprem/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./llm.base.html">Source</a></li><li class="breadcrumb-item"><a href="./llm.base.html">llm</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">llm</h1>
</div>

<div>
  <div class="description">
    Core functionality for LLMs
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L63" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="llm" class="level3">
<h3 class="anchored" data-anchor-id="llm">LLM</h3>
<blockquote class="blockquote">
<pre><code> LLM (model_url:Optional[str]=None, model_id:Optional[str]=None,
      default_model:str='zephyr', default_engine:str='llama.cpp',
      prompt_template:Optional[str]=None,
      model_download_path:Optional[str]=None,
      vectordb_path:Optional[str]=None, store_type:str='dense',
      vectorstore=None, max_tokens:int=512,
      n_gpu_layers:Optional[int]=None, n_ctx:int=3900, n_batch:int=1024,
      stop:list=[], mute_stream:bool=False, callbacks=[],
      embedding_model_name:str='sentence-transformers/all-MiniLM-L6-v2',
      embedding_model_kwargs:Optional[dict]=None,
      embedding_encode_kwargs:dict={'normalize_embeddings': False},
      rag_num_source_docs:int=4, rag_score_threshold:float=0.0,
      check_model_download:bool=True, confirm:bool=True,
      verbose:bool=True, **kwargs)</code></pre>
</blockquote>
<p>*LLM Constructor. Extra <code>kwargs</code> (e.g., temperature) are fed directly the LangChain LLM or Transformer Pipeline.</p>
<p><strong>Args:</strong></p>
<ul>
<li><em>model_url</em>: URL to <code>.GGUF</code> model (or the filename if already been downloaded to <code>model_download_path</code>). To use an OpenAI-compatible REST API (e.g., vLLM, OpenLLM, Ollama), supply the URL (e.g., <code>http://localhost:8080/v1</code>). To use a cloud-based OpenAI model, replace URL with: <code>openai://&lt;name_of_model&gt;</code> (e.g., <code>openai://gpt-3.5-turbo</code>). To use Azure OpenAI, replace URL with: with: <code>azure://&lt;deployment_name&gt;</code>. If None, use the model indicated by <code>default_model</code>.</li>
<li><em>model_id</em>: Name of or path to Hugging Face model (e.g., in SafeTensor format). Hugging Face Transformers is used for LLM generation instead of <strong>llama-cpp-python</strong>. Mutually-exclusive with <code>model_url</code> and <code>default_model</code>. The <code>n_gpu_layers</code> and <code>model_download_path</code> parameters are ignored if <code>model_id</code> is supplied.</li>
<li><em>default_model</em>: One of {‘mistral’, ‘zephyr’, ‘llama’}, where mistral is Mistral-Instruct-7B-v0.2, zephyr is Zephyr-7B-beta, and llama is Llama-3.1-8B.</li>
<li><em>default_engine</em>: The engine used to run the <code>default_model</code>. One of {‘llama.cpp’, ‘transformers’}.</li>
<li><em>prompt_template</em>: Optional prompt template (must have a variable named “prompt”). Prompt templates are not typically needed when using the <code>model_id</code> parameter, as transformers sets it automatically.</li>
<li><em>model_download_path</em>: Path to download model. Default is <code>onprem_data</code> in user’s home directory.</li>
<li><em>vectordb_path</em>: Path to vector database (created if it doesn’t exist). Default is <code>onprem_data/vectordb</code> in user’s home directory.</li>
<li><em>store_type</em>: One of <code>dense</code> for the default dense vector database (i.e., chroma) or <code>sparse</code> for the sparse vector store (i.e., a keyword search engine).<br>
(Documents stored in sparse vector databases are converted to dense vectors at inference time when used with <a href="https://amaiya.github.io/onprem/llm.base.html#llm.ask"><code>LLM.ask</code></a>.)</li>
<li><em>vectorstore</em>: an onprem.ingest.stores.base.VectorStore instance.</li>
<li><em>max_tokens</em>: The maximum number of tokens to generate.</li>
<li><em>n_gpu_layers</em>: Number of layers to be loaded into gpu memory. Default is <code>None</code>. Only used for llama-cpp backend.</li>
<li><em>n_ctx</em>: Token context window. Only used for llama-cpp backend. For Ollama backend, explicitly supply <code>num_ctx</code> instead which is passed to LiteLLM. Hugging Face Transformers backend (i.e., when using the model_id parameter) sets context window automatically.</li>
<li><em>n_batch</em>: Number of tokens to process in parallel. Only used for llama-cpp backend.</li>
<li><em>stop</em>: a list of strings to stop generation when encountered (applied to all calls to <a href="https://amaiya.github.io/onprem/llm.base.html#llm.prompt"><code>LLM.prompt</code></a>)</li>
<li><em>mute_stream</em>: Mute ChatGPT-like token stream output during generation</li>
<li><em>callbacks</em>: Callbacks to supply model</li>
<li><em>embedding_model_name</em>: name of sentence-transformers model. Used for <a href="https://amaiya.github.io/onprem/llm.base.html#llm.ingest"><code>LLM.ingest</code></a> and <a href="https://amaiya.github.io/onprem/llm.base.html#llm.ask"><code>LLM.ask</code></a>.</li>
<li><em>embedding_model_kwargs</em>: arguments to embedding model (e.g., <code>{device':'cpu'}</code>). If None, uses GPU if available.</li>
<li><em>embedding_encode_kwargs</em>: arguments to encode method of embedding model (e.g., <code>{'normalize_embeddings': False}</code>).</li>
<li><em>rag_num_source_docs</em>: The maximum number of documents retrieved and fed to <a href="https://amaiya.github.io/onprem/llm.base.html#llm.ask"><code>LLM.ask</code></a> and <a href="https://amaiya.github.io/onprem/llm.base.html#llm.chat"><code>LLM.chat</code></a> to generate answers.</li>
<li><em>rag_score_threshold</em>: Minimum similarity score for source to be considered by <a href="https://amaiya.github.io/onprem/llm.base.html#llm.ask"><code>LLM.ask</code></a> and <a href="https://amaiya.github.io/onprem/llm.base.html#llm.chat"><code>LLM.chat</code></a>.</li>
<li><em>confirm</em>: whether or not to confirm with user before downloading a model</li>
<li><em>verbose</em>: Verbosity*</li>
</ul>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L312" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.download_model" class="level3">
<h3 class="anchored" data-anchor-id="llm.download_model">LLM.download_model</h3>
<blockquote class="blockquote">
<pre><code> LLM.download_model (model_url:Optional[str]=None,
                     default_model:str='zephyr',
                     model_download_path:Optional[str]=None,
                     confirm:bool=True, ssl_verify:bool=True)</code></pre>
</blockquote>
<p>*Download an LLM in GGML format supported by <a href="https://github.com/ggerganov/llama.cpp">lLama.cpp</a>.</p>
<p><strong>Args:</strong></p>
<ul>
<li><em>model_url</em>: URL of model. If None, then use default_model.</li>
<li><em>default_model</em>: One of {‘mistral’, ‘zephyr’, ‘llama’}, where mistral is Mistral-Instruct-7B-v0.2, zephyr is Zephyr-7B-beta, and llama is Llama-3.1-8B.</li>
<li><em>model_download_path</em>: Path to download model. Default is <code>onprem_data</code> in user’s home directory.</li>
<li><em>confirm</em>: whether or not to confirm with user before downloading</li>
<li><em>ssl_verify</em>: If True, SSL certificates are verified. You can set to False if corporate firewall gives you problems.*</li>
</ul>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L528" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.load_llm" class="level3">
<h3 class="anchored" data-anchor-id="llm.load_llm">LLM.load_llm</h3>
<blockquote class="blockquote">
<pre><code> LLM.load_llm ()</code></pre>
</blockquote>
<p><em>Loads the LLM from the model path.</em></p>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L354" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.load_vectorstore" class="level3">
<h3 class="anchored" data-anchor-id="llm.load_vectorstore">LLM.load_vectorstore</h3>
<blockquote class="blockquote">
<pre><code> LLM.load_vectorstore (custom_vectorstore=None, reset=False)</code></pre>
</blockquote>
<p><em>Get <a href="https://amaiya.github.io/onprem/ingest.stores.base.html#vectorstore"><code>VectorStore</code></a> instance. Use the vectorstore’s methods directly instead of accessing the underlying database. Supply <code>custom_vectorstore</code> to use your own <a href="https://amaiya.github.io/onprem/ingest.stores.base.html#vectorstore"><code>VectorStore</code></a> instance (i.e., subclass <a href="https://amaiya.github.io/onprem/ingest.stores.dense.html#densestore"><code>DenseStore</code></a> or <a href="https://amaiya.github.io/onprem/ingest.stores.sparse.html#sparsestore"><code>SparseStore</code></a>). Supply <code>reset=True</code> to reload the default vectorstore.</em></p>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L814" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.load_chatbot" class="level3">
<h3 class="anchored" data-anchor-id="llm.load_chatbot">LLM.load_chatbot</h3>
<blockquote class="blockquote">
<pre><code> LLM.load_chatbot ()</code></pre>
</blockquote>
<p><em>Prepares and loads a <code>langchain.chains.ConversationChain</code> instance</em></p>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L832" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.query" class="level3">
<h3 class="anchored" data-anchor-id="llm.query">LLM.query</h3>
<blockquote class="blockquote">
<pre><code> LLM.query (*args, **kwargs)</code></pre>
</blockquote>
<p><em>Perform a semantic search of vectorstore.</em></p>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L720" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.prompt" class="level3">
<h3 class="anchored" data-anchor-id="llm.prompt">LLM.prompt</h3>
<blockquote class="blockquote">
<pre><code> LLM.prompt (prompt:Union[str,List[Dict]],
             output_parser:Optional[Any]=None,
             image_path_or_url:Optional[str]=None,
             prompt_template:Optional[str]=None, stop:list=[],
             truncate_prompt:bool=False, truncate_strategy:str='start',
             **kwargs)</code></pre>
</blockquote>
<p>*Send prompt to LLM to generate a response. Extra keyword arguments are sent directly to the model invocation.</p>
<p><strong>Args:</strong></p>
<ul>
<li><em>prompt</em>: The prompt to supply to the model. Either a string or OpenAI-style list of dictionaries representing messages (e.g., “human”, “system”).</li>
<li><em>image_path_or_url</em>: Path or URL to an image file</li>
<li><em>prompt_template</em>: Optional prompt template (must have a variable named “prompt”). This value will override any <code>prompt_template</code> value supplied to <a href="https://amaiya.github.io/onprem/llm.base.html#llm"><code>LLM</code></a> constructor.</li>
<li><em>stop</em>: a list of strings to stop generation when encountered. This value will override the <code>stop</code> parameter supplied to <a href="https://amaiya.github.io/onprem/llm.base.html#llm"><code>LLM</code></a> constructor.</li>
<li><em>truncate_prompt</em>: Truncate long string prompts. Only applies to <code>llama-cpp-python</code> and <code>transformers</code> LLMs.</li>
<li><em>truncate_strategy</em>: Either ‘first’ (keep latest) or ’last<code>(keep earliest). Ignored if</code>truncate_prompt=False`.*</li>
</ul>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L666" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.pydantic_prompt" class="level3">
<h3 class="anchored" data-anchor-id="llm.pydantic_prompt">LLM.pydantic_prompt</h3>
<blockquote class="blockquote">
<pre><code> LLM.pydantic_prompt (prompt:str, pydantic_model=None,
                      attempt_fix:bool=False, fix_llm=None, stop:list=[],
                      **kwargs)</code></pre>
</blockquote>
<p>*Accept a prompt as string and Pydantic model describing the desired output. Output will be a Pydantic object in the requested format.</p>
<p><strong>Args:</strong></p>
<ul>
<li><em>prompt</em>: The prompt to supply to the model. Either a string or OpenAI-style list of dictionaries representing messages (e.g., “human”, “system”).</li>
<li><em>pydantic_model</em>: A Pydanatic model (sublass of <code>pydantic.BaseModel</code> that describes the desired output format. Output will be a desired Pydantic object. If <code>put_format=None</code>, then output is a string.</li>
<li><em>attempt_fix</em>: Use an LLM call in attempt to correct malformed or incomplete outputs</li>
<li><em>fix_llm</em>: LLM to use for fixing (e.g., <code>langchain_openai.ChatOpenAI()</code>). If <code>None</code>, then existing <code>LLM.llm</code> used.</li>
<li><em>stop</em>: a list of strings to stop generation when encountered. This value will override the <code>stop</code> parameter supplied to <a href="https://amaiya.github.io/onprem/llm.base.html#llm"><code>LLM</code></a> constructor.*</li>
</ul>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L407" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.ingest" class="level3">
<h3 class="anchored" data-anchor-id="llm.ingest">LLM.ingest</h3>
<blockquote class="blockquote">
<pre><code> LLM.ingest (source_directory:str, chunk_size:int=500,
             chunk_overlap:int=50, ignore_fn:Optional[Callable]=None,
             batch_size:int=1000, **kwargs)</code></pre>
</blockquote>
<p><em>Ingests all documents in <code>source_folder</code> into vector database. Previously-ingested documents are ignored. Extra kwargs fed to <a href="https://amaiya.github.io/onprem/ingest.base.html#load_single_document"><code>load_single_document</code></a>, load_documents<code>, and/or [</code>chunk_documents`](https://amaiya.github.io/onprem/ingest.base.html#chunk_documents).</em></p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>source_directory</td>
<td>str</td>
<td></td>
<td>path to folder containing documents</td>
</tr>
<tr class="even">
<td>chunk_size</td>
<td>int</td>
<td>500</td>
<td>text is split to this many characters by <code>langchain.text_splitter.RecursiveCharacterTextSplitter</code></td>
</tr>
<tr class="odd">
<td>chunk_overlap</td>
<td>int</td>
<td>50</td>
<td>character overlap between chunks in <code>langchain.text_splitter.RecursiveCharacterTextSplitter</code></td>
</tr>
<tr class="even">
<td>ignore_fn</td>
<td>Optional</td>
<td>None</td>
<td>callable that accepts the file path and returns True for ignored files</td>
</tr>
<tr class="odd">
<td>batch_size</td>
<td>int</td>
<td>1000</td>
<td>batch size used when processing documents(e.g, creating embeddings).</td>
</tr>
<tr class="even">
<td>kwargs</td>
<td>VAR_KEYWORD</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L939" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.ask" class="level3">
<h3 class="anchored" data-anchor-id="llm.ask">LLM.ask</h3>
<blockquote class="blockquote">
<pre><code> LLM.ask (question:str, selfask:bool=False, qa_template='"Use the
          following pieces of context delimited by three backticks to
          answer the question at the end. If you don\'t know the answer,
          just say that you don\'t know, don\'t try to make up an
          answer.\n\n```{context}```\n\nQuestion: {question}\nHelpful
          Answer:', filters:Optional[Dict[str,str]]=None,
          where_document=None, folders:Optional[list]=None,
          limit:Optional[int]=None, score_threshold:Optional[float]=None,
          table_k:int=1, table_score_threshold:float=0.35, **kwargs)</code></pre>
</blockquote>
<p><em>Answer a question based on source documents fed to the <a href="https://amaiya.github.io/onprem/llm.base.html#llm.ingest"><code>LLM.ingest</code></a> method. Extra keyword arguments are sent directly to <a href="https://amaiya.github.io/onprem/llm.base.html#llm.prompt"><code>LLM.prompt</code></a>. Returns a dictionary with keys: <code>answer</code>, <code>source_documents</code>, <code>question</code></em></p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>question</td>
<td>str</td>
<td></td>
<td>question as sting</td>
</tr>
<tr class="even">
<td>selfask</td>
<td>bool</td>
<td>False</td>
<td>If True, use an agentic Self-Ask prompting strategy.</td>
</tr>
<tr class="odd">
<td>qa_template</td>
<td>str</td>
<td>“Use the following pieces of context delimited by three backticks to answer the question at the end. If you don’t know the answer, just say that you don’t know, don’t try to make up an answer.<br><br><code>{context}</code><br><br>Question: {question}<br>Helpful Answer:</td>
<td>question-answering prompt template to tuse</td>
</tr>
<tr class="even">
<td>filters</td>
<td>Optional</td>
<td>None</td>
<td>filter sources by metadata values using Chroma metadata syntax (e.g., {‘table’:True})</td>
</tr>
<tr class="odd">
<td>where_document</td>
<td>NoneType</td>
<td>None</td>
<td>filter sources by document content (syntax varies by store type)</td>
</tr>
<tr class="even">
<td>folders</td>
<td>Optional</td>
<td>None</td>
<td>folders to search (needed because LangChain does not forward “where” parameter)</td>
</tr>
<tr class="odd">
<td>limit</td>
<td>Optional</td>
<td>None</td>
<td>Number of sources to consider. If None, use <code>LLM.rag_num_source_docs</code>.</td>
</tr>
<tr class="even">
<td>score_threshold</td>
<td>Optional</td>
<td>None</td>
<td>minimum similarity score of source. If None, use <code>LLM.rag_score_threshold</code>.</td>
</tr>
<tr class="odd">
<td>table_k</td>
<td>int</td>
<td>1</td>
<td>maximum number of tables to consider when generating answer</td>
</tr>
<tr class="even">
<td>table_score_threshold</td>
<td>float</td>
<td>0.35</td>
<td>minimum similarity score for table to be considered in answer</td>
</tr>
<tr class="odd">
<td>kwargs</td>
<td>VAR_KEYWORD</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://github.com/amaiya/onprem/blob/master/onprem/llm/base.py#L994" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="llm.chat" class="level3">
<h3 class="anchored" data-anchor-id="llm.chat">LLM.chat</h3>
<blockquote class="blockquote">
<pre><code> LLM.chat (prompt:str, prompt_template=None, **kwargs)</code></pre>
</blockquote>
<p>*Chat with LLM.</p>
<p><strong>Args:</strong></p>
<ul>
<li><em>question</em>: a question you want to ask*</li>
</ul>
</section>
<section id="example-usage" class="level2">
<h2 class="anchored" data-anchor-id="example-usage">Example Usage</h2>
<p>We’ll use a small 3B-parameter model here for testing purposes. The vector database is stored under <code>~/onprem_data</code> by default. In this example, we will store the vector store in temporary folders.</p>
<div id="cell-14" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tempfile</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-15" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>vectordb_path <span class="op">=</span> tempfile.mkdtemp()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-16" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf'</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LLM(model_url<span class="op">=</span>url,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>          prompt_template <span class="op">=</span> <span class="st">"&lt;|system|&gt;</span><span class="ch">\n</span><span class="st">&lt;/s&gt;</span><span class="ch">\n</span><span class="st">&lt;|user|&gt;</span><span class="ch">\n</span><span class="sc">{prompt}</span><span class="st">&lt;/s&gt;</span><span class="ch">\n</span><span class="st">&lt;|assistant|&gt;"</span>, verbose<span class="op">=</span><span class="va">False</span>, confirm<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>llama_new_context_with_model: n_ctx_per_seq (3904) &lt; n_ctx_train (32768) -- the full capacity of the model will not be utilized</code></pre>
</div>
</div>
<div id="cell-17" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> os.path.isfile(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    os.path.join(U.get_datadir(), os.path.basename(url))</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>), <span class="st">"missing model"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-18" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"""List three cute names for a cat."""</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-19" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>saved_output <span class="op">=</span> llm.prompt(prompt)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
1. Luna - this name means "moon" in Latin and is perfect for a cat with soft, moon-like fur or bright green eyes that seem to glow like the full moon.

2. Willow - named after the delicate branches of a willow tree, this name would suit a sweet, gentle kitty who loves to snuggle and purr contentedly in your lap.

3. Marshmallow - if you have a fluffy cat with a round tummy and a plump body, why not call her Marshmallow? This adorable name is sure to melt your heart as soon as you see her cute little face.</code></pre>
</div>
</div>
<div id="cell-20" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>llm.ingest(<span class="st">"./tests/sample_data/ktrain_paper/"</span>, chunk_size<span class="op">=</span><span class="dv">500</span>, chunk_overlap<span class="op">=</span><span class="dv">50</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Appending to existing vectorstore at /home/amaiya/onprem_data/vectordb
Loading documents from ./sample_data/1/</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading new documents: 100%|██████████████████████| 1/1 [00:00&lt;00:00,  3.52it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loaded 6 new documents from ./sample_data/1/
Split into 41 chunks of text (max. 500 chars each for text; max. 2000 chars for tables)
Creating embeddings. May take some minutes...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  1.18it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Ingestion complete! You can now query your documents using the LLM.ask or LLM.chat methods</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<div id="cell-21" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"""What is ktrain?"""</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> llm.ask(question)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">References:</span><span class="ch">\n\n</span><span class="st">"</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, document <span class="kw">in</span> <span class="bu">enumerate</span>(result[<span class="st">"source_documents"</span>]):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">.&gt; "</span> <span class="op">+</span> document.metadata[<span class="st">"source"</span>] <span class="op">+</span> <span class="st">":"</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(document.page_content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Ktrain is a Python library for machine learning that aims to provide a simple and unified interface for easily executing the three main steps of the machine learning process - preparing data, training models, and evaluating results - regardless of the type of data being used (such as text, images, or graphs). It is designed to help beginners and domain experts with limited programming or data science experience to build sophisticated machine learning models with minimal coding, while also serving as a useful toolbox for more experienced users. Ktrain follows a standard template for supervised learning tasks and supports custom models and data formats. It is licensed under the Apache license and can be found on GitHub at https://github.com/amaiya/ktrain. The text material mentions that ktrain was inspired by other low-code (and no-code) open-source ML libraries such as fastai and ludwig, and aims to further democratize machine learning by making it more accessible to a wider range of users.

References:



1.&gt; /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:
transferred to, and executed on new data in a production environment.
ktrain is a Python library for machine learning with the goal of presenting a simple,
uniﬁed interface to easily perform the above steps regardless of the type of data (e.g., text
vs. images vs. graphs). Moreover, each of the three steps above can be accomplished in
©2022 Arun S. Maiya.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are

2.&gt; /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:
custom models and data formats, as well. Inspired by other low-code (and no-code) open-
source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig (Molino et al.,
2019), ktrain is intended to help further democratize machine learning by enabling begin-
ners and domain experts with minimal programming or data science experience to build
sophisticated machine learning models with minimal coding. It is also a useful toolbox for

3.&gt; /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:
ktrain.Learner instance, which is an abstraction to facilitate training.
1. https://www.fast.ai/2018/07/16/auto-ml2/
2

4.&gt; /home/amaiya/projects/ghub/onprem/nbs/sample_data/1/ktrain_paper.pdf:
Apache license, and available on GitHub at: https://github.com/amaiya/ktrain.
2. Building Models
Supervised learning tasks in ktrain follow a standard, easy-to-use template.
STEP 1: Load and Preprocess Data. This step involves loading data from diﬀerent
sources and preprocessing it in a way that is expected by the model. In the case of text,
this may involve language-speciﬁc preprocessing (e.g., tokenization). In the case of images,</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/amaiya\.github\.io\/onprem");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/amaiya/onprem/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>